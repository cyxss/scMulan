{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Fine-tuning on Pre-trained Model with Batch Integration\n",
    "In this tutorial, we demonstrate how to fine-tune a pre-trained model on a new dataset for the batch integration task. We use the PBMC 10K dataset as an example and fine-tune on the pre-trained whole-body model. \n",
    "\n",
    "We summarize the fine-tuning pipeline in the following steps, which can be used as a general recipe for finetuning on integration tasks and beyond: \n",
    "\n",
    "     1. Specify hyper-parameter setup for integration task\n",
    "     \n",
    "     2. Load and pre-process data\n",
    "     \n",
    "     3. Load the pre-trained scGPT model\n",
    "     \n",
    "     4. Finetune scGPT with task-specific objectives\n",
    "     \n",
    "     5. Evaluate fine-tuned scGPT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-28T06:16:18.135225Z",
     "iopub.status.busy": "2023-12-28T06:16:18.134702Z",
     "iopub.status.idle": "2023-12-28T06:16:18.153586Z",
     "shell.execute_reply": "2023-12-28T06:16:18.148848Z",
     "shell.execute_reply.started": "2023-12-28T06:16:18.135177Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['R_HOME'] = '/data1/chenyx/anaconda3/envs/scGPT/lib64/R'\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-28T06:16:18.162367Z",
     "iopub.status.busy": "2023-12-28T06:16:18.162064Z",
     "iopub.status.idle": "2023-12-28T06:16:20.323443Z",
     "shell.execute_reply": "2023-12-28T06:16:20.322031Z",
     "shell.execute_reply.started": "2023-12-28T06:16:18.162337Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-28T06:17:28.424072Z",
     "iopub.status.busy": "2023-12-28T06:17:28.423465Z",
     "iopub.status.idle": "2023-12-28T06:17:28.438133Z",
     "shell.execute_reply": "2023-12-28T06:17:28.437087Z",
     "shell.execute_reply.started": "2023-12-28T06:17:28.424016Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import gc\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import time\n",
    "import traceback\n",
    "from typing import List, Tuple, Dict, Union, Optional\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "from anndata import AnnData\n",
    "import scanpy as sc\n",
    "import scvi\n",
    "import numpy as np\n",
    "import wandb\n",
    "from scipy.sparse import issparse\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext._torchtext import (\n",
    "    Vocab as VocabPybind,\n",
    ")\n",
    "\n",
    "from scgpt.tokenizer.gene_tokenizer import GeneVocab\n",
    "\n",
    "sys.path.insert(0, \"../\")\n",
    "import scgpt as scg\n",
    "from scgpt.model import TransformerModel, AdversarialDiscriminator\n",
    "from scgpt.tokenizer import tokenize_and_pad_batch, random_mask_value\n",
    "from scgpt.loss import (\n",
    "    masked_mse_loss,\n",
    "    masked_relative_error,\n",
    "    criterion_neg_log_bernoulli,\n",
    ")\n",
    "from scgpt.preprocess import Preprocessor\n",
    "from scgpt import SubsetsBatchSampler\n",
    "from scgpt.utils import set_seed, category_str2int, eval_scib_metrics\n",
    "\n",
    "sc.set_figure_params(figsize=(4, 4))\n",
    "os.environ[\"KMP_WARNINGS\"] = \"off\"\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1: Specify hyper-parameter setup for integration task\n",
    "Here we provide some hyper-parameter recommendations here for the integration task. Note that the PBMC 10K dataset contains multiple batches to be integrated. Therefore, in addition to the default gene modelling objectives, we also turn on ESC, DAR and DSBN objectives specifically to faciliate batch integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-28T06:18:09.089930Z",
     "iopub.status.busy": "2023-12-28T06:18:09.088921Z",
     "iopub.status.idle": "2023-12-28T06:18:27.582109Z",
     "shell.execute_reply": "2023-12-28T06:18:27.580554Z",
     "shell.execute_reply.started": "2023-12-28T06:18:09.089872Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'seed': 42, 'dataset_name': 'Lung', 'do_train': True, 'load_model': '../whole_human/', 'GEPC': True, 'ecs_thres': 0.8, 'dab_weight': 1.0, 'mask_ratio': 0.4, 'epochs': 15, 'n_bins': 51, 'lr': 0.0001, 'batch_size': 64, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 25, 'log_interval': 100, 'fast_transformer': True, 'pre_norm': False, 'amp': True}\n"
     ]
    }
   ],
   "source": [
    "hyperparameter_defaults = dict(\n",
    "    seed=42,\n",
    "    dataset_name=\"Lung\", # Dataset name\n",
    "    do_train=True, # Flag to indicate whether to do update model parameters during training\n",
    "    load_model=\"../whole_human/\", # Path to pre-trained model\n",
    "    GEPC=True,  # Gene expression modelling for cell objective\n",
    "    ecs_thres=0.8,  # Elastic cell similarity objective, 0.0 to 1.0, 0.0 to disable\n",
    "    dab_weight=1.0, # DAR objective weight for batch correction\n",
    "    mask_ratio=0.4, # Default mask ratio\n",
    "    epochs=15, # Default number of epochs for fine-tuning\n",
    "    n_bins=51, # Default number of bins for value binning in data pre-processing\n",
    "    lr=1e-4, # Default learning rate for fine-tuning\n",
    "    batch_size=64, # Default batch size for fine-tuning\n",
    "    layer_size=128,\n",
    "    nlayers=4,\n",
    "    nhead=4, # if load model, batch_size, layer_size, nlayers, nhead will be ignored\n",
    "    dropout=0.2, # Default dropout rate during model fine-tuning\n",
    "    schedule_ratio=0.9,  # Default rate for learning rate decay\n",
    "    save_eval_interval=25, # Default model evaluation interval\n",
    "    log_interval=100, # Default log interval\n",
    "    fast_transformer=True, # Default setting\n",
    "    pre_norm=False, # Default setting\n",
    "    amp=True,  # # Default setting: Automatic Mixed Precision\n",
    ")\n",
    "run = wandb.init(\n",
    "    config=hyperparameter_defaults,\n",
    "    project=\"scGPT\",\n",
    "    reinit=True,\n",
    "    settings=wandb.Settings(start_method=\"fork\"),\n",
    "    mode=\"offline\"\n",
    ")\n",
    "config = wandb.config\n",
    "print(config)\n",
    "\n",
    "set_seed(config.seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-28T06:18:50.116614Z",
     "iopub.status.busy": "2023-12-28T06:18:50.115784Z",
     "iopub.status.idle": "2023-12-28T06:18:50.123017Z",
     "shell.execute_reply": "2023-12-28T06:18:50.122037Z",
     "shell.execute_reply.started": "2023-12-28T06:18:50.116558Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# settings for input and preprocessing\n",
    "pad_token = \"<pad>\"\n",
    "special_tokens = [pad_token, \"<cls>\", \"<eoc>\"]\n",
    "mask_ratio = config.mask_ratio\n",
    "mask_value = -1\n",
    "pad_value = -2\n",
    "n_input_bins = config.n_bins\n",
    "\n",
    "n_hvg = 2000  # number of highly variable genes\n",
    "max_seq_len = n_hvg + 1\n",
    "per_seq_batch_sample = True\n",
    "DSBN = True  # Domain-spec batchnorm\n",
    "explicit_zero_prob = True  # whether explicit bernoulli for zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-28T06:18:51.422550Z",
     "iopub.status.busy": "2023-12-28T06:18:51.421966Z",
     "iopub.status.idle": "2023-12-28T06:18:51.438538Z",
     "shell.execute_reply": "2023-12-28T06:18:51.437092Z",
     "shell.execute_reply.started": "2023-12-28T06:18:51.422504Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save to save/dev_Lung-12æœˆ28-14-18\n"
     ]
    }
   ],
   "source": [
    "dataset_name = config.dataset_name\n",
    "save_dir = Path(f\"./save/dev_{dataset_name}-{time.strftime('%b%d-%H-%M')}/\")\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"save to {save_dir}\")\n",
    "logger = scg.logger\n",
    "scg.utils.add_file_handler(logger, save_dir / \"run.log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load and pre-process data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.1 Load the Heart data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-28T06:18:53.268817Z",
     "iopub.status.busy": "2023-12-28T06:18:53.268020Z",
     "iopub.status.idle": "2023-12-28T06:18:53.275185Z",
     "shell.execute_reply": "2023-12-28T06:18:53.273930Z",
     "shell.execute_reply.started": "2023-12-28T06:18:53.268766Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-28T06:18:55.542226Z",
     "iopub.status.busy": "2023-12-28T06:18:55.541563Z",
     "iopub.status.idle": "2023-12-28T06:19:01.918129Z",
     "shell.execute_reply": "2023-12-28T06:19:01.917251Z",
     "shell.execute_reply.started": "2023-12-28T06:18:55.542183Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "adata = sc.read_h5ad(\"/nfs/public/cell_gpt_data/Intergation_Data/Lung/raw.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-28T06:19:09.520806Z",
     "iopub.status.busy": "2023-12-28T06:19:09.520289Z",
     "iopub.status.idle": "2023-12-28T06:19:09.528675Z",
     "shell.execute_reply": "2023-12-28T06:19:09.527539Z",
     "shell.execute_reply.started": "2023-12-28T06:19:09.520765Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "adata.var[\"gene_name\"] = adata.var.index\n",
    "data_is_raw = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.2 Cross-check gene set with the pre-trained model \n",
    "Note that we retain the common gene set between the data and the pre-trained model for further fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-28T06:19:13.407236Z",
     "iopub.status.busy": "2023-12-28T06:19:13.406438Z",
     "iopub.status.idle": "2023-12-28T06:19:13.958877Z",
     "shell.execute_reply": "2023-12-28T06:19:13.957583Z",
     "shell.execute_reply.started": "2023-12-28T06:19:13.407184Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - match 1997/2000 genes in vocabulary of size 60697.\n",
      "scGPT - INFO - Resume model from ../whole_human/best_model.pt, the model args will be overriden by the config ../whole_human/args.json.\n"
     ]
    }
   ],
   "source": [
    "if config.load_model is not None:\n",
    "    model_dir = Path(config.load_model)\n",
    "    model_config_file = model_dir / \"args.json\"\n",
    "    model_file = model_dir / \"best_model.pt\"\n",
    "    vocab_file = model_dir / \"vocab.json\"\n",
    "\n",
    "    vocab = GeneVocab.from_file(vocab_file)\n",
    "    for s in special_tokens:\n",
    "        if s not in vocab:\n",
    "            vocab.append_token(s)\n",
    "\n",
    "    adata.var[\"id_in_vocab\"] = [\n",
    "        1 if gene in vocab else -1 for gene in adata.var[\"gene_name\"]\n",
    "    ]\n",
    "    gene_ids_in_vocab = np.array(adata.var[\"id_in_vocab\"])\n",
    "    logger.info(\n",
    "        f\"match {np.sum(gene_ids_in_vocab >= 0)}/{len(gene_ids_in_vocab)} genes \"\n",
    "        f\"in vocabulary of size {len(vocab)}.\"\n",
    "    )\n",
    "    adata = adata[:, adata.var[\"id_in_vocab\"] >= 0]\n",
    "    \n",
    "    # model\n",
    "    with open(model_config_file, \"r\") as f:\n",
    "        model_configs = json.load(f)\n",
    "    logger.info(\n",
    "        f\"Resume model from {model_file}, the model args will be overriden by the \"\n",
    "        f\"config {model_config_file}.\"\n",
    "    )\n",
    "    embsize = model_configs[\"embsize\"]\n",
    "    nhead = model_configs[\"nheads\"]\n",
    "    d_hid = model_configs[\"d_hid\"]\n",
    "    nlayers = model_configs[\"nlayers\"]\n",
    "    n_layers_cls = model_configs[\"n_layers_cls\"]\n",
    "else:\n",
    "    embsize = config.layer_size \n",
    "    nhead = config.nhead\n",
    "    nlayers = config.nlayers  \n",
    "    d_hid = config.layer_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.3 Pre-process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-28T06:19:27.777590Z",
     "iopub.status.busy": "2023-12-28T06:19:27.776803Z",
     "iopub.status.idle": "2023-12-28T06:19:35.996419Z",
     "shell.execute_reply": "2023-12-28T06:19:35.995308Z",
     "shell.execute_reply.started": "2023-12-28T06:19:27.777539Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - Filtering cells by counts ...\n",
      "scGPT - INFO - Normalizing total counts ...\n",
      "scGPT - INFO - Binning data ...\n"
     ]
    }
   ],
   "source": [
    "# set up the preprocessor, use the args to config the workflow\n",
    "preprocessor = Preprocessor(\n",
    "    use_key=\"X\",  # the key in adata.layers to use as raw data\n",
    "    filter_gene_by_counts=False,  # step 1\n",
    "    filter_cell_by_counts=False,  # step 2\n",
    "    normalize_total=1e4,  # 3. whether to normalize the raw data and to what sum\n",
    "    result_normed_key=\"X_normed\",  # the key in adata.layers to store the normalized data\n",
    "    log1p=data_is_raw,  # 4. whether to log1p the normalized data\n",
    "    result_log1p_key=\"X_log1p\",\n",
    "    subset_hvg=False,  # 5. whether to subset the raw data to highly variable genes\n",
    "    hvg_flavor=\"seurat_v3\" if data_is_raw else \"cell_ranger\",\n",
    "    binning=config.n_bins,  # 6. whether to bin the raw data and to what number of bins\n",
    "    result_binned_key=\"X_binned\",  # the key in adata.layers to store the binned data\n",
    ")\n",
    "preprocessor(adata, batch_key=\"str_batch\" if dataset_name != \"heart_cell\" else None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-28T06:19:36.941375Z",
     "iopub.status.busy": "2023-12-28T06:19:36.940629Z",
     "iopub.status.idle": "2023-12-28T06:19:36.949537Z",
     "shell.execute_reply": "2023-12-28T06:19:36.948798Z",
     "shell.execute_reply.started": "2023-12-28T06:19:36.941328Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs Ã— n_vars = 32472 Ã— 1997\n",
       "    obs: 'dataset', 'location', 'nGene', 'nUMI', 'patientGroup', 'percent.mito', 'protocol', 'sanger_type', 'size_factors', 'sampling_method', 'batch', 'cell_type', 'donor', 'n_counts'\n",
       "    var: 'gene_symbol', 'gene_name', 'id_in_vocab'\n",
       "    uns: 'log1p'\n",
       "    obsm: 'bin_edges'\n",
       "    layers: 'X_normed', 'X_binned'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-28T06:19:39.316439Z",
     "iopub.status.busy": "2023-12-28T06:19:39.315868Z",
     "iopub.status.idle": "2023-12-28T06:19:39.324845Z",
     "shell.execute_reply": "2023-12-28T06:19:39.324006Z",
     "shell.execute_reply.started": "2023-12-28T06:19:39.316394Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "adata.obs['batch_id'] = pd.factorize(adata.obs['batch'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-28T06:19:40.723873Z",
     "iopub.status.busy": "2023-12-28T06:19:40.723011Z",
     "iopub.status.idle": "2023-12-28T06:19:41.637958Z",
     "shell.execute_reply": "2023-12-28T06:19:41.637118Z",
     "shell.execute_reply.started": "2023-12-28T06:19:40.723817Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if per_seq_batch_sample:\n",
    "    # sort the adata by batch_id in advance\n",
    "    adata_sorted = adata[adata.obs[\"batch_id\"].argsort()].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Tokenize the input data for model fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-28T06:19:50.902724Z",
     "iopub.status.busy": "2023-12-28T06:19:50.901421Z",
     "iopub.status.idle": "2023-12-28T06:19:51.084567Z",
     "shell.execute_reply": "2023-12-28T06:19:51.083498Z",
     "shell.execute_reply.started": "2023-12-28T06:19:50.902656Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_layer_key = \"X_binned\"\n",
    "all_counts = (\n",
    "    adata.layers[input_layer_key].A\n",
    "    if issparse(adata.layers[input_layer_key])\n",
    "    else adata.layers[input_layer_key]\n",
    ")\n",
    "genes = adata.var[\"gene_name\"].tolist()\n",
    "\n",
    "celltypes_labels = adata.obs[\"cell_type\"].tolist()  # make sure count from 0\n",
    "num_types = len(set(celltypes_labels))\n",
    "celltypes_labels = np.array(celltypes_labels)\n",
    "\n",
    "batch_ids = adata.obs[\"batch_id\"].tolist()\n",
    "num_batch_types = len(set(batch_ids))\n",
    "batch_ids = np.array(batch_ids)\n",
    "\n",
    "(\n",
    "    train_data,\n",
    "    valid_data,\n",
    "    train_celltype_labels,\n",
    "    valid_celltype_labels,\n",
    "    train_batch_labels,\n",
    "    valid_batch_labels,\n",
    ") = train_test_split(\n",
    "    all_counts, celltypes_labels, batch_ids, test_size=0.1, shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-28T06:19:53.276789Z",
     "iopub.status.busy": "2023-12-28T06:19:53.276187Z",
     "iopub.status.idle": "2023-12-28T06:19:53.306983Z",
     "shell.execute_reply": "2023-12-28T06:19:53.305984Z",
     "shell.execute_reply.started": "2023-12-28T06:19:53.276755Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if config.load_model is None:\n",
    "    vocab = Vocab(\n",
    "        VocabPybind(genes + special_tokens, None)\n",
    "    )  # bidirectional lookup [gene <-> int]\n",
    "vocab.set_default_index(vocab[\"<pad>\"])\n",
    "gene_ids = np.array(vocab(genes), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-28T06:19:56.022718Z",
     "iopub.status.busy": "2023-12-28T06:19:56.021860Z",
     "iopub.status.idle": "2023-12-28T06:19:59.472194Z",
     "shell.execute_reply": "2023-12-28T06:19:59.470715Z",
     "shell.execute_reply.started": "2023-12-28T06:19:56.022664Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - train set number of samples: 29224, \n",
      "\t feature length: 2001\n",
      "scGPT - INFO - valid set number of samples: 3248, \n",
      "\t feature length: 2001\n"
     ]
    }
   ],
   "source": [
    "tokenized_train = tokenize_and_pad_batch(\n",
    "    train_data,\n",
    "    gene_ids,\n",
    "    max_len=max_seq_len,\n",
    "    vocab=vocab,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    append_cls=True,  # append <cls> token at the beginning\n",
    "    include_zero_gene=True,\n",
    ")\n",
    "tokenized_valid = tokenize_and_pad_batch(\n",
    "    valid_data,\n",
    "    gene_ids,\n",
    "    max_len=max_seq_len,\n",
    "    vocab=vocab,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    append_cls=True,\n",
    "    include_zero_gene=True,\n",
    ")\n",
    "logger.info(\n",
    "    f\"train set number of samples: {tokenized_train['genes'].shape[0]}, \"\n",
    "    f\"\\n\\t feature length: {tokenized_train['genes'].shape[1]}\"\n",
    ")\n",
    "logger.info(\n",
    "    f\"valid set number of samples: {tokenized_valid['genes'].shape[0]}, \"\n",
    "    f\"\\n\\t feature length: {tokenized_valid['genes'].shape[1]}\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-28T06:20:10.307172Z",
     "iopub.status.busy": "2023-12-28T06:20:10.306492Z",
     "iopub.status.idle": "2023-12-28T06:20:10.322357Z",
     "shell.execute_reply": "2023-12-28T06:20:10.321468Z",
     "shell.execute_reply.started": "2023-12-28T06:20:10.307118Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_data(sort_seq_batch=False) -> Tuple[Dict[str, torch.Tensor]]:\n",
    "    masked_values_train = random_mask_value(\n",
    "        tokenized_train[\"values\"],\n",
    "        mask_ratio=mask_ratio,\n",
    "        mask_value=mask_value,\n",
    "        pad_value=pad_value,\n",
    "    )\n",
    "    masked_values_valid = random_mask_value(\n",
    "        tokenized_valid[\"values\"],\n",
    "        mask_ratio=mask_ratio,\n",
    "        mask_value=mask_value,\n",
    "        pad_value=pad_value,\n",
    "    )\n",
    "    print(\n",
    "        f\"random masking at epoch {epoch:3d}, ratio of masked values in train: \",\n",
    "        f\"{(masked_values_train == mask_value).sum() / (masked_values_train - pad_value).count_nonzero():.4f}\",\n",
    "    )\n",
    "\n",
    "    input_gene_ids_train, input_gene_ids_valid = (\n",
    "        tokenized_train[\"genes\"],\n",
    "        tokenized_valid[\"genes\"],\n",
    "    )\n",
    "    input_values_train, input_values_valid = masked_values_train, masked_values_valid\n",
    "    target_values_train, target_values_valid = (\n",
    "        tokenized_train[\"values\"],\n",
    "        tokenized_valid[\"values\"],\n",
    "    )\n",
    "\n",
    "    tensor_batch_labels_train = torch.from_numpy(train_batch_labels).long()\n",
    "    tensor_batch_labels_valid = torch.from_numpy(valid_batch_labels).long()\n",
    "\n",
    "    if sort_seq_batch:\n",
    "        train_sort_ids = np.argsort(train_batch_labels)\n",
    "        input_gene_ids_train = input_gene_ids_train[train_sort_ids]\n",
    "        input_values_train = input_values_train[train_sort_ids]\n",
    "        target_values_train = target_values_train[train_sort_ids]\n",
    "        tensor_batch_labels_train = tensor_batch_labels_train[train_sort_ids]\n",
    "\n",
    "        valid_sort_ids = np.argsort(valid_batch_labels)\n",
    "        input_gene_ids_valid = input_gene_ids_valid[valid_sort_ids]\n",
    "        input_values_valid = input_values_valid[valid_sort_ids]\n",
    "        target_values_valid = target_values_valid[valid_sort_ids]\n",
    "        tensor_batch_labels_valid = tensor_batch_labels_valid[valid_sort_ids]\n",
    "\n",
    "    train_data_pt = {\n",
    "        \"gene_ids\": input_gene_ids_train,\n",
    "        \"values\": input_values_train,\n",
    "        \"target_values\": target_values_train,\n",
    "        \"batch_labels\": tensor_batch_labels_train,\n",
    "    }\n",
    "    valid_data_pt = {\n",
    "        \"gene_ids\": input_gene_ids_valid,\n",
    "        \"values\": input_values_valid,\n",
    "        \"target_values\": target_values_valid,\n",
    "        \"batch_labels\": tensor_batch_labels_valid,\n",
    "    }\n",
    "\n",
    "    return train_data_pt, valid_data_pt\n",
    "\n",
    "\n",
    "# dataset\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, data: Dict[str, torch.Tensor]):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data[\"gene_ids\"].shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {k: v[idx] for k, v in self.data.items()}\n",
    "\n",
    "\n",
    "# data_loader\n",
    "def prepare_dataloader(\n",
    "    data_pt: Dict[str, torch.Tensor],\n",
    "    batch_size: int,\n",
    "    shuffle: bool = False,\n",
    "    intra_domain_shuffle: bool = False,\n",
    "    drop_last: bool = False,\n",
    "    num_workers: int = 0,\n",
    ") -> DataLoader:\n",
    "    dataset = SeqDataset(data_pt)\n",
    "\n",
    "    if per_seq_batch_sample:\n",
    "        # find the indices of samples in each seq batch\n",
    "        subsets = []\n",
    "        batch_labels_array = data_pt[\"batch_labels\"].numpy()\n",
    "        for batch_label in np.unique(batch_labels_array):\n",
    "            batch_indices = np.where(batch_labels_array == batch_label)[0].tolist()\n",
    "            subsets.append(batch_indices)\n",
    "        data_loader = DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_sampler=SubsetsBatchSampler(\n",
    "                subsets,\n",
    "                batch_size,\n",
    "                intra_subset_shuffle=intra_domain_shuffle,\n",
    "                inter_subset_shuffle=shuffle,\n",
    "                drop_last=drop_last,\n",
    "            ),\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        return data_loader\n",
    "\n",
    "    data_loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Step 3: Load the pre-trained scGPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-12-28T06:20:29.451416Z",
     "iopub.status.busy": "2023-12-28T06:20:29.450363Z",
     "iopub.status.idle": "2023-12-28T06:20:37.561254Z",
     "shell.execute_reply": "2023-12-28T06:20:37.560286Z",
     "shell.execute_reply.started": "2023-12-28T06:20:29.451358Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use domain specific batchnorm with affine=False\n",
      "scGPT - INFO - Loading params encoder.embedding.weight with shape torch.Size([60697, 512])\n",
      "scGPT - INFO - Loading params encoder.enc_norm.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params encoder.enc_norm.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.linear1.weight with shape torch.Size([512, 1])\n",
      "scGPT - INFO - Loading params value_encoder.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params value_encoder.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.norm.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.norm.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params decoder.fc.0.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params decoder.fc.2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params decoder.fc.2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params decoder.fc.4.weight with shape torch.Size([1, 512])\n",
      "scGPT - INFO - Loading params decoder.fc.4.bias with shape torch.Size([1])\n",
      "scGPT - INFO - Loading params mvc_decoder.gene2query.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params mvc_decoder.gene2query.bias with shape torch.Size([512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "ntokens = len(vocab)  # size of vocabulary\n",
    "model = TransformerModel(\n",
    "    ntokens,\n",
    "    embsize,\n",
    "    nhead,\n",
    "    d_hid,\n",
    "    nlayers,\n",
    "    vocab=vocab,\n",
    "    dropout=config.dropout,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    do_mvc=config.GEPC,\n",
    "    do_dab=True,\n",
    "    use_batch_labels=True,\n",
    "    num_batch_labels=num_batch_types,\n",
    "    domain_spec_batchnorm=DSBN,\n",
    "    n_input_bins=n_input_bins,\n",
    "    ecs_threshold=config.ecs_thres,\n",
    "    explicit_zero_prob=explicit_zero_prob,\n",
    "    use_fast_transformer=config.fast_transformer,\n",
    "    pre_norm=config.pre_norm,\n",
    ")\n",
    "if config.load_model is not None:\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(model_file))\n",
    "        logger.info(f\"Loading all model params from {model_file}\")\n",
    "    except:\n",
    "        # only load params that are in the model and match the size\n",
    "        model_dict = model.state_dict()\n",
    "        pretrained_dict = torch.load(model_file)\n",
    "        pretrained_dict = {\n",
    "            k: v\n",
    "            for k, v in pretrained_dict.items()\n",
    "            if k in model_dict and v.shape == model_dict[k].shape\n",
    "        }\n",
    "        for k, v in pretrained_dict.items():\n",
    "            logger.info(f\"Loading params {k} with shape {v.shape}\")\n",
    "        model_dict.update(pretrained_dict)\n",
    "        model.load_state_dict(model_dict)\n",
    "\n",
    "model.to(device)\n",
    "wandb.watch(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-28T06:20:41.006278Z",
     "iopub.status.busy": "2023-12-28T06:20:41.005388Z",
     "iopub.status.idle": "2023-12-28T06:20:41.015910Z",
     "shell.execute_reply": "2023-12-28T06:20:41.015041Z",
     "shell.execute_reply.started": "2023-12-28T06:20:41.006213Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion = masked_mse_loss\n",
    "criterion_dab = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=config.lr, eps=1e-4 if config.amp else 1e-8\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=config.schedule_ratio)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=config.amp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-28T08:06:32.386519Z",
     "iopub.status.busy": "2023-12-28T08:06:32.386025Z",
     "iopub.status.idle": "2023-12-28T08:06:32.421846Z",
     "shell.execute_reply": "2023-12-28T08:06:32.420683Z",
     "shell.execute_reply.started": "2023-12-28T08:06:32.386480Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model: nn.Module, loader: DataLoader) -> None:\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss, total_mse, total_gepc = 0.0, 0.0, 0.0\n",
    "    total_error = 0.0\n",
    "    log_interval = config.log_interval\n",
    "    start_time = time.time()\n",
    "\n",
    "    num_batches = len(loader)\n",
    "    for batch, batch_data in enumerate(loader):\n",
    "        input_gene_ids = batch_data[\"gene_ids\"].to(device)\n",
    "        input_values = batch_data[\"values\"].to(device)\n",
    "        target_values = batch_data[\"target_values\"].to(device)\n",
    "        batch_labels = batch_data[\"batch_labels\"].to(device)\n",
    "\n",
    "        src_key_padding_mask = input_gene_ids.eq(vocab[pad_token])\n",
    "        with torch.cuda.amp.autocast(enabled=config.amp):\n",
    "            output_dict = model(\n",
    "                input_gene_ids,\n",
    "                input_values,\n",
    "                src_key_padding_mask=src_key_padding_mask,\n",
    "                batch_labels=batch_labels if DSBN else None,\n",
    "                MVC=config.GEPC,\n",
    "                ECS=config.ecs_thres > 0,\n",
    "            )\n",
    "\n",
    "            masked_positions = input_values.eq(mask_value)  # the postions to predict\n",
    "            loss = loss_mse = criterion(\n",
    "                output_dict[\"mlm_output\"], target_values, masked_positions\n",
    "            )\n",
    "            metrics_to_log = {\"train/mse\": loss_mse.item()}\n",
    "            if explicit_zero_prob:\n",
    "                loss_zero_log_prob = criterion_neg_log_bernoulli(\n",
    "                    output_dict[\"mlm_zero_probs\"], target_values, masked_positions\n",
    "                )\n",
    "                loss = loss + loss_zero_log_prob\n",
    "                metrics_to_log.update({\"train/nzlp\": loss_zero_log_prob.item()})\n",
    "            if config.GEPC:\n",
    "                loss_gepc = criterion(\n",
    "                    output_dict[\"mvc_output\"], target_values, masked_positions\n",
    "                )\n",
    "                loss = loss + loss_gepc\n",
    "                metrics_to_log.update({\"train/mvc\": loss_gepc.item()})\n",
    "            if config.GEPC and explicit_zero_prob:\n",
    "                loss_gepc_zero_log_prob = criterion_neg_log_bernoulli(\n",
    "                    output_dict[\"mvc_zero_probs\"], target_values, masked_positions\n",
    "                )\n",
    "                loss = loss + loss_gepc_zero_log_prob\n",
    "                metrics_to_log.update(\n",
    "                    {\"train/mvc_nzlp\": loss_gepc_zero_log_prob.item()}\n",
    "                )\n",
    "            if config.ecs_thres > 0:\n",
    "                loss_ecs = 10 * output_dict[\"loss_ecs\"]\n",
    "                loss = loss + loss_ecs\n",
    "                metrics_to_log.update({\"train/ecs\": loss_ecs.item()})\n",
    "            loss_dab = criterion_dab(output_dict[\"dab_output\"], batch_labels)\n",
    "            loss = loss + config.dab_weight * loss_dab\n",
    "            metrics_to_log.update({\"train/dab\": loss_dab.item()})\n",
    "\n",
    "        model.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        with warnings.catch_warnings(record=True) as w:\n",
    "            warnings.filterwarnings(\"always\")\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                model.parameters(),\n",
    "                1.0,\n",
    "                error_if_nonfinite=False if scaler.is_enabled() else True,\n",
    "            )\n",
    "            if len(w) > 0:\n",
    "                logger.warning(\n",
    "                    f\"Found infinite gradient. This may be caused by the gradient \"\n",
    "                    f\"scaler. The current scale is {scaler.get_scale()}. This warning \"\n",
    "                    \"can be ignored if no longer occurs after autoscaling of the scaler.\"\n",
    "                )\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        wandb.log(metrics_to_log)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            mre = masked_relative_error(\n",
    "                output_dict[\"mlm_output\"], target_values, masked_positions\n",
    "            )\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_mse += loss_mse.item()\n",
    "        total_gepc += loss_gepc.item() if config.GEPC else 0.0\n",
    "        total_error += mre.item()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            cur_mse = total_mse / log_interval\n",
    "            cur_gepc = total_gepc / log_interval if config.GEPC else 0.0\n",
    "            cur_error = total_error / log_interval\n",
    "            # ppl = math.exp(cur_loss)\n",
    "            logger.info(\n",
    "                f\"| epoch {epoch:3d} | {batch:3d}/{num_batches:3d} batches | \"\n",
    "                f\"lr {lr:05.4f} | ms/batch {ms_per_batch:5.2f} | \"\n",
    "                f\"loss {cur_loss:5.2f} | mse {cur_mse:5.2f} | mre {cur_error:5.2f} |\"\n",
    "                + (f\"gepc {cur_gepc:5.2f} |\" if config.GEPC else \"\")\n",
    "            )\n",
    "            total_loss = 0\n",
    "            total_mse = 0\n",
    "            total_gepc = 0\n",
    "            total_error = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "\n",
    "def define_wandb_metrcis():\n",
    "    wandb.define_metric(\"valid/mse\", summary=\"min\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"valid/mre\", summary=\"min\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"valid/dab\", summary=\"min\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"valid/sum_mse_dab\", summary=\"min\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"test/avg_bio\", summary=\"max\")\n",
    "\n",
    "\n",
    "def evaluate(model: nn.Module, loader: DataLoader) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate the model on the evaluation data.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_error = 0.0\n",
    "    total_dab = 0.0\n",
    "    total_num = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_data in loader:\n",
    "            input_gene_ids = batch_data[\"gene_ids\"].to(device)\n",
    "            input_values = batch_data[\"values\"].to(device)\n",
    "            target_values = batch_data[\"target_values\"].to(device)\n",
    "            batch_labels = batch_data[\"batch_labels\"].to(device)\n",
    "\n",
    "            src_key_padding_mask = input_gene_ids.eq(vocab[pad_token])\n",
    "            with torch.cuda.amp.autocast(enabled=config.amp):\n",
    "                output_dict = model(\n",
    "                    input_gene_ids,\n",
    "                    input_values,\n",
    "                    src_key_padding_mask=src_key_padding_mask,\n",
    "                    batch_labels=batch_labels if DSBN else None,\n",
    "                )\n",
    "                output_values = output_dict[\"mlm_output\"]\n",
    "\n",
    "                masked_positions = input_values.eq(mask_value)\n",
    "                loss = criterion(output_values, target_values, masked_positions)\n",
    "                loss_dab = criterion_dab(output_dict[\"dab_output\"], batch_labels)\n",
    "\n",
    "            total_loss += loss.item() * len(input_gene_ids)\n",
    "            total_error += masked_relative_error(\n",
    "                output_values, target_values, masked_positions\n",
    "            ).item() * len(input_gene_ids)\n",
    "            total_dab += loss_dab.item() * len(input_gene_ids)\n",
    "            total_num += len(input_gene_ids)\n",
    "\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"valid/mse\": total_loss / total_num,\n",
    "            \"valid/mre\": total_error / total_num,\n",
    "            \"valid/dab\": total_dab / total_num,\n",
    "            \"valid/sum_mse_dab\": (total_loss + config.dab_weight * total_dab)\n",
    "            / total_num,\n",
    "            \"epoch\": epoch,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return total_loss / total_num, total_error / total_num\n",
    "\n",
    "\n",
    "def eval_testdata(\n",
    "    model: nn.Module,\n",
    "    adata_t: AnnData,\n",
    "    include_types: List[str] = [\"cls\"],\n",
    ") -> Optional[Dict]:\n",
    "    \"\"\"evaluate the model on test dataset of adata_t\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # copy adata_t to avoid reuse previously computed results stored in adata_t\n",
    "    adata_t = adata_t.copy()\n",
    "\n",
    "    all_counts = (\n",
    "        adata_t.layers[input_layer_key].A\n",
    "        if issparse(adata_t.layers[input_layer_key])\n",
    "        else adata_t.layers[input_layer_key]\n",
    "    )\n",
    "\n",
    "    celltypes_labels = adata_t.obs[\"celltype\"].tolist()\n",
    "    celltypes_labels = np.array(celltypes_labels)\n",
    "\n",
    "    batch_ids = adata_t.obs[\"batch_id\"].tolist()\n",
    "    batch_ids = np.array(batch_ids)\n",
    "\n",
    "    # Evaluate cls cell embeddings\n",
    "    if \"cls\" in include_types:\n",
    "        logger.info(\"Evaluating cls cell embeddings\")\n",
    "        tokenized_all = tokenize_and_pad_batch(\n",
    "            all_counts,\n",
    "            gene_ids,\n",
    "            max_len=max_seq_len,\n",
    "            vocab=vocab,\n",
    "            pad_token=pad_token,\n",
    "            pad_value=pad_value,\n",
    "            append_cls=True,  # append <cls> token at the beginning\n",
    "            include_zero_gene=True,\n",
    "        )\n",
    "        all_gene_ids, all_values = tokenized_all[\"genes\"], tokenized_all[\"values\"]\n",
    "        src_key_padding_mask = all_gene_ids.eq(vocab[pad_token])\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast(enabled=config.amp):\n",
    "            cell_embeddings = model.encode_batch(\n",
    "                all_gene_ids,\n",
    "                all_values.float(),\n",
    "                src_key_padding_mask=src_key_padding_mask,\n",
    "                batch_size=config.batch_size,\n",
    "                batch_labels=torch.from_numpy(batch_ids).long() if DSBN else None,\n",
    "                time_step=0,\n",
    "                return_np=True,\n",
    "            )\n",
    "        cell_embeddings = cell_embeddings / np.linalg.norm(\n",
    "            cell_embeddings, axis=1, keepdims=True\n",
    "        )\n",
    "\n",
    "        adata_t.obsm[\"X_scGPT\"] = cell_embeddings\n",
    "\n",
    "        results = {}\n",
    "        try:\n",
    "            results = eval_scib_metrics(adata_t)\n",
    "        except Exception as e:\n",
    "            traceback.print_exc()\n",
    "            logger.error(e)\n",
    "\n",
    "        sc.pp.neighbors(adata_t, use_rep=\"X_scGPT\")\n",
    "        sc.tl.umap(adata_t, min_dist=0.3)\n",
    "        fig = sc.pl.umap(\n",
    "            adata_t,\n",
    "            color=[\"batch\"],\n",
    "            title=[f\"batch, avg_bio = {results.get('avg_bio', 0.0):.4f}\"],\n",
    "            frameon=False,\n",
    "            return_fig=True,\n",
    "            show=False,\n",
    "        )\n",
    "\n",
    "        results[\"batch_umap\"] = fig\n",
    "\n",
    "        sc.pp.neighbors(adata_t, use_rep=\"X_scGPT\")\n",
    "        sc.tl.umap(adata_t, min_dist=0.3)\n",
    "        fig = sc.pl.umap(\n",
    "            adata_t,\n",
    "            color=[\"cell_type\"],\n",
    "            title=[\n",
    "                f\"celltype, avg_bio = {results.get('avg_bio', 0.0):.4f}\",\n",
    "            ],\n",
    "            frameon=False,\n",
    "            return_fig=True,\n",
    "            show=False,\n",
    "        )\n",
    "\n",
    "        results[\"celltype_umap\"] = fig\n",
    "\n",
    "    if len(include_types) == 1:\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Finetune scGPT with task-specific objectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-28T06:20:56.409810Z",
     "iopub.status.busy": "2023-12-28T06:20:56.408243Z",
     "iopub.status.idle": "2023-12-28T07:54:11.903322Z",
     "shell.execute_reply": "2023-12-28T07:54:11.901548Z",
     "shell.execute_reply.started": "2023-12-28T06:20:56.409738Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random masking at epoch   1, ratio of masked values in train:  0.3999\n",
      "scGPT - INFO - | epoch   1 | 100/464 batches | lr 0.0001 | ms/batch 688.78 | loss 116.03 | mse 40.45 | mre 1573555.30 |gepc 62.62 |\n",
      "scGPT - INFO - | epoch   1 | 200/464 batches | lr 0.0001 | ms/batch 641.48 | loss 145.00 | mse 74.21 | mre 2120742.89 |gepc 56.76 |\n",
      "scGPT - INFO - | epoch   1 | 300/464 batches | lr 0.0001 | ms/batch 716.84 | loss 170.08 | mse 62.98 | mre 2028696.99 |gepc 92.48 |\n",
      "scGPT - INFO - | epoch   1 | 400/464 batches | lr 0.0001 | ms/batch 707.94 | loss 117.47 | mse 42.20 | mre 1427687.63 |gepc 62.29 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   1 | time: 343.77s | valid loss/mse 43.0903 | mre 1986964.8516\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 43.0903\n",
      "random masking at epoch   2, ratio of masked values in train:  0.3999\n",
      "scGPT - INFO - | epoch   2 | 100/464 batches | lr 0.0001 | ms/batch 917.96 | loss 82.89 | mse 22.40 | mre 992946.90 |gepc 46.48 |\n",
      "scGPT - INFO - | epoch   2 | 200/464 batches | lr 0.0001 | ms/batch 875.43 | loss 95.26 | mse 37.90 | mre 1496430.87 |gepc 43.99 |\n",
      "scGPT - INFO - | epoch   2 | 300/464 batches | lr 0.0001 | ms/batch 880.99 | loss 119.52 | mse 43.00 | mre 1331507.03 |gepc 62.42 |\n",
      "scGPT - INFO - | epoch   2 | 400/464 batches | lr 0.0001 | ms/batch 855.97 | loss 108.98 | mse 36.87 | mre 1124565.14 |gepc 58.74 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   2 | time: 424.04s | valid loss/mse 34.1774 | mre 1061274.8900\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 34.1774\n",
      "random masking at epoch   3, ratio of masked values in train:  0.3999\n",
      "scGPT - INFO - | epoch   3 | 100/464 batches | lr 0.0001 | ms/batch 914.78 | loss 56.59 | mse 19.82 | mre 678259.17 |gepc 24.15 |\n",
      "scGPT - INFO - | epoch   3 | 200/464 batches | lr 0.0001 | ms/batch 870.09 | loss 89.59 | mse 32.10 | mre 1000907.84 |gepc 43.84 |\n",
      "scGPT - INFO - | epoch   3 | 300/464 batches | lr 0.0001 | ms/batch 910.25 | loss 120.57 | mse 41.25 | mre 1301806.89 |gepc 65.43 |\n",
      "scGPT - INFO - | epoch   3 | 400/464 batches | lr 0.0001 | ms/batch 888.18 | loss 121.70 | mse 35.96 | mre 1123283.43 |gepc 73.04 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   3 | time: 430.57s | valid loss/mse 33.2524 | mre 951742.3100\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 33.2524\n",
      "random masking at epoch   4, ratio of masked values in train:  0.3999\n",
      "scGPT - INFO - | epoch   4 | 100/464 batches | lr 0.0001 | ms/batch 920.12 | loss 56.50 | mse 19.43 | mre 624509.16 |gepc 24.41 |\n",
      "scGPT - INFO - | epoch   4 | 200/464 batches | lr 0.0001 | ms/batch 898.92 | loss 83.70 | mse 30.61 | mre 976970.20 |gepc 40.28 |\n",
      "scGPT - INFO - | epoch   4 | 300/464 batches | lr 0.0001 | ms/batch 869.27 | loss 112.04 | mse 40.87 | mre 1262495.81 |gepc 58.00 |\n",
      "scGPT - INFO - | epoch   4 | 400/464 batches | lr 0.0001 | ms/batch 863.71 | loss 102.74 | mse 35.54 | mre 1070184.12 |gepc 55.33 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   4 | time: 425.09s | valid loss/mse 33.4069 | mre 1094359.7248\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "random masking at epoch   5, ratio of masked values in train:  0.3999\n",
      "scGPT - INFO - | epoch   5 | 100/464 batches | lr 0.0001 | ms/batch 844.14 | loss 57.71 | mse 19.54 | mre 640073.83 |gepc 25.13 |\n",
      "scGPT - INFO - | epoch   5 | 200/464 batches | lr 0.0001 | ms/batch 899.05 | loss 78.41 | mse 30.25 | mre 918983.47 |gepc 36.16 |\n",
      "scGPT - INFO - | epoch   5 | 300/464 batches | lr 0.0001 | ms/batch 876.07 | loss 121.15 | mse 39.75 | mre 1155658.77 |gepc 69.92 |\n",
      "scGPT - INFO - | epoch   5 | 400/464 batches | lr 0.0001 | ms/batch 883.74 | loss 99.16 | mse 35.11 | mre 1083014.30 |gepc 53.81 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   5 | time: 422.70s | valid loss/mse 31.9039 | mre 906751.8187\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 31.9039\n",
      "random masking at epoch   6, ratio of masked values in train:  0.3999\n",
      "scGPT - INFO - | epoch   6 | 100/464 batches | lr 0.0001 | ms/batch 928.59 | loss 54.45 | mse 19.17 | mre 610683.10 |gepc 23.10 |\n",
      "scGPT - INFO - | epoch   6 | 200/464 batches | lr 0.0001 | ms/batch 886.20 | loss 79.02 | mse 29.95 | mre 929281.37 |gepc 38.65 |\n",
      "scGPT - INFO - | epoch   6 | 300/464 batches | lr 0.0001 | ms/batch 875.20 | loss 104.61 | mse 38.75 | mre 1210702.76 |gepc 55.36 |\n",
      "scGPT - INFO - | epoch   6 | 400/464 batches | lr 0.0001 | ms/batch 856.71 | loss 93.77 | mse 34.87 | mre 1086136.01 |gepc 48.89 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   6 | time: 423.79s | valid loss/mse 31.4396 | mre 834140.7816\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 31.4396\n",
      "random masking at epoch   7, ratio of masked values in train:  0.3999\n",
      "scGPT - INFO - | epoch   7 | 100/464 batches | lr 0.0001 | ms/batch 867.13 | loss 60.59 | mse 19.21 | mre 589431.11 |gepc 30.57 |\n",
      "scGPT - INFO - | epoch   7 | 200/464 batches | lr 0.0001 | ms/batch 834.47 | loss 72.94 | mse 29.34 | mre 921004.89 |gepc 34.26 |\n",
      "scGPT - INFO - | epoch   7 | 300/464 batches | lr 0.0001 | ms/batch 809.99 | loss 107.21 | mse 38.53 | mre 1212810.97 |gepc 58.54 |\n",
      "scGPT - INFO - | epoch   7 | 400/464 batches | lr 0.0001 | ms/batch 866.02 | loss 89.92 | mse 34.35 | mre 1052709.83 |gepc 46.12 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   7 | time: 407.79s | valid loss/mse 31.4508 | mre 1009409.1436\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "random masking at epoch   8, ratio of masked values in train:  0.3999\n",
      "scGPT - INFO - | epoch   8 | 100/464 batches | lr 0.0000 | ms/batch 893.65 | loss 52.98 | mse 18.96 | mre 597800.57 |gepc 24.12 |\n",
      "scGPT - INFO - | epoch   8 | 200/464 batches | lr 0.0000 | ms/batch 877.51 | loss 71.62 | mse 29.10 | mre 905805.32 |gepc 33.37 |\n",
      "scGPT - INFO - | epoch   8 | 300/464 batches | lr 0.0000 | ms/batch 883.70 | loss 96.04 | mse 38.34 | mre 1179707.89 |gepc 47.66 |\n",
      "scGPT - INFO - | epoch   8 | 400/464 batches | lr 0.0000 | ms/batch 894.49 | loss 87.99 | mse 34.19 | mre 1055355.57 |gepc 44.37 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   8 | time: 426.92s | valid loss/mse 30.7223 | mre 818625.8516\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 30.7223\n",
      "random masking at epoch   9, ratio of masked values in train:  0.3999\n",
      "scGPT - INFO - | epoch   9 | 100/464 batches | lr 0.0000 | ms/batch 834.98 | loss 53.01 | mse 19.02 | mre 559307.70 |gepc 24.38 |\n",
      "scGPT - INFO - | epoch   9 | 200/464 batches | lr 0.0000 | ms/batch 769.81 | loss 71.04 | mse 29.05 | mre 887836.88 |gepc 33.40 |\n",
      "scGPT - INFO - | epoch   9 | 300/464 batches | lr 0.0000 | ms/batch 743.90 | loss 91.05 | mse 37.93 | mre 1179613.71 |gepc 43.55 |\n",
      "scGPT - INFO - | epoch   9 | 400/464 batches | lr 0.0000 | ms/batch 745.94 | loss 85.16 | mse 34.04 | mre 1066997.24 |gepc 41.97 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   9 | time: 368.94s | valid loss/mse 31.2877 | mre 1020926.9781\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "random masking at epoch  10, ratio of masked values in train:  0.3999\n",
      "scGPT - INFO - | epoch  10 | 100/464 batches | lr 0.0000 | ms/batch 757.36 | loss 50.00 | mse 18.77 | mre 599251.77 |gepc 21.90 |\n",
      "scGPT - INFO - | epoch  10 | 200/464 batches | lr 0.0000 | ms/batch 750.94 | loss 68.23 | mse 28.61 | mre 900409.61 |gepc 31.16 |\n",
      "scGPT - INFO - | epoch  10 | 300/464 batches | lr 0.0000 | ms/batch 745.09 | loss 93.08 | mse 37.86 | mre 1162351.08 |gepc 45.68 |\n",
      "scGPT - INFO - | epoch  10 | 400/464 batches | lr 0.0000 | ms/batch 700.44 | loss 83.08 | mse 33.87 | mre 1049514.21 |gepc 40.39 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  10 | time: 347.46s | valid loss/mse 30.7017 | mre 885535.5860\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 30.7017\n",
      "random masking at epoch  11, ratio of masked values in train:  0.3999\n",
      "scGPT - INFO - | epoch  11 | 100/464 batches | lr 0.0000 | ms/batch 658.63 | loss 50.71 | mse 18.78 | mre 585696.12 |gepc 22.90 |\n",
      "scGPT - INFO - | epoch  11 | 200/464 batches | lr 0.0000 | ms/batch 648.52 | loss 69.15 | mse 28.61 | mre 886622.42 |gepc 31.97 |\n",
      "scGPT - INFO - | epoch  11 | 300/464 batches | lr 0.0000 | ms/batch 647.61 | loss 90.71 | mse 37.73 | mre 1166498.62 |gepc 43.64 |\n",
      "scGPT - INFO - | epoch  11 | 400/464 batches | lr 0.0000 | ms/batch 656.20 | loss 82.32 | mse 33.85 | mre 1053009.35 |gepc 40.13 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  11 | time: 313.72s | valid loss/mse 30.3194 | mre 858673.4025\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 30.3194\n",
      "random masking at epoch  12, ratio of masked values in train:  0.3999\n",
      "scGPT - INFO - | epoch  12 | 100/464 batches | lr 0.0000 | ms/batch 663.20 | loss 49.28 | mse 18.68 | mre 573171.66 |gepc 22.10 |\n",
      "scGPT - INFO - | epoch  12 | 200/464 batches | lr 0.0000 | ms/batch 651.48 | loss 68.26 | mse 28.60 | mre 884124.32 |gepc 31.67 |\n",
      "scGPT - INFO - | epoch  12 | 300/464 batches | lr 0.0000 | ms/batch 650.76 | loss 87.52 | mse 37.37 | mre 1168605.98 |gepc 41.42 |\n",
      "scGPT - INFO - | epoch  12 | 400/464 batches | lr 0.0000 | ms/batch 654.92 | loss 80.09 | mse 33.70 | mre 1041013.27 |gepc 38.60 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  12 | time: 315.26s | valid loss/mse 30.8013 | mre 940134.4552\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "random masking at epoch  13, ratio of masked values in train:  0.3999\n",
      "scGPT - INFO - | epoch  13 | 100/464 batches | lr 0.0000 | ms/batch 667.04 | loss 47.88 | mse 18.61 | mre 574458.05 |gepc 21.07 |\n",
      "scGPT - INFO - | epoch  13 | 200/464 batches | lr 0.0000 | ms/batch 653.44 | loss 67.33 | mse 28.45 | mre 889415.61 |gepc 30.84 |\n",
      "scGPT - INFO - | epoch  13 | 300/464 batches | lr 0.0000 | ms/batch 660.67 | loss 86.35 | mse 37.16 | mre 1156940.81 |gepc 40.84 |\n",
      "scGPT - INFO - | epoch  13 | 400/464 batches | lr 0.0000 | ms/batch 654.01 | loss 79.50 | mse 33.56 | mre 1063269.12 |gepc 38.32 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  13 | time: 315.77s | valid loss/mse 30.3514 | mre 920718.7075\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "random masking at epoch  14, ratio of masked values in train:  0.3999\n",
      "scGPT - INFO - | epoch  14 | 100/464 batches | lr 0.0000 | ms/batch 663.91 | loss 47.41 | mse 18.65 | mre 577609.20 |gepc 20.72 |\n",
      "scGPT - INFO - | epoch  14 | 200/464 batches | lr 0.0000 | ms/batch 647.28 | loss 67.12 | mse 28.35 | mre 887774.26 |gepc 30.93 |\n",
      "scGPT - INFO - | epoch  14 | 300/464 batches | lr 0.0000 | ms/batch 650.72 | loss 85.51 | mse 37.01 | mre 1128704.20 |gepc 40.33 |\n",
      "scGPT - INFO - | epoch  14 | 400/464 batches | lr 0.0000 | ms/batch 651.04 | loss 78.52 | mse 33.50 | mre 1072104.08 |gepc 37.62 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  14 | time: 314.16s | valid loss/mse 30.3233 | mre 993509.8930\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "random masking at epoch  15, ratio of masked values in train:  0.3999\n",
      "scGPT - INFO - | epoch  15 | 100/464 batches | lr 0.0000 | ms/batch 662.95 | loss 47.51 | mse 18.54 | mre 578466.35 |gepc 21.09 |\n",
      "scGPT - INFO - | epoch  15 | 200/464 batches | lr 0.0000 | ms/batch 655.77 | loss 65.93 | mse 28.18 | mre 883328.91 |gepc 30.14 |\n",
      "scGPT - INFO - | epoch  15 | 300/464 batches | lr 0.0000 | ms/batch 645.52 | loss 85.91 | mse 36.90 | mre 1136228.75 |gepc 40.86 |\n",
      "scGPT - INFO - | epoch  15 | 400/464 batches | lr 0.0000 | ms/batch 648.43 | loss 77.80 | mse 33.32 | mre 1034891.60 |gepc 36.96 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  15 | time: 314.10s | valid loss/mse 30.2340 | mre 938592.8445\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 30.2340\n",
      "scGPT - INFO - Saving model to save/dev_Lung-12æœˆ28-14-18\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'celltype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/data1/chenyx/anaconda3/envs/scGPT/lib/python3.8/site-packages/pandas/core/indexes/base.py:3361\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3360\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3361\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3362\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/data1/chenyx/anaconda3/envs/scGPT/lib/python3.8/site-packages/pandas/_libs/index.pyx:76\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/data1/chenyx/anaconda3/envs/scGPT/lib/python3.8/site-packages/pandas/_libs/index.pyx:108\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'celltype'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 52\u001b[0m\n\u001b[1;32m     49\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(best_model\u001b[38;5;241m.\u001b[39mstate_dict(), save_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_e\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_model_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# eval on testdata\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43meval_testdata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbest_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43madata_t\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madata_sorted\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mper_seq_batch_sample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43madata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_umap\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msavefig(\n\u001b[1;32m     58\u001b[0m     save_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings_batch_umap[cls]_e\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_model_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m, dpi\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m\n\u001b[1;32m     59\u001b[0m )\n\u001b[1;32m     61\u001b[0m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcelltype_umap\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msavefig(\n\u001b[1;32m     62\u001b[0m     save_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings_celltype_umap[cls]_e\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_model_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m, dpi\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m\n\u001b[1;32m     63\u001b[0m )\n",
      "Cell \u001b[0;32mIn[26], line 189\u001b[0m, in \u001b[0;36meval_testdata\u001b[0;34m(model, adata_t, include_types)\u001b[0m\n\u001b[1;32m    181\u001b[0m adata_t \u001b[38;5;241m=\u001b[39m adata_t\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    183\u001b[0m all_counts \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    184\u001b[0m     adata_t\u001b[38;5;241m.\u001b[39mlayers[input_layer_key]\u001b[38;5;241m.\u001b[39mA\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m issparse(adata_t\u001b[38;5;241m.\u001b[39mlayers[input_layer_key])\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m adata_t\u001b[38;5;241m.\u001b[39mlayers[input_layer_key]\n\u001b[1;32m    187\u001b[0m )\n\u001b[0;32m--> 189\u001b[0m celltypes_labels \u001b[38;5;241m=\u001b[39m \u001b[43madata_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcelltype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m    190\u001b[0m celltypes_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(celltypes_labels)\n\u001b[1;32m    192\u001b[0m batch_ids \u001b[38;5;241m=\u001b[39m adata_t\u001b[38;5;241m.\u001b[39mobs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[0;32m/data1/chenyx/anaconda3/envs/scGPT/lib/python3.8/site-packages/pandas/core/frame.py:3458\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3458\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3459\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3460\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/data1/chenyx/anaconda3/envs/scGPT/lib/python3.8/site-packages/pandas/core/indexes/base.py:3363\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3361\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3362\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3363\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3365\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_scalar(key) \u001b[38;5;129;01mand\u001b[39;00m isna(key) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhasnans:\n\u001b[1;32m   3366\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'celltype'"
     ]
    }
   ],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "best_avg_bio = 0.0\n",
    "best_model = None\n",
    "define_wandb_metrcis()\n",
    "\n",
    "for epoch in range(1, config.epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train_data_pt, valid_data_pt = prepare_data(sort_seq_batch=per_seq_batch_sample)\n",
    "    train_loader = prepare_dataloader(\n",
    "        train_data_pt,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        intra_domain_shuffle=True,\n",
    "        drop_last=False,\n",
    "    )\n",
    "    valid_loader = prepare_dataloader(\n",
    "        valid_data_pt,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        intra_domain_shuffle=False,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    if config.do_train:\n",
    "        train(\n",
    "            model,\n",
    "            loader=train_loader,\n",
    "        )\n",
    "    val_loss, val_mre = evaluate(\n",
    "        model,\n",
    "        loader=valid_loader,\n",
    "    )\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    logger.info(\"-\" * 89)\n",
    "    logger.info(\n",
    "        f\"| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | \"\n",
    "        f\"valid loss/mse {val_loss:5.4f} | mre {val_mre:5.4f}\"\n",
    "    )\n",
    "    logger.info(\"-\" * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "        best_model_epoch = epoch\n",
    "        logger.info(f\"Best model with score {best_val_loss:5.4f}\")\n",
    "\n",
    "    if epoch % config.save_eval_interval == 0 or epoch == config.epochs:\n",
    "        logger.info(f\"Saving model to {save_dir}\")\n",
    "        torch.save(best_model.state_dict(), save_dir / f\"model_e{best_model_epoch}.pt\")\n",
    "\n",
    "        # eval on testdata\n",
    "        results = eval_testdata(\n",
    "            best_model,\n",
    "            adata_t=adata_sorted if per_seq_batch_sample else adata,\n",
    "            include_types=[\"cls\"],\n",
    "        )\n",
    "        results[\"batch_umap\"].savefig(\n",
    "            save_dir / f\"embeddings_batch_umap[cls]_e{best_model_epoch}.png\", dpi=300\n",
    "        )\n",
    "\n",
    "        results[\"celltype_umap\"].savefig(\n",
    "            save_dir / f\"embeddings_celltype_umap[cls]_e{best_model_epoch}.png\", dpi=300\n",
    "        )\n",
    "        metrics_to_log = {\"test/\" + k: v for k, v in results.items()}\n",
    "        metrics_to_log[\"test/batch_umap\"] = wandb.Image(\n",
    "            str(save_dir / f\"embeddings_batch_umap[cls]_e{best_model_epoch}.png\"),\n",
    "            caption=f\"celltype avg_bio epoch {best_model_epoch}\",\n",
    "        )\n",
    "\n",
    "        metrics_to_log[\"test/celltype_umap\"] = wandb.Image(\n",
    "            str(save_dir / f\"embeddings_celltype_umap[cls]_e{best_model_epoch}.png\"),\n",
    "            caption=f\"celltype avg_bio epoch {best_model_epoch}\",\n",
    "        )\n",
    "        metrics_to_log[\"test/best_model_epoch\"] = best_model_epoch\n",
    "        wandb.log(metrics_to_log)\n",
    "        wandb.log({\"avg_bio\": results.get(\"avg_bio\", 0.0)})\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-28T08:06:43.702586Z",
     "iopub.status.busy": "2023-12-28T08:06:43.701814Z",
     "iopub.status.idle": "2023-12-28T08:06:45.920572Z",
     "shell.execute_reply": "2023-12-28T08:06:45.919161Z",
     "shell.execute_reply.started": "2023-12-28T08:06:43.702541Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save the best model\n",
    "save_dir = Path(\"/nfs/public/cell_gpt_data/Intergation_Data/Lung/Results/\")\n",
    "torch.save(best_model.state_dict(), save_dir / \"scGPT_best_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-28T08:07:01.539820Z",
     "iopub.status.busy": "2023-12-28T08:07:01.538650Z",
     "iopub.status.idle": "2023-12-28T08:07:01.551736Z",
     "shell.execute_reply": "2023-12-28T08:07:01.550843Z",
     "shell.execute_reply.started": "2023-12-28T08:07:01.539764Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def GetEmbedding(model: nn.Module,\n",
    "    adata: AnnData,\n",
    "    include_types: List[str] = [\"cls\"],):\n",
    "    model.eval()\n",
    "\n",
    "    # copy adata_t to avoid reuse previously computed results stored in adata_t\n",
    "    adata_t = adata.copy()\n",
    "\n",
    "    all_counts = (\n",
    "        adata_t.layers[input_layer_key].A\n",
    "        if issparse(adata_t.layers[input_layer_key])\n",
    "        else adata_t.layers[input_layer_key]\n",
    "    )\n",
    "\n",
    "    celltypes_labels = adata_t.obs[\"cell_type\"].tolist()\n",
    "    celltypes_labels = np.array(celltypes_labels)\n",
    "\n",
    "    batch_ids = adata_t.obs[\"batch_id\"].tolist()\n",
    "    batch_ids = np.array(batch_ids)\n",
    "\n",
    "    # Evaluate cls cell embeddings\n",
    "    if \"cls\" in include_types:\n",
    "        logger.info(\"Evaluating cls cell embeddings\")\n",
    "        tokenized_all = tokenize_and_pad_batch(\n",
    "            all_counts,\n",
    "            gene_ids,\n",
    "            max_len=max_seq_len,\n",
    "            vocab=vocab,\n",
    "            pad_token=pad_token,\n",
    "            pad_value=pad_value,\n",
    "            append_cls=True,  # append <cls> token at the beginning\n",
    "            include_zero_gene=True,\n",
    "        )\n",
    "        all_gene_ids, all_values = tokenized_all[\"genes\"], tokenized_all[\"values\"]\n",
    "        src_key_padding_mask = all_gene_ids.eq(vocab[pad_token])\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast(enabled=config.amp):\n",
    "            cell_embeddings = model.encode_batch(\n",
    "                all_gene_ids,\n",
    "                all_values.float(),\n",
    "                src_key_padding_mask=src_key_padding_mask,\n",
    "                batch_size=config.batch_size,\n",
    "                batch_labels=torch.from_numpy(batch_ids).long() if DSBN else None,\n",
    "                time_step=0,\n",
    "                return_np=True,\n",
    "            )\n",
    "        cell_embeddings = cell_embeddings / np.linalg.norm(\n",
    "            cell_embeddings, axis=1, keepdims=True\n",
    "        )\n",
    "\n",
    "        adata_t.obsm[\"X_scGPT\"] = cell_embeddings\n",
    "        return adata_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-28T08:07:06.814301Z",
     "iopub.status.busy": "2023-12-28T08:07:06.813496Z",
     "iopub.status.idle": "2023-12-28T08:07:06.821054Z",
     "shell.execute_reply": "2023-12-28T08:07:06.819661Z",
     "shell.execute_reply.started": "2023-12-28T08:07:06.814246Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-28T08:07:07.680310Z",
     "iopub.status.busy": "2023-12-28T08:07:07.679359Z",
     "iopub.status.idle": "2023-12-28T08:08:47.700440Z",
     "shell.execute_reply": "2023-12-28T08:08:47.699275Z",
     "shell.execute_reply.started": "2023-12-28T08:07:07.680257Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - Evaluating cls cell embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 508/508 [01:35<00:00,  5.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.01250004768372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "adata_result = GetEmbedding(best_model, adata)\n",
    "end_time = time.time()\n",
    "print(end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-28T08:09:05.605344Z",
     "iopub.status.busy": "2023-12-28T08:09:05.603937Z",
     "iopub.status.idle": "2023-12-28T08:09:05.614749Z",
     "shell.execute_reply": "2023-12-28T08:09:05.613929Z",
     "shell.execute_reply.started": "2023-12-28T08:09:05.605285Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs Ã— n_vars = 32472 Ã— 1997\n",
       "    obs: 'dataset', 'location', 'nGene', 'nUMI', 'patientGroup', 'percent.mito', 'protocol', 'sanger_type', 'size_factors', 'sampling_method', 'batch', 'cell_type', 'donor', 'n_counts', 'batch_id'\n",
       "    var: 'gene_symbol', 'gene_name', 'id_in_vocab'\n",
       "    uns: 'log1p'\n",
       "    obsm: 'bin_edges', 'X_scGPT'\n",
       "    layers: 'X_normed', 'X_binned'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-28T08:09:14.595710Z",
     "iopub.status.busy": "2023-12-28T08:09:14.594889Z",
     "iopub.status.idle": "2023-12-28T08:09:26.450853Z",
     "shell.execute_reply": "2023-12-28T08:09:26.448789Z",
     "shell.execute_reply.started": "2023-12-28T08:09:14.595657Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "adata_result.write_h5ad(\"/nfs/public/cell_gpt_data/Intergation_Data/Lung/Results/scGPT_result.h5ad\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scGPT",
   "language": "python",
   "name": "scgpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
