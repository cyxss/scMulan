{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Fine-tuning on Pre-trained Model with Batch Integration\n",
    "In this tutorial, we demonstrate how to fine-tune a pre-trained model on a new dataset for the batch integration task. We use the PBMC 10K dataset as an example and fine-tune on the pre-trained whole-body model. \n",
    "\n",
    "We summarize the fine-tuning pipeline in the following steps, which can be used as a general recipe for finetuning on integration tasks and beyond: \n",
    "\n",
    "     1. Specify hyper-parameter setup for integration task\n",
    "     \n",
    "     2. Load and pre-process data\n",
    "     \n",
    "     3. Load the pre-trained scGPT model\n",
    "     \n",
    "     4. Finetune scGPT with task-specific objectives\n",
    "     \n",
    "     5. Evaluate fine-tuned scGPT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T07:20:28.641787Z",
     "iopub.status.busy": "2023-11-03T07:20:28.640954Z",
     "iopub.status.idle": "2023-11-03T07:20:28.653280Z",
     "shell.execute_reply": "2023-11-03T07:20:28.652566Z",
     "shell.execute_reply.started": "2023-11-03T07:20:28.641732Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['R_HOME'] = '/data1/chenyx/anaconda3/envs/scGPT/lib64/R'\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T07:20:29.471163Z",
     "iopub.status.busy": "2023-11-03T07:20:29.470360Z",
     "iopub.status.idle": "2023-11-03T07:20:29.754308Z",
     "shell.execute_reply": "2023-11-03T07:20:29.753791Z",
     "shell.execute_reply.started": "2023-11-03T07:20:29.471110Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T07:20:30.161082Z",
     "iopub.status.busy": "2023-11-03T07:20:30.159600Z",
     "iopub.status.idle": "2023-11-03T07:20:35.285338Z",
     "shell.execute_reply": "2023-11-03T07:20:35.284572Z",
     "shell.execute_reply.started": "2023-11-03T07:20:30.161025Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n",
      "/data1/chenyx/anaconda3/envs/scGPT/lib/python3.8/site-packages/scanpy/_settings.py:450: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  IPython.display.set_matplotlib_formats(*ipython_format)\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import gc\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import time\n",
    "import traceback\n",
    "from typing import List, Tuple, Dict, Union, Optional\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "from anndata import AnnData\n",
    "import scanpy as sc\n",
    "import scvi\n",
    "import numpy as np\n",
    "import wandb\n",
    "from scipy.sparse import issparse\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext._torchtext import (\n",
    "    Vocab as VocabPybind,\n",
    ")\n",
    "\n",
    "from scgpt.tokenizer.gene_tokenizer import GeneVocab\n",
    "\n",
    "sys.path.insert(0, \"../\")\n",
    "import scgpt as scg\n",
    "from scgpt.model import TransformerModel, AdversarialDiscriminator\n",
    "from scgpt.tokenizer import tokenize_and_pad_batch, random_mask_value\n",
    "from scgpt.loss import (\n",
    "    masked_mse_loss,\n",
    "    masked_relative_error,\n",
    "    criterion_neg_log_bernoulli,\n",
    ")\n",
    "from scgpt.preprocess import Preprocessor\n",
    "from scgpt import SubsetsBatchSampler\n",
    "from scgpt.utils import set_seed, category_str2int, eval_scib_metrics\n",
    "\n",
    "sc.set_figure_params(figsize=(4, 4))\n",
    "os.environ[\"KMP_WARNINGS\"] = \"off\"\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1: Specify hyper-parameter setup for integration task\n",
    "Here we provide some hyper-parameter recommendations here for the integration task. Note that the PBMC 10K dataset contains multiple batches to be integrated. Therefore, in addition to the default gene modelling objectives, we also turn on ESC, DAR and DSBN objectives specifically to faciliate batch integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T07:20:37.695906Z",
     "iopub.status.busy": "2023-11-03T07:20:37.694094Z",
     "iopub.status.idle": "2023-11-03T07:20:47.541133Z",
     "shell.execute_reply": "2023-11-03T07:20:47.540314Z",
     "shell.execute_reply.started": "2023-11-03T07:20:37.695838Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myixinchen\u001b[0m (\u001b[33mcellgenesis\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/chenyx/scGPT_test/Integration/wandb/run-20231103_152039-frno9dxs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cellgenesis/scGPT/runs/frno9dxs' target=\"_blank\">ethereal-water-38</a></strong> to <a href='https://wandb.ai/cellgenesis/scGPT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cellgenesis/scGPT' target=\"_blank\">https://wandb.ai/cellgenesis/scGPT</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cellgenesis/scGPT/runs/frno9dxs' target=\"_blank\">https://wandb.ai/cellgenesis/scGPT/runs/frno9dxs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'seed': 42, 'dataset_name': 'COVID', 'do_train': True, 'load_model': '../whole_human/', 'GEPC': True, 'ecs_thres': 0.8, 'dab_weight': 1.0, 'mask_ratio': 0.4, 'epochs': 15, 'n_bins': 51, 'lr': 0.0001, 'batch_size': 64, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 25, 'log_interval': 100, 'fast_transformer': True, 'pre_norm': False, 'amp': True}\n"
     ]
    }
   ],
   "source": [
    "hyperparameter_defaults = dict(\n",
    "    seed=42,\n",
    "    dataset_name=\"COVID\", # Dataset name\n",
    "    do_train=True, # Flag to indicate whether to do update model parameters during training\n",
    "    load_model=\"../whole_human/\", # Path to pre-trained model\n",
    "    GEPC=True,  # Gene expression modelling for cell objective\n",
    "    ecs_thres=0.8,  # Elastic cell similarity objective, 0.0 to 1.0, 0.0 to disable\n",
    "    dab_weight=1.0, # DAR objective weight for batch correction\n",
    "    mask_ratio=0.4, # Default mask ratio\n",
    "    epochs=15, # Default number of epochs for fine-tuning\n",
    "    n_bins=51, # Default number of bins for value binning in data pre-processing\n",
    "    lr=1e-4, # Default learning rate for fine-tuning\n",
    "    batch_size=64, # Default batch size for fine-tuning\n",
    "    layer_size=128,\n",
    "    nlayers=4,\n",
    "    nhead=4, # if load model, batch_size, layer_size, nlayers, nhead will be ignored\n",
    "    dropout=0.2, # Default dropout rate during model fine-tuning\n",
    "    schedule_ratio=0.9,  # Default rate for learning rate decay\n",
    "    save_eval_interval=25, # Default model evaluation interval\n",
    "    log_interval=100, # Default log interval\n",
    "    fast_transformer=True, # Default setting\n",
    "    pre_norm=False, # Default setting\n",
    "    amp=True,  # # Default setting: Automatic Mixed Precision\n",
    ")\n",
    "run = wandb.init(\n",
    "    config=hyperparameter_defaults,\n",
    "    project=\"scGPT\",\n",
    "    reinit=True,\n",
    "    settings=wandb.Settings(start_method=\"fork\"),\n",
    ")\n",
    "config = wandb.config\n",
    "print(config)\n",
    "\n",
    "set_seed(config.seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T07:20:58.973004Z",
     "iopub.status.busy": "2023-11-03T07:20:58.971712Z",
     "iopub.status.idle": "2023-11-03T07:20:58.982221Z",
     "shell.execute_reply": "2023-11-03T07:20:58.981287Z",
     "shell.execute_reply.started": "2023-11-03T07:20:58.972949Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# settings for input and preprocessing\n",
    "pad_token = \"<pad>\"\n",
    "special_tokens = [pad_token, \"<cls>\", \"<eoc>\"]\n",
    "mask_ratio = config.mask_ratio\n",
    "mask_value = -1\n",
    "pad_value = -2\n",
    "n_input_bins = config.n_bins\n",
    "\n",
    "n_hvg = 2000  # number of highly variable genes\n",
    "max_seq_len = n_hvg + 1\n",
    "per_seq_batch_sample = True\n",
    "DSBN = True  # Domain-spec batchnorm\n",
    "explicit_zero_prob = True  # whether explicit bernoulli for zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T07:20:59.967243Z",
     "iopub.status.busy": "2023-11-03T07:20:59.966413Z",
     "iopub.status.idle": "2023-11-03T07:20:59.977100Z",
     "shell.execute_reply": "2023-11-03T07:20:59.976446Z",
     "shell.execute_reply.started": "2023-11-03T07:20:59.967189Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save to save/dev_COVID-11月03-15-20\n"
     ]
    }
   ],
   "source": [
    "dataset_name = config.dataset_name\n",
    "save_dir = Path(f\"./save/dev_{dataset_name}-{time.strftime('%b%d-%H-%M')}/\")\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"save to {save_dir}\")\n",
    "logger = scg.logger\n",
    "scg.utils.add_file_handler(logger, save_dir / \"run.log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load and pre-process data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.1 Load the COVID data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T07:21:11.711080Z",
     "iopub.status.busy": "2023-11-03T07:21:11.710247Z",
     "iopub.status.idle": "2023-11-03T07:21:11.717382Z",
     "shell.execute_reply": "2023-11-03T07:21:11.716480Z",
     "shell.execute_reply.started": "2023-11-03T07:21:11.711027Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T07:21:11.719415Z",
     "iopub.status.busy": "2023-11-03T07:21:11.718778Z",
     "iopub.status.idle": "2023-11-03T07:21:16.171076Z",
     "shell.execute_reply": "2023-11-03T07:21:16.169941Z",
     "shell.execute_reply.started": "2023-11-03T07:21:11.719394Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "adata = sc.read_h5ad(\"/nfs/public/cell_gpt_data/Intergation_COVID/Results/raw.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T07:21:16.172451Z",
     "iopub.status.busy": "2023-11-03T07:21:16.172219Z",
     "iopub.status.idle": "2023-11-03T07:21:16.178666Z",
     "shell.execute_reply": "2023-11-03T07:21:16.178085Z",
     "shell.execute_reply.started": "2023-11-03T07:21:16.172429Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "adata.var[\"gene_name\"] = adata.var.index\n",
    "data_is_raw = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.2 Cross-check gene set with the pre-trained model \n",
    "Note that we retain the common gene set between the data and the pre-trained model for further fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T07:21:16.179655Z",
     "iopub.status.busy": "2023-11-03T07:21:16.179442Z",
     "iopub.status.idle": "2023-11-03T07:21:16.751380Z",
     "shell.execute_reply": "2023-11-03T07:21:16.750472Z",
     "shell.execute_reply.started": "2023-11-03T07:21:16.179637Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - match 1997/2000 genes in vocabulary of size 60697.\n",
      "scGPT - INFO - Resume model from ../whole_human/best_model.pt, the model args will be overriden by the config ../whole_human/args.json.\n"
     ]
    }
   ],
   "source": [
    "if config.load_model is not None:\n",
    "    model_dir = Path(config.load_model)\n",
    "    model_config_file = model_dir / \"args.json\"\n",
    "    model_file = model_dir / \"best_model.pt\"\n",
    "    vocab_file = model_dir / \"vocab.json\"\n",
    "\n",
    "    vocab = GeneVocab.from_file(vocab_file)\n",
    "    for s in special_tokens:\n",
    "        if s not in vocab:\n",
    "            vocab.append_token(s)\n",
    "\n",
    "    adata.var[\"id_in_vocab\"] = [\n",
    "        1 if gene in vocab else -1 for gene in adata.var[\"gene_name\"]\n",
    "    ]\n",
    "    gene_ids_in_vocab = np.array(adata.var[\"id_in_vocab\"])\n",
    "    logger.info(\n",
    "        f\"match {np.sum(gene_ids_in_vocab >= 0)}/{len(gene_ids_in_vocab)} genes \"\n",
    "        f\"in vocabulary of size {len(vocab)}.\"\n",
    "    )\n",
    "    adata = adata[:, adata.var[\"id_in_vocab\"] >= 0]\n",
    "    \n",
    "    # model\n",
    "    with open(model_config_file, \"r\") as f:\n",
    "        model_configs = json.load(f)\n",
    "    logger.info(\n",
    "        f\"Resume model from {model_file}, the model args will be overriden by the \"\n",
    "        f\"config {model_config_file}.\"\n",
    "    )\n",
    "    embsize = model_configs[\"embsize\"]\n",
    "    nhead = model_configs[\"nheads\"]\n",
    "    d_hid = model_configs[\"d_hid\"]\n",
    "    nlayers = model_configs[\"nlayers\"]\n",
    "    n_layers_cls = model_configs[\"n_layers_cls\"]\n",
    "else:\n",
    "    embsize = config.layer_size \n",
    "    nhead = config.nhead\n",
    "    nlayers = config.nlayers  \n",
    "    d_hid = config.layer_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.3 Pre-process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T07:21:16.753603Z",
     "iopub.status.busy": "2023-11-03T07:21:16.753378Z",
     "iopub.status.idle": "2023-11-03T07:22:19.962718Z",
     "shell.execute_reply": "2023-11-03T07:22:19.961698Z",
     "shell.execute_reply.started": "2023-11-03T07:21:16.753583Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - Filtering cells by counts ...\n",
      "scGPT - INFO - Normalizing total counts ...\n",
      "scGPT - INFO - Binning data ...\n"
     ]
    }
   ],
   "source": [
    "# set up the preprocessor, use the args to config the workflow\n",
    "preprocessor = Preprocessor(\n",
    "    use_key=\"X\",  # the key in adata.layers to use as raw data\n",
    "    filter_gene_by_counts=False,  # step 1\n",
    "    filter_cell_by_counts=False,  # step 2\n",
    "    normalize_total=1e4,  # 3. whether to normalize the raw data and to what sum\n",
    "    result_normed_key=\"X_normed\",  # the key in adata.layers to store the normalized data\n",
    "    log1p=data_is_raw,  # 4. whether to log1p the normalized data\n",
    "    result_log1p_key=\"X_log1p\",\n",
    "    subset_hvg=False,  # 5. whether to subset the raw data to highly variable genes\n",
    "    hvg_flavor=\"seurat_v3\" if data_is_raw else \"cell_ranger\",\n",
    "    binning=config.n_bins,  # 6. whether to bin the raw data and to what number of bins\n",
    "    result_binned_key=\"X_binned\",  # the key in adata.layers to store the binned data\n",
    ")\n",
    "preprocessor(adata, batch_key=\"str_batch\" if dataset_name != \"heart_cell\" else None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T07:22:19.963801Z",
     "iopub.status.busy": "2023-11-03T07:22:19.963628Z",
     "iopub.status.idle": "2023-11-03T07:22:19.970181Z",
     "shell.execute_reply": "2023-11-03T07:22:19.969678Z",
     "shell.execute_reply.started": "2023-11-03T07:22:19.963786Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 274346 × 1997\n",
       "    obs: 'age', 'age_range', 'anatomical_region', 'anatomical_region_detailed', 'batch', 'dataset', 'disease', 'donor', 'ethnicity', 'ethnicity_mixed', 'last_author/PI', 'lung_vs_nasal', 'original_celltype_ann', 'pack_years', 'sample', 'sample_alias', 'sample_type', 'sampling_method', 'sex', 'smoking', 'total_counts', 'log10_total_counts', 'n_genes_detected', 'mito_frac', 'ribo_frac', 'compl', 'ann_level_1', 'ann_level_2', 'ann_level_3', 'ann_level_4', 'ann_level_5', 'ann_highest_res', 'ann_new', 'subject_type', 'study', 'study2', 'celltype', 'condition', 'cellnames', 'cluster', 'stage', 'ID', 'sample_new', 'chemistry', 'data_type', 'dpt_pseudotime', 'final_annotation', 'mt_frac', 'n_counts', 'n_genes', 'sample_ID', 'size_factors', 'species', 'tissue'\n",
       "    var: 'gene_name', 'id_in_vocab'\n",
       "    uns: 'log1p'\n",
       "    obsm: 'bin_edges'\n",
       "    layers: 'X_normed', 'X_binned'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T07:22:19.971041Z",
     "iopub.status.busy": "2023-11-03T07:22:19.970860Z",
     "iopub.status.idle": "2023-11-03T07:22:19.999752Z",
     "shell.execute_reply": "2023-11-03T07:22:19.999272Z",
     "shell.execute_reply.started": "2023-11-03T07:22:19.971026Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "adata.obs['batch_id'] = pd.factorize(adata.obs['study'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T07:22:20.000495Z",
     "iopub.status.busy": "2023-11-03T07:22:20.000349Z",
     "iopub.status.idle": "2023-11-03T07:22:23.751759Z",
     "shell.execute_reply": "2023-11-03T07:22:23.750868Z",
     "shell.execute_reply.started": "2023-11-03T07:22:20.000483Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if per_seq_batch_sample:\n",
    "    # sort the adata by batch_id in advance\n",
    "    adata_sorted = adata[adata.obs[\"batch_id\"].argsort()].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Tokenize the input data for model fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T07:22:23.752749Z",
     "iopub.status.busy": "2023-11-03T07:22:23.752581Z",
     "iopub.status.idle": "2023-11-03T07:22:25.103633Z",
     "shell.execute_reply": "2023-11-03T07:22:25.101787Z",
     "shell.execute_reply.started": "2023-11-03T07:22:23.752735Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_layer_key = \"X_binned\"\n",
    "all_counts = (\n",
    "    adata.layers[input_layer_key].A\n",
    "    if issparse(adata.layers[input_layer_key])\n",
    "    else adata.layers[input_layer_key]\n",
    ")\n",
    "genes = adata.var[\"gene_name\"].tolist()\n",
    "\n",
    "celltypes_labels = adata.obs[\"celltype\"].tolist()  # make sure count from 0\n",
    "num_types = len(set(celltypes_labels))\n",
    "celltypes_labels = np.array(celltypes_labels)\n",
    "\n",
    "batch_ids = adata.obs[\"batch_id\"].tolist()\n",
    "num_batch_types = len(set(batch_ids))\n",
    "batch_ids = np.array(batch_ids)\n",
    "\n",
    "(\n",
    "    train_data,\n",
    "    valid_data,\n",
    "    train_celltype_labels,\n",
    "    valid_celltype_labels,\n",
    "    train_batch_labels,\n",
    "    valid_batch_labels,\n",
    ") = train_test_split(\n",
    "    all_counts, celltypes_labels, batch_ids, test_size=0.1, shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T07:22:25.105937Z",
     "iopub.status.busy": "2023-11-03T07:22:25.105599Z",
     "iopub.status.idle": "2023-11-03T07:22:25.118207Z",
     "shell.execute_reply": "2023-11-03T07:22:25.116776Z",
     "shell.execute_reply.started": "2023-11-03T07:22:25.105912Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if config.load_model is None:\n",
    "    vocab = Vocab(\n",
    "        VocabPybind(genes + special_tokens, None)\n",
    "    )  # bidirectional lookup [gene <-> int]\n",
    "vocab.set_default_index(vocab[\"<pad>\"])\n",
    "gene_ids = np.array(vocab(genes), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T07:22:25.119747Z",
     "iopub.status.busy": "2023-11-03T07:22:25.119466Z",
     "iopub.status.idle": "2023-11-03T07:22:49.144917Z",
     "shell.execute_reply": "2023-11-03T07:22:49.144021Z",
     "shell.execute_reply.started": "2023-11-03T07:22:25.119715Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - train set number of samples: 246911, \n",
      "\t feature length: 2001\n",
      "scGPT - INFO - valid set number of samples: 27435, \n",
      "\t feature length: 2001\n"
     ]
    }
   ],
   "source": [
    "tokenized_train = tokenize_and_pad_batch(\n",
    "    train_data,\n",
    "    gene_ids,\n",
    "    max_len=max_seq_len,\n",
    "    vocab=vocab,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    append_cls=True,  # append <cls> token at the beginning\n",
    "    include_zero_gene=True,\n",
    ")\n",
    "tokenized_valid = tokenize_and_pad_batch(\n",
    "    valid_data,\n",
    "    gene_ids,\n",
    "    max_len=max_seq_len,\n",
    "    vocab=vocab,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    append_cls=True,\n",
    "    include_zero_gene=True,\n",
    ")\n",
    "logger.info(\n",
    "    f\"train set number of samples: {tokenized_train['genes'].shape[0]}, \"\n",
    "    f\"\\n\\t feature length: {tokenized_train['genes'].shape[1]}\"\n",
    ")\n",
    "logger.info(\n",
    "    f\"valid set number of samples: {tokenized_valid['genes'].shape[0]}, \"\n",
    "    f\"\\n\\t feature length: {tokenized_valid['genes'].shape[1]}\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T07:22:49.146270Z",
     "iopub.status.busy": "2023-11-03T07:22:49.146107Z",
     "iopub.status.idle": "2023-11-03T07:22:49.159887Z",
     "shell.execute_reply": "2023-11-03T07:22:49.159141Z",
     "shell.execute_reply.started": "2023-11-03T07:22:49.146255Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_data(sort_seq_batch=False) -> Tuple[Dict[str, torch.Tensor]]:\n",
    "    masked_values_train = random_mask_value(\n",
    "        tokenized_train[\"values\"],\n",
    "        mask_ratio=mask_ratio,\n",
    "        mask_value=mask_value,\n",
    "        pad_value=pad_value,\n",
    "    )\n",
    "    masked_values_valid = random_mask_value(\n",
    "        tokenized_valid[\"values\"],\n",
    "        mask_ratio=mask_ratio,\n",
    "        mask_value=mask_value,\n",
    "        pad_value=pad_value,\n",
    "    )\n",
    "    print(\n",
    "        f\"random masking at epoch {epoch:3d}, ratio of masked values in train: \",\n",
    "        f\"{(masked_values_train == mask_value).sum() / (masked_values_train - pad_value).count_nonzero():.4f}\",\n",
    "    )\n",
    "\n",
    "    input_gene_ids_train, input_gene_ids_valid = (\n",
    "        tokenized_train[\"genes\"],\n",
    "        tokenized_valid[\"genes\"],\n",
    "    )\n",
    "    input_values_train, input_values_valid = masked_values_train, masked_values_valid\n",
    "    target_values_train, target_values_valid = (\n",
    "        tokenized_train[\"values\"],\n",
    "        tokenized_valid[\"values\"],\n",
    "    )\n",
    "\n",
    "    tensor_batch_labels_train = torch.from_numpy(train_batch_labels).long()\n",
    "    tensor_batch_labels_valid = torch.from_numpy(valid_batch_labels).long()\n",
    "\n",
    "    if sort_seq_batch:\n",
    "        train_sort_ids = np.argsort(train_batch_labels)\n",
    "        input_gene_ids_train = input_gene_ids_train[train_sort_ids]\n",
    "        input_values_train = input_values_train[train_sort_ids]\n",
    "        target_values_train = target_values_train[train_sort_ids]\n",
    "        tensor_batch_labels_train = tensor_batch_labels_train[train_sort_ids]\n",
    "\n",
    "        valid_sort_ids = np.argsort(valid_batch_labels)\n",
    "        input_gene_ids_valid = input_gene_ids_valid[valid_sort_ids]\n",
    "        input_values_valid = input_values_valid[valid_sort_ids]\n",
    "        target_values_valid = target_values_valid[valid_sort_ids]\n",
    "        tensor_batch_labels_valid = tensor_batch_labels_valid[valid_sort_ids]\n",
    "\n",
    "    train_data_pt = {\n",
    "        \"gene_ids\": input_gene_ids_train,\n",
    "        \"values\": input_values_train,\n",
    "        \"target_values\": target_values_train,\n",
    "        \"batch_labels\": tensor_batch_labels_train,\n",
    "    }\n",
    "    valid_data_pt = {\n",
    "        \"gene_ids\": input_gene_ids_valid,\n",
    "        \"values\": input_values_valid,\n",
    "        \"target_values\": target_values_valid,\n",
    "        \"batch_labels\": tensor_batch_labels_valid,\n",
    "    }\n",
    "\n",
    "    return train_data_pt, valid_data_pt\n",
    "\n",
    "\n",
    "# dataset\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, data: Dict[str, torch.Tensor]):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data[\"gene_ids\"].shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {k: v[idx] for k, v in self.data.items()}\n",
    "\n",
    "\n",
    "# data_loader\n",
    "def prepare_dataloader(\n",
    "    data_pt: Dict[str, torch.Tensor],\n",
    "    batch_size: int,\n",
    "    shuffle: bool = False,\n",
    "    intra_domain_shuffle: bool = False,\n",
    "    drop_last: bool = False,\n",
    "    num_workers: int = 0,\n",
    ") -> DataLoader:\n",
    "    dataset = SeqDataset(data_pt)\n",
    "\n",
    "    if per_seq_batch_sample:\n",
    "        # find the indices of samples in each seq batch\n",
    "        subsets = []\n",
    "        batch_labels_array = data_pt[\"batch_labels\"].numpy()\n",
    "        for batch_label in np.unique(batch_labels_array):\n",
    "            batch_indices = np.where(batch_labels_array == batch_label)[0].tolist()\n",
    "            subsets.append(batch_indices)\n",
    "        data_loader = DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_sampler=SubsetsBatchSampler(\n",
    "                subsets,\n",
    "                batch_size,\n",
    "                intra_subset_shuffle=intra_domain_shuffle,\n",
    "                inter_subset_shuffle=shuffle,\n",
    "                drop_last=drop_last,\n",
    "            ),\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        return data_loader\n",
    "\n",
    "    data_loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Step 3: Load the pre-trained scGPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-11-03T07:22:49.160670Z",
     "iopub.status.busy": "2023-11-03T07:22:49.160531Z",
     "iopub.status.idle": "2023-11-03T07:22:53.670365Z",
     "shell.execute_reply": "2023-11-03T07:22:53.669815Z",
     "shell.execute_reply.started": "2023-11-03T07:22:49.160657Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use domain specific batchnorm with affine=False\n",
      "scGPT - INFO - Loading params encoder.embedding.weight with shape torch.Size([60697, 512])\n",
      "scGPT - INFO - Loading params encoder.enc_norm.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params encoder.enc_norm.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.linear1.weight with shape torch.Size([512, 1])\n",
      "scGPT - INFO - Loading params value_encoder.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params value_encoder.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.norm.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.norm.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params decoder.fc.0.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params decoder.fc.2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params decoder.fc.2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params decoder.fc.4.weight with shape torch.Size([1, 512])\n",
      "scGPT - INFO - Loading params decoder.fc.4.bias with shape torch.Size([1])\n",
      "scGPT - INFO - Loading params mvc_decoder.gene2query.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params mvc_decoder.gene2query.bias with shape torch.Size([512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "ntokens = len(vocab)  # size of vocabulary\n",
    "model = TransformerModel(\n",
    "    ntokens,\n",
    "    embsize,\n",
    "    nhead,\n",
    "    d_hid,\n",
    "    nlayers,\n",
    "    vocab=vocab,\n",
    "    dropout=config.dropout,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    do_mvc=config.GEPC,\n",
    "    do_dab=True,\n",
    "    use_batch_labels=True,\n",
    "    num_batch_labels=num_batch_types,\n",
    "    domain_spec_batchnorm=DSBN,\n",
    "    n_input_bins=n_input_bins,\n",
    "    ecs_threshold=config.ecs_thres,\n",
    "    explicit_zero_prob=explicit_zero_prob,\n",
    "    use_fast_transformer=config.fast_transformer,\n",
    "    pre_norm=config.pre_norm,\n",
    ")\n",
    "if config.load_model is not None:\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(model_file))\n",
    "        logger.info(f\"Loading all model params from {model_file}\")\n",
    "    except:\n",
    "        # only load params that are in the model and match the size\n",
    "        model_dict = model.state_dict()\n",
    "        pretrained_dict = torch.load(model_file)\n",
    "        pretrained_dict = {\n",
    "            k: v\n",
    "            for k, v in pretrained_dict.items()\n",
    "            if k in model_dict and v.shape == model_dict[k].shape\n",
    "        }\n",
    "        for k, v in pretrained_dict.items():\n",
    "            logger.info(f\"Loading params {k} with shape {v.shape}\")\n",
    "        model_dict.update(pretrained_dict)\n",
    "        model.load_state_dict(model_dict)\n",
    "\n",
    "model.to(device)\n",
    "wandb.watch(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T07:22:53.672308Z",
     "iopub.status.busy": "2023-11-03T07:22:53.672067Z",
     "iopub.status.idle": "2023-11-03T07:22:53.676930Z",
     "shell.execute_reply": "2023-11-03T07:22:53.676378Z",
     "shell.execute_reply.started": "2023-11-03T07:22:53.672292Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion = masked_mse_loss\n",
    "criterion_dab = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=config.lr, eps=1e-4 if config.amp else 1e-8\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=config.schedule_ratio)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=config.amp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T07:22:53.677872Z",
     "iopub.status.busy": "2023-11-03T07:22:53.677710Z",
     "iopub.status.idle": "2023-11-03T07:22:53.702378Z",
     "shell.execute_reply": "2023-11-03T07:22:53.701682Z",
     "shell.execute_reply.started": "2023-11-03T07:22:53.677859Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model: nn.Module, loader: DataLoader) -> None:\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss, total_mse, total_gepc = 0.0, 0.0, 0.0\n",
    "    total_error = 0.0\n",
    "    log_interval = config.log_interval\n",
    "    start_time = time.time()\n",
    "\n",
    "    num_batches = len(loader)\n",
    "    for batch, batch_data in enumerate(loader):\n",
    "        input_gene_ids = batch_data[\"gene_ids\"].to(device)\n",
    "        input_values = batch_data[\"values\"].to(device)\n",
    "        target_values = batch_data[\"target_values\"].to(device)\n",
    "        batch_labels = batch_data[\"batch_labels\"].to(device)\n",
    "\n",
    "        src_key_padding_mask = input_gene_ids.eq(vocab[pad_token])\n",
    "        with torch.cuda.amp.autocast(enabled=config.amp):\n",
    "            output_dict = model(\n",
    "                input_gene_ids,\n",
    "                input_values,\n",
    "                src_key_padding_mask=src_key_padding_mask,\n",
    "                batch_labels=batch_labels if DSBN else None,\n",
    "                MVC=config.GEPC,\n",
    "                ECS=config.ecs_thres > 0,\n",
    "            )\n",
    "\n",
    "            masked_positions = input_values.eq(mask_value)  # the postions to predict\n",
    "            loss = loss_mse = criterion(\n",
    "                output_dict[\"mlm_output\"], target_values, masked_positions\n",
    "            )\n",
    "            metrics_to_log = {\"train/mse\": loss_mse.item()}\n",
    "            if explicit_zero_prob:\n",
    "                loss_zero_log_prob = criterion_neg_log_bernoulli(\n",
    "                    output_dict[\"mlm_zero_probs\"], target_values, masked_positions\n",
    "                )\n",
    "                loss = loss + loss_zero_log_prob\n",
    "                metrics_to_log.update({\"train/nzlp\": loss_zero_log_prob.item()})\n",
    "            if config.GEPC:\n",
    "                loss_gepc = criterion(\n",
    "                    output_dict[\"mvc_output\"], target_values, masked_positions\n",
    "                )\n",
    "                loss = loss + loss_gepc\n",
    "                metrics_to_log.update({\"train/mvc\": loss_gepc.item()})\n",
    "            if config.GEPC and explicit_zero_prob:\n",
    "                loss_gepc_zero_log_prob = criterion_neg_log_bernoulli(\n",
    "                    output_dict[\"mvc_zero_probs\"], target_values, masked_positions\n",
    "                )\n",
    "                loss = loss + loss_gepc_zero_log_prob\n",
    "                metrics_to_log.update(\n",
    "                    {\"train/mvc_nzlp\": loss_gepc_zero_log_prob.item()}\n",
    "                )\n",
    "            if config.ecs_thres > 0:\n",
    "                loss_ecs = 10 * output_dict[\"loss_ecs\"]\n",
    "                loss = loss + loss_ecs\n",
    "                metrics_to_log.update({\"train/ecs\": loss_ecs.item()})\n",
    "            loss_dab = criterion_dab(output_dict[\"dab_output\"], batch_labels)\n",
    "            loss = loss + config.dab_weight * loss_dab\n",
    "            metrics_to_log.update({\"train/dab\": loss_dab.item()})\n",
    "\n",
    "        model.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        with warnings.catch_warnings(record=True) as w:\n",
    "            warnings.filterwarnings(\"always\")\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                model.parameters(),\n",
    "                1.0,\n",
    "                error_if_nonfinite=False if scaler.is_enabled() else True,\n",
    "            )\n",
    "            if len(w) > 0:\n",
    "                logger.warning(\n",
    "                    f\"Found infinite gradient. This may be caused by the gradient \"\n",
    "                    f\"scaler. The current scale is {scaler.get_scale()}. This warning \"\n",
    "                    \"can be ignored if no longer occurs after autoscaling of the scaler.\"\n",
    "                )\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        wandb.log(metrics_to_log)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            mre = masked_relative_error(\n",
    "                output_dict[\"mlm_output\"], target_values, masked_positions\n",
    "            )\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_mse += loss_mse.item()\n",
    "        total_gepc += loss_gepc.item() if config.GEPC else 0.0\n",
    "        total_error += mre.item()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            cur_mse = total_mse / log_interval\n",
    "            cur_gepc = total_gepc / log_interval if config.GEPC else 0.0\n",
    "            cur_error = total_error / log_interval\n",
    "            # ppl = math.exp(cur_loss)\n",
    "            logger.info(\n",
    "                f\"| epoch {epoch:3d} | {batch:3d}/{num_batches:3d} batches | \"\n",
    "                f\"lr {lr:05.4f} | ms/batch {ms_per_batch:5.2f} | \"\n",
    "                f\"loss {cur_loss:5.2f} | mse {cur_mse:5.2f} | mre {cur_error:5.2f} |\"\n",
    "                + (f\"gepc {cur_gepc:5.2f} |\" if config.GEPC else \"\")\n",
    "            )\n",
    "            total_loss = 0\n",
    "            total_mse = 0\n",
    "            total_gepc = 0\n",
    "            total_error = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "\n",
    "def define_wandb_metrcis():\n",
    "    wandb.define_metric(\"valid/mse\", summary=\"min\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"valid/mre\", summary=\"min\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"valid/dab\", summary=\"min\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"valid/sum_mse_dab\", summary=\"min\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"test/avg_bio\", summary=\"max\")\n",
    "\n",
    "\n",
    "def evaluate(model: nn.Module, loader: DataLoader) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate the model on the evaluation data.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_error = 0.0\n",
    "    total_dab = 0.0\n",
    "    total_num = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_data in loader:\n",
    "            input_gene_ids = batch_data[\"gene_ids\"].to(device)\n",
    "            input_values = batch_data[\"values\"].to(device)\n",
    "            target_values = batch_data[\"target_values\"].to(device)\n",
    "            batch_labels = batch_data[\"batch_labels\"].to(device)\n",
    "\n",
    "            src_key_padding_mask = input_gene_ids.eq(vocab[pad_token])\n",
    "            with torch.cuda.amp.autocast(enabled=config.amp):\n",
    "                output_dict = model(\n",
    "                    input_gene_ids,\n",
    "                    input_values,\n",
    "                    src_key_padding_mask=src_key_padding_mask,\n",
    "                    batch_labels=batch_labels if DSBN else None,\n",
    "                )\n",
    "                output_values = output_dict[\"mlm_output\"]\n",
    "\n",
    "                masked_positions = input_values.eq(mask_value)\n",
    "                loss = criterion(output_values, target_values, masked_positions)\n",
    "                loss_dab = criterion_dab(output_dict[\"dab_output\"], batch_labels)\n",
    "\n",
    "            total_loss += loss.item() * len(input_gene_ids)\n",
    "            total_error += masked_relative_error(\n",
    "                output_values, target_values, masked_positions\n",
    "            ).item() * len(input_gene_ids)\n",
    "            total_dab += loss_dab.item() * len(input_gene_ids)\n",
    "            total_num += len(input_gene_ids)\n",
    "\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"valid/mse\": total_loss / total_num,\n",
    "            \"valid/mre\": total_error / total_num,\n",
    "            \"valid/dab\": total_dab / total_num,\n",
    "            \"valid/sum_mse_dab\": (total_loss + config.dab_weight * total_dab)\n",
    "            / total_num,\n",
    "            \"epoch\": epoch,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return total_loss / total_num, total_error / total_num\n",
    "\n",
    "\n",
    "def eval_testdata(\n",
    "    model: nn.Module,\n",
    "    adata_t: AnnData,\n",
    "    include_types: List[str] = [\"cls\"],\n",
    ") -> Optional[Dict]:\n",
    "    \"\"\"evaluate the model on test dataset of adata_t\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # copy adata_t to avoid reuse previously computed results stored in adata_t\n",
    "    adata_t = adata_t.copy()\n",
    "\n",
    "    all_counts = (\n",
    "        adata_t.layers[input_layer_key].A\n",
    "        if issparse(adata_t.layers[input_layer_key])\n",
    "        else adata_t.layers[input_layer_key]\n",
    "    )\n",
    "\n",
    "    celltypes_labels = adata_t.obs[\"celltype\"].tolist()\n",
    "    celltypes_labels = np.array(celltypes_labels)\n",
    "\n",
    "    batch_ids = adata_t.obs[\"batch_id\"].tolist()\n",
    "    batch_ids = np.array(batch_ids)\n",
    "\n",
    "    # Evaluate cls cell embeddings\n",
    "    if \"cls\" in include_types:\n",
    "        logger.info(\"Evaluating cls cell embeddings\")\n",
    "        tokenized_all = tokenize_and_pad_batch(\n",
    "            all_counts,\n",
    "            gene_ids,\n",
    "            max_len=max_seq_len,\n",
    "            vocab=vocab,\n",
    "            pad_token=pad_token,\n",
    "            pad_value=pad_value,\n",
    "            append_cls=True,  # append <cls> token at the beginning\n",
    "            include_zero_gene=True,\n",
    "        )\n",
    "        all_gene_ids, all_values = tokenized_all[\"genes\"], tokenized_all[\"values\"]\n",
    "        src_key_padding_mask = all_gene_ids.eq(vocab[pad_token])\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast(enabled=config.amp):\n",
    "            cell_embeddings = model.encode_batch(\n",
    "                all_gene_ids,\n",
    "                all_values.float(),\n",
    "                src_key_padding_mask=src_key_padding_mask,\n",
    "                batch_size=config.batch_size,\n",
    "                batch_labels=torch.from_numpy(batch_ids).long() if DSBN else None,\n",
    "                time_step=0,\n",
    "                return_np=True,\n",
    "            )\n",
    "        cell_embeddings = cell_embeddings / np.linalg.norm(\n",
    "            cell_embeddings, axis=1, keepdims=True\n",
    "        )\n",
    "\n",
    "        adata_t.obsm[\"X_scGPT\"] = cell_embeddings\n",
    "\n",
    "        results = {}\n",
    "        try:\n",
    "            results = eval_scib_metrics(adata_t)\n",
    "        except Exception as e:\n",
    "            traceback.print_exc()\n",
    "            logger.error(e)\n",
    "\n",
    "        sc.pp.neighbors(adata_t, use_rep=\"X_scGPT\")\n",
    "        sc.tl.umap(adata_t, min_dist=0.3)\n",
    "        fig = sc.pl.umap(\n",
    "            adata_t,\n",
    "            color=[\"str_batch\"],\n",
    "            title=[f\"batch, avg_bio = {results.get('avg_bio', 0.0):.4f}\"],\n",
    "            frameon=False,\n",
    "            return_fig=True,\n",
    "            show=False,\n",
    "        )\n",
    "\n",
    "        results[\"batch_umap\"] = fig\n",
    "\n",
    "        sc.pp.neighbors(adata_t, use_rep=\"X_scGPT\")\n",
    "        sc.tl.umap(adata_t, min_dist=0.3)\n",
    "        fig = sc.pl.umap(\n",
    "            adata_t,\n",
    "            color=[\"celltype\"],\n",
    "            title=[\n",
    "                f\"celltype, avg_bio = {results.get('avg_bio', 0.0):.4f}\",\n",
    "            ],\n",
    "            frameon=False,\n",
    "            return_fig=True,\n",
    "            show=False,\n",
    "        )\n",
    "\n",
    "        results[\"celltype_umap\"] = fig\n",
    "\n",
    "    if len(include_types) == 1:\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Finetune scGPT with task-specific objectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T07:22:53.703178Z",
     "iopub.status.busy": "2023-11-03T07:22:53.703042Z",
     "iopub.status.idle": "2023-11-03T18:27:16.720374Z",
     "shell.execute_reply": "2023-11-03T18:27:16.719329Z",
     "shell.execute_reply.started": "2023-11-03T07:22:53.703166Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random masking at epoch   1, ratio of masked values in train:  0.3999\n",
      "scGPT - INFO - | epoch   1 | 100/3867 batches | lr 0.0001 | ms/batch 623.10 | loss 179.63 | mse 92.00 | mre 2711274.65 |gepc 75.57 |\n",
      "scGPT - INFO - | epoch   1 | 200/3867 batches | lr 0.0001 | ms/batch 619.29 | loss 89.56 | mse 34.60 | mre 1394193.71 |gepc 43.56 |\n",
      "scGPT - INFO - | epoch   1 | 300/3867 batches | lr 0.0001 | ms/batch 619.81 | loss 70.64 | mse 24.30 | mre 953312.13 |gepc 35.59 |\n",
      "scGPT - INFO - | epoch   1 | 400/3867 batches | lr 0.0001 | ms/batch 621.07 | loss 83.32 | mse 27.59 | mre 1067105.44 |gepc 43.37 |\n",
      "scGPT - INFO - | epoch   1 | 500/3867 batches | lr 0.0001 | ms/batch 618.81 | loss 78.91 | mse 28.50 | mre 1021975.41 |gepc 38.30 |\n",
      "scGPT - INFO - | epoch   1 | 600/3867 batches | lr 0.0001 | ms/batch 621.30 | loss 67.40 | mse 26.40 | mre 870792.13 |gepc 30.75 |\n",
      "scGPT - INFO - | epoch   1 | 700/3867 batches | lr 0.0001 | ms/batch 621.68 | loss 65.77 | mse 26.01 | mre 857574.26 |gepc 29.56 |\n",
      "scGPT - INFO - | epoch   1 | 800/3867 batches | lr 0.0001 | ms/batch 615.84 | loss 65.61 | mse 25.85 | mre 850728.01 |gepc 29.60 |\n",
      "scGPT - INFO - | epoch   1 | 900/3867 batches | lr 0.0001 | ms/batch 616.00 | loss 64.39 | mse 25.37 | mre 832096.58 |gepc 28.87 |\n",
      "scGPT - INFO - | epoch   1 | 1000/3867 batches | lr 0.0001 | ms/batch 616.93 | loss 73.24 | mse 27.19 | mre 938241.45 |gepc 34.74 |\n",
      "scGPT - INFO - | epoch   1 | 1100/3867 batches | lr 0.0001 | ms/batch 616.69 | loss 68.46 | mse 25.88 | mre 848895.62 |gepc 32.36 |\n",
      "scGPT - INFO - | epoch   1 | 1200/3867 batches | lr 0.0001 | ms/batch 618.26 | loss 66.90 | mse 25.53 | mre 831862.31 |gepc 31.23 |\n",
      "scGPT - INFO - | epoch   1 | 1300/3867 batches | lr 0.0001 | ms/batch 619.35 | loss 65.69 | mse 25.36 | mre 827352.86 |gepc 30.59 |\n",
      "scGPT - INFO - | epoch   1 | 1400/3867 batches | lr 0.0001 | ms/batch 623.44 | loss 64.92 | mse 25.25 | mre 842460.06 |gepc 30.66 |\n",
      "scGPT - INFO - | epoch   1 | 1500/3867 batches | lr 0.0001 | ms/batch 614.11 | loss 62.65 | mse 22.19 | mre 727660.48 |gepc 29.26 |\n",
      "scGPT - INFO - | epoch   1 | 1600/3867 batches | lr 0.0001 | ms/batch 619.33 | loss 58.39 | mse 21.46 | mre 669330.79 |gepc 26.88 |\n",
      "scGPT - INFO - | epoch   1 | 1700/3867 batches | lr 0.0001 | ms/batch 617.74 | loss 57.17 | mse 21.17 | mre 651090.93 |gepc 25.98 |\n",
      "scGPT - INFO - | epoch   1 | 1800/3867 batches | lr 0.0001 | ms/batch 618.26 | loss 56.41 | mse 20.92 | mre 644194.49 |gepc 25.48 |\n",
      "scGPT - INFO - | epoch   1 | 1900/3867 batches | lr 0.0001 | ms/batch 623.23 | loss 56.09 | mse 20.90 | mre 643557.00 |gepc 25.18 |\n",
      "scGPT - INFO - | epoch   1 | 2000/3867 batches | lr 0.0001 | ms/batch 653.45 | loss 55.66 | mse 20.78 | mre 637664.32 |gepc 24.87 |\n",
      "scGPT - INFO - | epoch   1 | 2100/3867 batches | lr 0.0001 | ms/batch 641.83 | loss 55.76 | mse 20.93 | mre 642945.69 |gepc 24.82 |\n",
      "scGPT - INFO - | epoch   1 | 2200/3867 batches | lr 0.0001 | ms/batch 640.01 | loss 54.79 | mse 20.48 | mre 630825.32 |gepc 24.30 |\n",
      "scGPT - INFO - | epoch   1 | 2300/3867 batches | lr 0.0001 | ms/batch 637.55 | loss 55.02 | mse 20.65 | mre 631186.64 |gepc 24.37 |\n",
      "scGPT - INFO - | epoch   1 | 2400/3867 batches | lr 0.0001 | ms/batch 615.88 | loss 54.40 | mse 20.38 | mre 625009.08 |gepc 24.02 |\n",
      "scGPT - INFO - | epoch   1 | 2500/3867 batches | lr 0.0001 | ms/batch 620.69 | loss 53.97 | mse 20.49 | mre 642910.44 |gepc 23.46 |\n",
      "scGPT - INFO - | epoch   1 | 2600/3867 batches | lr 0.0001 | ms/batch 629.74 | loss 75.32 | mse 27.07 | mre 903100.20 |gepc 36.29 |\n",
      "scGPT - INFO - | epoch   1 | 2700/3867 batches | lr 0.0001 | ms/batch 617.29 | loss 67.70 | mse 25.98 | mre 850130.84 |gepc 33.12 |\n",
      "scGPT - INFO - | epoch   1 | 2800/3867 batches | lr 0.0001 | ms/batch 618.30 | loss 62.95 | mse 25.40 | mre 826672.71 |gepc 30.75 |\n",
      "scGPT - INFO - | epoch   1 | 2900/3867 batches | lr 0.0001 | ms/batch 614.98 | loss 59.93 | mse 24.79 | mre 796402.47 |gepc 28.81 |\n",
      "scGPT - INFO - | epoch   1 | 3000/3867 batches | lr 0.0001 | ms/batch 620.58 | loss 58.48 | mse 24.22 | mre 773669.29 |gepc 28.15 |\n",
      "scGPT - INFO - | epoch   1 | 3100/3867 batches | lr 0.0001 | ms/batch 615.36 | loss 57.18 | mse 23.83 | mre 762739.02 |gepc 27.44 |\n",
      "scGPT - INFO - | epoch   1 | 3200/3867 batches | lr 0.0001 | ms/batch 644.78 | loss 56.75 | mse 23.71 | mre 752356.23 |gepc 27.29 |\n",
      "scGPT - INFO - | epoch   1 | 3300/3867 batches | lr 0.0001 | ms/batch 623.70 | loss 56.17 | mse 23.50 | mre 740153.05 |gepc 27.02 |\n",
      "scGPT - INFO - | epoch   1 | 3400/3867 batches | lr 0.0001 | ms/batch 614.85 | loss 55.66 | mse 23.35 | mre 742608.15 |gepc 26.74 |\n",
      "scGPT - INFO - | epoch   1 | 3500/3867 batches | lr 0.0001 | ms/batch 603.27 | loss 68.24 | mse 23.76 | mre 868943.25 |gepc 33.20 |\n",
      "scGPT - INFO - | epoch   1 | 3600/3867 batches | lr 0.0001 | ms/batch 606.72 | loss 62.21 | mse 21.02 | mre 712073.66 |gepc 31.55 |\n",
      "scGPT - INFO - | epoch   1 | 3700/3867 batches | lr 0.0001 | ms/batch 632.53 | loss 59.94 | mse 23.91 | mre 760086.86 |gepc 29.37 |\n",
      "scGPT - INFO - | epoch   1 | 3800/3867 batches | lr 0.0001 | ms/batch 649.36 | loss 59.42 | mse 20.65 | mre 707374.43 |gepc 29.63 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   1 | time: 2492.25s | valid loss/mse 27.7957 | mre 556192.5924\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 27.7957\n",
      "random masking at epoch   2, ratio of masked values in train:  0.3999\n",
      "scGPT - INFO - | epoch   2 | 100/3867 batches | lr 0.0001 | ms/batch 640.67 | loss 64.36 | mse 24.44 | mre 837488.50 |gepc 30.37 |\n",
      "scGPT - INFO - | epoch   2 | 200/3867 batches | lr 0.0001 | ms/batch 621.25 | loss 57.03 | mse 22.03 | mre 752283.00 |gepc 26.94 |\n",
      "scGPT - INFO - | epoch   2 | 300/3867 batches | lr 0.0001 | ms/batch 615.11 | loss 51.68 | mse 20.00 | mre 675535.34 |gepc 25.73 |\n",
      "scGPT - INFO - | epoch   2 | 400/3867 batches | lr 0.0001 | ms/batch 631.84 | loss 62.15 | mse 23.31 | mre 780169.44 |gepc 29.90 |\n",
      "scGPT - INFO - | epoch   2 | 500/3867 batches | lr 0.0001 | ms/batch 622.00 | loss 63.34 | mse 25.61 | mre 817664.22 |gepc 30.10 |\n",
      "scGPT - INFO - | epoch   2 | 600/3867 batches | lr 0.0001 | ms/batch 624.97 | loss 58.54 | mse 25.03 | mre 783863.58 |gepc 27.61 |\n",
      "scGPT - INFO - | epoch   2 | 700/3867 batches | lr 0.0001 | ms/batch 629.50 | loss 57.32 | mse 24.84 | mre 778365.50 |gepc 26.93 |\n",
      "scGPT - INFO - | epoch   2 | 800/3867 batches | lr 0.0001 | ms/batch 625.09 | loss 56.51 | mse 24.60 | mre 774966.71 |gepc 26.56 |\n",
      "scGPT - INFO - | epoch   2 | 900/3867 batches | lr 0.0001 | ms/batch 642.65 | loss 56.12 | mse 24.51 | mre 760276.92 |gepc 26.33 |\n",
      "scGPT - INFO - | epoch   2 | 1000/3867 batches | lr 0.0001 | ms/batch 648.42 | loss 64.79 | mse 25.71 | mre 800232.69 |gepc 31.31 |\n",
      "scGPT - INFO - | epoch   2 | 1100/3867 batches | lr 0.0001 | ms/batch 638.71 | loss 57.59 | mse 24.76 | mre 775460.50 |gepc 27.61 |\n",
      "scGPT - INFO - | epoch   2 | 1200/3867 batches | lr 0.0001 | ms/batch 624.76 | loss 56.17 | mse 24.31 | mre 764469.81 |gepc 26.78 |\n",
      "scGPT - INFO - | epoch   2 | 1300/3867 batches | lr 0.0001 | ms/batch 953.00 | loss 56.01 | mse 24.40 | mre 769957.90 |gepc 26.57 |\n",
      "scGPT - INFO - | epoch   2 | 1400/3867 batches | lr 0.0001 | ms/batch 1024.34 | loss 58.24 | mse 24.13 | mre 750314.64 |gepc 28.76 |\n",
      "scGPT - INFO - | epoch   2 | 1500/3867 batches | lr 0.0001 | ms/batch 1009.39 | loss 54.96 | mse 21.53 | mre 631799.22 |gepc 25.32 |\n",
      "scGPT - INFO - | epoch   2 | 1600/3867 batches | lr 0.0001 | ms/batch 1008.45 | loss 48.28 | mse 20.43 | mre 611112.21 |gepc 22.54 |\n",
      "scGPT - INFO - | epoch   2 | 1700/3867 batches | lr 0.0001 | ms/batch 1010.67 | loss 60.69 | mse 20.79 | mre 621097.86 |gepc 24.81 |\n",
      "scGPT - INFO - | epoch   2 | 1800/3867 batches | lr 0.0001 | ms/batch 1006.04 | loss 58.02 | mse 20.53 | mre 600501.15 |gepc 24.32 |\n",
      "scGPT - INFO - | epoch   2 | 1900/3867 batches | lr 0.0001 | ms/batch 1007.77 | loss 48.49 | mse 20.14 | mre 599746.30 |gepc 22.32 |\n",
      "scGPT - INFO - | epoch   2 | 2000/3867 batches | lr 0.0001 | ms/batch 1013.89 | loss 46.59 | mse 20.17 | mre 600074.27 |gepc 21.72 |\n",
      "scGPT - INFO - | epoch   2 | 2100/3867 batches | lr 0.0001 | ms/batch 811.71 | loss 46.40 | mse 20.18 | mre 599036.86 |gepc 21.59 |\n",
      "scGPT - INFO - | epoch   2 | 2200/3867 batches | lr 0.0001 | ms/batch 613.70 | loss 45.91 | mse 20.02 | mre 594646.80 |gepc 21.30 |\n",
      "scGPT - INFO - | epoch   2 | 2300/3867 batches | lr 0.0001 | ms/batch 612.00 | loss 45.59 | mse 19.94 | mre 595466.29 |gepc 21.11 |\n",
      "scGPT - INFO - | epoch   2 | 2400/3867 batches | lr 0.0001 | ms/batch 613.94 | loss 45.45 | mse 19.89 | mre 592013.04 |gepc 21.03 |\n",
      "scGPT - INFO - | epoch   2 | 2500/3867 batches | lr 0.0001 | ms/batch 613.38 | loss 44.83 | mse 19.60 | mre 581279.51 |gepc 20.73 |\n",
      "scGPT - INFO - | epoch   2 | 2600/3867 batches | lr 0.0001 | ms/batch 611.45 | loss 61.49 | mse 24.01 | mre 744737.68 |gepc 28.62 |\n",
      "scGPT - INFO - | epoch   2 | 2700/3867 batches | lr 0.0001 | ms/batch 615.77 | loss 56.34 | mse 23.84 | mre 741551.61 |gepc 27.12 |\n",
      "scGPT - INFO - | epoch   2 | 2800/3867 batches | lr 0.0001 | ms/batch 616.39 | loss 54.10 | mse 23.17 | mre 724085.19 |gepc 25.81 |\n",
      "scGPT - INFO - | epoch   2 | 2900/3867 batches | lr 0.0001 | ms/batch 617.52 | loss 53.48 | mse 23.01 | mre 720330.24 |gepc 25.47 |\n",
      "scGPT - INFO - | epoch   2 | 3000/3867 batches | lr 0.0001 | ms/batch 641.25 | loss 53.01 | mse 22.85 | mre 715589.69 |gepc 25.19 |\n",
      "scGPT - INFO - | epoch   2 | 3100/3867 batches | lr 0.0001 | ms/batch 614.87 | loss 52.70 | mse 22.78 | mre 711586.05 |gepc 25.03 |\n",
      "scGPT - INFO - | epoch   2 | 3200/3867 batches | lr 0.0001 | ms/batch 614.43 | loss 52.13 | mse 22.64 | mre 708526.37 |gepc 24.65 |\n",
      "scGPT - INFO - | epoch   2 | 3300/3867 batches | lr 0.0001 | ms/batch 613.33 | loss 52.03 | mse 22.61 | mre 706037.41 |gepc 24.60 |\n",
      "scGPT - INFO - | epoch   2 | 3400/3867 batches | lr 0.0001 | ms/batch 615.99 | loss 51.88 | mse 22.54 | mre 707901.90 |gepc 24.56 |\n",
      "scGPT - INFO - | epoch   2 | 3500/3867 batches | lr 0.0001 | ms/batch 606.39 | loss 68.23 | mse 22.56 | mre 766577.32 |gepc 34.90 |\n",
      "scGPT - INFO - | epoch   2 | 3600/3867 batches | lr 0.0001 | ms/batch 604.85 | loss 53.84 | mse 20.41 | mre 642336.64 |gepc 24.84 |\n",
      "scGPT - INFO - | epoch   2 | 3700/3867 batches | lr 0.0001 | ms/batch 611.81 | loss 55.94 | mse 23.25 | mre 728570.88 |gepc 26.35 |\n",
      "scGPT - INFO - | epoch   2 | 3800/3867 batches | lr 0.0001 | ms/batch 603.56 | loss 56.33 | mse 20.10 | mre 639898.06 |gepc 27.79 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   2 | time: 2870.73s | valid loss/mse 26.1562 | mre 597827.5680\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 26.1562\n",
      "random masking at epoch   3, ratio of masked values in train:  0.3999\n",
      "scGPT - INFO - | epoch   3 | 100/3867 batches | lr 0.0001 | ms/batch 1028.70 | loss 58.05 | mse 22.61 | mre 753842.76 |gepc 26.64 |\n",
      "scGPT - INFO - | epoch   3 | 200/3867 batches | lr 0.0001 | ms/batch 1011.45 | loss 53.29 | mse 21.18 | mre 707071.59 |gepc 24.56 |\n",
      "scGPT - INFO - | epoch   3 | 300/3867 batches | lr 0.0001 | ms/batch 1021.35 | loss 47.17 | mse 19.54 | mre 670710.95 |gepc 22.14 |\n",
      "scGPT - INFO - | epoch   3 | 400/3867 batches | lr 0.0001 | ms/batch 1002.03 | loss 58.37 | mse 22.65 | mre 749309.54 |gepc 27.60 |\n",
      "scGPT - INFO - | epoch   3 | 500/3867 batches | lr 0.0001 | ms/batch 995.76 | loss 61.49 | mse 25.09 | mre 789768.12 |gepc 28.52 |\n",
      "scGPT - INFO - | epoch   3 | 600/3867 batches | lr 0.0001 | ms/batch 1008.82 | loss 65.40 | mse 24.88 | mre 787279.10 |gepc 28.23 |\n",
      "scGPT - INFO - | epoch   3 | 700/3867 batches | lr 0.0001 | ms/batch 822.06 | loss 61.84 | mse 24.77 | mre 777213.29 |gepc 27.40 |\n",
      "scGPT - INFO - | epoch   3 | 800/3867 batches | lr 0.0001 | ms/batch 617.67 | loss 60.26 | mse 24.62 | mre 772516.77 |gepc 26.99 |\n",
      "scGPT - INFO - | epoch   3 | 900/3867 batches | lr 0.0001 | ms/batch 615.73 | loss 58.58 | mse 24.32 | mre 766166.27 |gepc 26.53 |\n",
      "scGPT - INFO - | epoch   3 | 1000/3867 batches | lr 0.0001 | ms/batch 616.80 | loss 61.78 | mse 25.08 | mre 789128.31 |gepc 28.97 |\n",
      "scGPT - INFO - | epoch   3 | 1100/3867 batches | lr 0.0001 | ms/batch 612.95 | loss 55.69 | mse 24.27 | mre 755415.03 |gepc 26.48 |\n",
      "scGPT - INFO - | epoch   3 | 1200/3867 batches | lr 0.0001 | ms/batch 618.89 | loss 54.60 | mse 23.92 | mre 748902.31 |gepc 25.85 |\n",
      "scGPT - INFO - | epoch   3 | 1300/3867 batches | lr 0.0001 | ms/batch 615.90 | loss 54.19 | mse 23.80 | mre 748776.69 |gepc 25.61 |\n",
      "scGPT - INFO - | epoch   3 | 1400/3867 batches | lr 0.0001 | ms/batch 609.35 | loss 54.42 | mse 23.72 | mre 742166.00 |gepc 25.57 |\n",
      "scGPT - INFO - | epoch   3 | 1500/3867 batches | lr 0.0001 | ms/batch 613.03 | loss 51.35 | mse 20.60 | mre 614870.21 |gepc 22.76 |\n",
      "scGPT - INFO - | epoch   3 | 1600/3867 batches | lr 0.0001 | ms/batch 619.20 | loss 46.09 | mse 20.03 | mre 593387.24 |gepc 21.40 |\n",
      "scGPT - INFO - | epoch   3 | 1700/3867 batches | lr 0.0001 | ms/batch 613.76 | loss 45.51 | mse 19.91 | mre 589482.88 |gepc 21.08 |\n",
      "scGPT - INFO - | epoch   3 | 1800/3867 batches | lr 0.0001 | ms/batch 615.24 | loss 45.13 | mse 19.81 | mre 585893.39 |gepc 20.84 |\n",
      "scGPT - INFO - | epoch   3 | 1900/3867 batches | lr 0.0001 | ms/batch 635.36 | loss 45.00 | mse 19.79 | mre 589771.93 |gepc 20.75 |\n",
      "scGPT - INFO - | epoch   3 | 2000/3867 batches | lr 0.0001 | ms/batch 614.92 | loss 44.65 | mse 19.64 | mre 585687.38 |gepc 20.57 |\n",
      "scGPT - INFO - | epoch   3 | 2100/3867 batches | lr 0.0001 | ms/batch 627.34 | loss 44.41 | mse 19.56 | mre 580362.42 |gepc 20.44 |\n",
      "scGPT - INFO - | epoch   3 | 2200/3867 batches | lr 0.0001 | ms/batch 648.94 | loss 44.77 | mse 19.75 | mre 582109.53 |gepc 20.61 |\n",
      "scGPT - INFO - | epoch   3 | 2300/3867 batches | lr 0.0001 | ms/batch 611.59 | loss 44.50 | mse 19.62 | mre 584971.87 |gepc 20.47 |\n",
      "scGPT - INFO - | epoch   3 | 2400/3867 batches | lr 0.0001 | ms/batch 611.57 | loss 44.18 | mse 19.48 | mre 576185.99 |gepc 20.31 |\n",
      "scGPT - INFO - | epoch   3 | 2500/3867 batches | lr 0.0001 | ms/batch 632.52 | loss 44.15 | mse 19.48 | mre 577711.39 |gepc 20.29 |\n",
      "scGPT - INFO - | epoch   3 | 2600/3867 batches | lr 0.0001 | ms/batch 610.52 | loss 57.34 | mse 23.07 | mre 714275.46 |gepc 26.49 |\n",
      "scGPT - INFO - | epoch   3 | 2700/3867 batches | lr 0.0001 | ms/batch 614.35 | loss 53.07 | mse 22.75 | mre 710631.17 |gepc 25.38 |\n",
      "scGPT - INFO - | epoch   3 | 2800/3867 batches | lr 0.0001 | ms/batch 636.85 | loss 52.68 | mse 22.83 | mre 712881.47 |gepc 25.08 |\n",
      "scGPT - INFO - | epoch   3 | 2900/3867 batches | lr 0.0001 | ms/batch 615.71 | loss 51.72 | mse 22.50 | mre 707329.06 |gepc 24.49 |\n",
      "scGPT - INFO - | epoch   3 | 3000/3867 batches | lr 0.0001 | ms/batch 642.11 | loss 51.47 | mse 22.43 | mre 704576.01 |gepc 24.33 |\n",
      "scGPT - INFO - | epoch   3 | 3100/3867 batches | lr 0.0001 | ms/batch 687.32 | loss 50.67 | mse 22.12 | mre 694813.61 |gepc 23.87 |\n",
      "scGPT - INFO - | epoch   3 | 3200/3867 batches | lr 0.0001 | ms/batch 620.55 | loss 50.88 | mse 22.23 | mre 695023.83 |gepc 23.98 |\n",
      "scGPT - INFO - | epoch   3 | 3300/3867 batches | lr 0.0001 | ms/batch 615.22 | loss 50.55 | mse 22.11 | mre 692561.50 |gepc 23.80 |\n",
      "scGPT - INFO - | epoch   3 | 3400/3867 batches | lr 0.0001 | ms/batch 618.23 | loss 50.49 | mse 22.04 | mre 690060.86 |gepc 23.81 |\n",
      "scGPT - INFO - | epoch   3 | 3500/3867 batches | lr 0.0001 | ms/batch 614.67 | loss 58.00 | mse 21.51 | mre 693974.05 |gepc 26.50 |\n",
      "scGPT - INFO - | epoch   3 | 3600/3867 batches | lr 0.0001 | ms/batch 616.50 | loss 50.07 | mse 20.10 | mre 645536.96 |gepc 22.39 |\n",
      "scGPT - INFO - | epoch   3 | 3700/3867 batches | lr 0.0001 | ms/batch 621.49 | loss 54.27 | mse 23.02 | mre 715252.23 |gepc 25.30 |\n",
      "scGPT - INFO - | epoch   3 | 3800/3867 batches | lr 0.0001 | ms/batch 610.65 | loss 51.13 | mse 19.83 | mre 622618.12 |gepc 23.22 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   3 | time: 2744.94s | valid loss/mse 25.0103 | mre 539157.9141\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 25.0103\n",
      "random masking at epoch   4, ratio of masked values in train:  0.3999\n",
      "scGPT - INFO - | epoch   4 | 100/3867 batches | lr 0.0001 | ms/batch 623.80 | loss 56.14 | mse 22.16 | mre 739690.59 |gepc 25.52 |\n",
      "scGPT - INFO - | epoch   4 | 200/3867 batches | lr 0.0001 | ms/batch 614.01 | loss 51.10 | mse 20.84 | mre 688296.30 |gepc 23.38 |\n",
      "scGPT - INFO - | epoch   4 | 300/3867 batches | lr 0.0001 | ms/batch 601.64 | loss 45.77 | mse 19.33 | mre 647197.14 |gepc 21.13 |\n",
      "scGPT - INFO - | epoch   4 | 400/3867 batches | lr 0.0001 | ms/batch 622.31 | loss 54.18 | mse 22.12 | mre 730859.92 |gepc 24.47 |\n",
      "scGPT - INFO - | epoch   4 | 500/3867 batches | lr 0.0001 | ms/batch 611.42 | loss 60.59 | mse 24.85 | mre 761402.60 |gepc 28.73 |\n",
      "scGPT - INFO - | epoch   4 | 600/3867 batches | lr 0.0001 | ms/batch 625.85 | loss 55.98 | mse 24.26 | mre 756651.05 |gepc 26.05 |\n",
      "scGPT - INFO - | epoch   4 | 700/3867 batches | lr 0.0001 | ms/batch 627.67 | loss 54.39 | mse 24.01 | mre 753145.73 |gepc 25.41 |\n",
      "scGPT - INFO - | epoch   4 | 800/3867 batches | lr 0.0001 | ms/batch 636.50 | loss 54.18 | mse 23.98 | mre 750302.81 |gepc 25.29 |\n",
      "scGPT - INFO - | epoch   4 | 900/3867 batches | lr 0.0001 | ms/batch 646.51 | loss 53.78 | mse 23.88 | mre 749302.10 |gepc 25.08 |\n",
      "scGPT - INFO - | epoch   4 | 1000/3867 batches | lr 0.0001 | ms/batch 958.12 | loss 58.27 | mse 24.66 | mre 778097.04 |gepc 27.06 |\n",
      "scGPT - INFO - | epoch   4 | 1100/3867 batches | lr 0.0001 | ms/batch 1085.45 | loss 54.34 | mse 23.91 | mre 751455.82 |gepc 25.66 |\n",
      "scGPT - INFO - | epoch   4 | 1200/3867 batches | lr 0.0001 | ms/batch 1030.69 | loss 53.36 | mse 23.56 | mre 738166.94 |gepc 25.10 |\n",
      "scGPT - INFO - | epoch   4 | 1300/3867 batches | lr 0.0001 | ms/batch 807.55 | loss 53.08 | mse 23.47 | mre 740154.98 |gepc 24.96 |\n",
      "scGPT - INFO - | epoch   4 | 1400/3867 batches | lr 0.0001 | ms/batch 624.15 | loss 53.48 | mse 23.54 | mre 735769.86 |gepc 25.04 |\n",
      "scGPT - INFO - | epoch   4 | 1500/3867 batches | lr 0.0001 | ms/batch 612.49 | loss 47.79 | mse 20.06 | mre 599885.71 |gepc 21.72 |\n",
      "scGPT - INFO - | epoch   4 | 1600/3867 batches | lr 0.0001 | ms/batch 608.76 | loss 44.82 | mse 19.63 | mre 586006.79 |gepc 20.73 |\n",
      "scGPT - INFO - | epoch   4 | 1700/3867 batches | lr 0.0001 | ms/batch 609.27 | loss 44.29 | mse 19.46 | mre 575593.99 |gepc 20.43 |\n",
      "scGPT - INFO - | epoch   4 | 1800/3867 batches | lr 0.0001 | ms/batch 612.55 | loss 44.81 | mse 19.78 | mre 587158.06 |gepc 20.65 |\n",
      "scGPT - INFO - | epoch   4 | 1900/3867 batches | lr 0.0001 | ms/batch 611.83 | loss 44.25 | mse 19.54 | mre 581740.17 |gepc 20.34 |\n",
      "scGPT - INFO - | epoch   4 | 2000/3867 batches | lr 0.0001 | ms/batch 612.88 | loss 44.10 | mse 19.47 | mre 580232.91 |gepc 20.27 |\n",
      "scGPT - INFO - | epoch   4 | 2100/3867 batches | lr 0.0001 | ms/batch 618.03 | loss 44.19 | mse 19.54 | mre 577567.47 |gepc 20.31 |\n",
      "scGPT - INFO - | epoch   4 | 2200/3867 batches | lr 0.0001 | ms/batch 618.11 | loss 44.11 | mse 19.50 | mre 577584.07 |gepc 20.27 |\n",
      "scGPT - INFO - | epoch   4 | 2300/3867 batches | lr 0.0001 | ms/batch 614.58 | loss 44.05 | mse 19.50 | mre 580707.87 |gepc 20.21 |\n",
      "scGPT - INFO - | epoch   4 | 2400/3867 batches | lr 0.0001 | ms/batch 614.64 | loss 44.10 | mse 19.53 | mre 582505.26 |gepc 20.23 |\n",
      "scGPT - INFO - | epoch   4 | 2500/3867 batches | lr 0.0001 | ms/batch 613.49 | loss 43.90 | mse 19.44 | mre 575884.12 |gepc 20.14 |\n",
      "scGPT - INFO - | epoch   4 | 2600/3867 batches | lr 0.0001 | ms/batch 602.86 | loss 55.59 | mse 22.53 | mre 693427.97 |gepc 25.84 |\n",
      "scGPT - INFO - | epoch   4 | 2700/3867 batches | lr 0.0001 | ms/batch 611.64 | loss 52.05 | mse 22.43 | mre 701335.86 |gepc 24.72 |\n",
      "scGPT - INFO - | epoch   4 | 2800/3867 batches | lr 0.0001 | ms/batch 618.78 | loss 51.22 | mse 22.33 | mre 696247.35 |gepc 24.24 |\n",
      "scGPT - INFO - | epoch   4 | 2900/3867 batches | lr 0.0001 | ms/batch 613.28 | loss 50.33 | mse 21.99 | mre 689346.31 |gepc 23.71 |\n",
      "scGPT - INFO - | epoch   4 | 3000/3867 batches | lr 0.0001 | ms/batch 617.37 | loss 50.60 | mse 22.14 | mre 691908.11 |gepc 23.84 |\n",
      "scGPT - INFO - | epoch   4 | 3100/3867 batches | lr 0.0001 | ms/batch 612.87 | loss 49.81 | mse 21.78 | mre 684315.71 |gepc 23.40 |\n",
      "scGPT - INFO - | epoch   4 | 3200/3867 batches | lr 0.0001 | ms/batch 610.59 | loss 50.15 | mse 21.98 | mre 691174.77 |gepc 23.57 |\n",
      "scGPT - INFO - | epoch   4 | 3300/3867 batches | lr 0.0001 | ms/batch 610.11 | loss 50.17 | mse 22.03 | mre 689106.65 |gepc 23.56 |\n",
      "scGPT - INFO - | epoch   4 | 3400/3867 batches | lr 0.0001 | ms/batch 614.95 | loss 49.73 | mse 21.82 | mre 686528.44 |gepc 23.34 |\n",
      "scGPT - INFO - | epoch   4 | 3500/3867 batches | lr 0.0001 | ms/batch 606.32 | loss 53.48 | mse 20.96 | mre 679220.91 |gepc 24.45 |\n",
      "scGPT - INFO - | epoch   4 | 3600/3867 batches | lr 0.0001 | ms/batch 600.35 | loss 50.97 | mse 19.92 | mre 632010.65 |gepc 23.17 |\n",
      "scGPT - INFO - | epoch   4 | 3700/3867 batches | lr 0.0001 | ms/batch 606.04 | loss 52.87 | mse 22.60 | mre 710930.37 |gepc 24.70 |\n",
      "scGPT - INFO - | epoch   4 | 3800/3867 batches | lr 0.0001 | ms/batch 603.36 | loss 50.10 | mse 19.62 | mre 610251.09 |gepc 22.64 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   4 | time: 2606.11s | valid loss/mse 23.7160 | mre 717332.5184\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 23.7160\n",
      "random masking at epoch   5, ratio of masked values in train:  0.3999\n",
      "scGPT - INFO - | epoch   5 | 100/3867 batches | lr 0.0001 | ms/batch 613.44 | loss 54.38 | mse 21.74 | mre 728694.66 |gepc 24.43 |\n",
      "scGPT - INFO - | epoch   5 | 200/3867 batches | lr 0.0001 | ms/batch 610.28 | loss 49.50 | mse 20.50 | mre 690606.16 |gepc 22.60 |\n",
      "scGPT - INFO - | epoch   5 | 300/3867 batches | lr 0.0001 | ms/batch 607.76 | loss 45.29 | mse 19.24 | mre 640618.26 |gepc 20.79 |\n",
      "scGPT - INFO - | epoch   5 | 400/3867 batches | lr 0.0001 | ms/batch 602.35 | loss 54.18 | mse 22.06 | mre 716619.06 |gepc 24.66 |\n",
      "scGPT - INFO - | epoch   5 | 500/3867 batches | lr 0.0001 | ms/batch 604.53 | loss 56.89 | mse 24.25 | mre 763737.68 |gepc 26.52 |\n",
      "scGPT - INFO - | epoch   5 | 600/3867 batches | lr 0.0001 | ms/batch 615.29 | loss 54.60 | mse 24.02 | mre 748937.04 |gepc 25.43 |\n",
      "scGPT - INFO - | epoch   5 | 700/3867 batches | lr 0.0001 | ms/batch 610.90 | loss 53.84 | mse 23.80 | mre 745062.68 |gepc 25.09 |\n",
      "scGPT - INFO - | epoch   5 | 800/3867 batches | lr 0.0001 | ms/batch 612.19 | loss 53.66 | mse 23.83 | mre 744440.58 |gepc 24.99 |\n",
      "scGPT - INFO - | epoch   5 | 900/3867 batches | lr 0.0001 | ms/batch 613.80 | loss 53.31 | mse 23.70 | mre 742465.11 |gepc 24.84 |\n",
      "scGPT - INFO - | epoch   5 | 1000/3867 batches | lr 0.0001 | ms/batch 613.32 | loss 57.56 | mse 24.34 | mre 767800.46 |gepc 26.68 |\n",
      "scGPT - INFO - | epoch   5 | 1100/3867 batches | lr 0.0001 | ms/batch 611.26 | loss 53.93 | mse 23.73 | mre 744919.53 |gepc 25.43 |\n",
      "scGPT - INFO - | epoch   5 | 1200/3867 batches | lr 0.0001 | ms/batch 611.52 | loss 53.42 | mse 23.64 | mre 745026.23 |gepc 25.12 |\n",
      "scGPT - INFO - | epoch   5 | 1300/3867 batches | lr 0.0001 | ms/batch 610.63 | loss 52.97 | mse 23.50 | mre 740676.38 |gepc 24.86 |\n",
      "scGPT - INFO - | epoch   5 | 1400/3867 batches | lr 0.0001 | ms/batch 611.56 | loss 52.94 | mse 23.33 | mre 727025.28 |gepc 24.79 |\n",
      "scGPT - INFO - | epoch   5 | 1500/3867 batches | lr 0.0001 | ms/batch 611.87 | loss 47.34 | mse 19.85 | mre 588688.14 |gepc 21.50 |\n",
      "scGPT - INFO - | epoch   5 | 1600/3867 batches | lr 0.0001 | ms/batch 614.37 | loss 44.51 | mse 19.53 | mre 581558.19 |gepc 20.56 |\n",
      "scGPT - INFO - | epoch   5 | 1700/3867 batches | lr 0.0001 | ms/batch 612.16 | loss 44.26 | mse 19.51 | mre 579006.43 |gepc 20.39 |\n",
      "scGPT - INFO - | epoch   5 | 1800/3867 batches | lr 0.0001 | ms/batch 610.16 | loss 43.99 | mse 19.44 | mre 574649.53 |gepc 20.22 |\n",
      "scGPT - INFO - | epoch   5 | 1900/3867 batches | lr 0.0001 | ms/batch 614.66 | loss 44.23 | mse 19.55 | mre 580395.48 |gepc 20.34 |\n",
      "scGPT - INFO - | epoch   5 | 2000/3867 batches | lr 0.0001 | ms/batch 613.01 | loss 43.89 | mse 19.41 | mre 573684.82 |gepc 20.15 |\n",
      "scGPT - INFO - | epoch   5 | 2100/3867 batches | lr 0.0001 | ms/batch 615.59 | loss 44.13 | mse 19.53 | mre 584136.81 |gepc 20.29 |\n",
      "scGPT - INFO - | epoch   5 | 2200/3867 batches | lr 0.0001 | ms/batch 608.31 | loss 43.60 | mse 19.31 | mre 574578.77 |gepc 19.98 |\n",
      "scGPT - INFO - | epoch   5 | 2300/3867 batches | lr 0.0001 | ms/batch 610.73 | loss 43.67 | mse 19.35 | mre 575427.66 |gepc 20.02 |\n",
      "scGPT - INFO - | epoch   5 | 2400/3867 batches | lr 0.0001 | ms/batch 610.49 | loss 43.96 | mse 19.49 | mre 578544.22 |gepc 20.17 |\n",
      "scGPT - INFO - | epoch   5 | 2500/3867 batches | lr 0.0001 | ms/batch 612.24 | loss 43.87 | mse 19.44 | mre 578991.33 |gepc 20.13 |\n",
      "scGPT - INFO - | epoch   5 | 2600/3867 batches | lr 0.0001 | ms/batch 613.05 | loss 53.21 | mse 21.96 | mre 676372.73 |gepc 24.84 |\n",
      "scGPT - INFO - | epoch   5 | 2700/3867 batches | lr 0.0001 | ms/batch 618.69 | loss 51.29 | mse 22.14 | mre 695466.77 |gepc 24.27 |\n",
      "scGPT - INFO - | epoch   5 | 2800/3867 batches | lr 0.0001 | ms/batch 609.89 | loss 50.44 | mse 22.00 | mre 690300.32 |gepc 23.80 |\n",
      "scGPT - INFO - | epoch   5 | 2900/3867 batches | lr 0.0001 | ms/batch 621.97 | loss 50.01 | mse 21.90 | mre 686726.46 |gepc 23.53 |\n",
      "scGPT - INFO - | epoch   5 | 3000/3867 batches | lr 0.0001 | ms/batch 624.26 | loss 49.81 | mse 21.82 | mre 688108.70 |gepc 23.41 |\n",
      "scGPT - INFO - | epoch   5 | 3100/3867 batches | lr 0.0001 | ms/batch 614.66 | loss 49.64 | mse 21.78 | mre 684967.27 |gepc 23.29 |\n",
      "scGPT - INFO - | epoch   5 | 3200/3867 batches | lr 0.0001 | ms/batch 613.18 | loss 49.55 | mse 21.77 | mre 684667.62 |gepc 23.22 |\n",
      "scGPT - INFO - | epoch   5 | 3300/3867 batches | lr 0.0001 | ms/batch 616.43 | loss 49.15 | mse 21.61 | mre 682604.12 |gepc 22.99 |\n",
      "scGPT - INFO - | epoch   5 | 3400/3867 batches | lr 0.0001 | ms/batch 622.30 | loss 49.66 | mse 21.86 | mre 685882.26 |gepc 23.26 |\n",
      "scGPT - INFO - | epoch   5 | 3500/3867 batches | lr 0.0001 | ms/batch 620.15 | loss 53.74 | mse 20.86 | mre 678357.23 |gepc 25.09 |\n",
      "scGPT - INFO - | epoch   5 | 3600/3867 batches | lr 0.0001 | ms/batch 609.76 | loss 48.66 | mse 19.72 | mre 620047.52 |gepc 21.56 |\n",
      "scGPT - INFO - | epoch   5 | 3700/3867 batches | lr 0.0001 | ms/batch 621.79 | loss 52.42 | mse 22.46 | mre 708098.97 |gepc 24.42 |\n",
      "scGPT - INFO - | epoch   5 | 3800/3867 batches | lr 0.0001 | ms/batch 609.22 | loss 50.12 | mse 19.75 | mre 618875.45 |gepc 22.51 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   5 | time: 2458.12s | valid loss/mse 23.4649 | mre 682474.0369\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 23.4649\n",
      "random masking at epoch   6, ratio of masked values in train:  0.3999\n",
      "scGPT - INFO - | epoch   6 | 100/3867 batches | lr 0.0001 | ms/batch 626.09 | loss 53.62 | mse 21.55 | mre 721687.75 |gepc 24.07 |\n",
      "scGPT - INFO - | epoch   6 | 200/3867 batches | lr 0.0001 | ms/batch 615.65 | loss 49.26 | mse 20.36 | mre 679765.94 |gepc 22.48 |\n",
      "scGPT - INFO - | epoch   6 | 300/3867 batches | lr 0.0001 | ms/batch 621.32 | loss 44.78 | mse 18.99 | mre 638069.31 |gepc 20.57 |\n",
      "scGPT - INFO - | epoch   6 | 400/3867 batches | lr 0.0001 | ms/batch 611.52 | loss 52.87 | mse 21.87 | mre 722717.01 |gepc 23.72 |\n",
      "scGPT - INFO - | epoch   6 | 500/3867 batches | lr 0.0001 | ms/batch 619.78 | loss 56.08 | mse 24.10 | mre 753999.32 |gepc 26.10 |\n",
      "scGPT - INFO - | epoch   6 | 600/3867 batches | lr 0.0001 | ms/batch 618.65 | loss 53.61 | mse 23.72 | mre 747317.21 |gepc 25.06 |\n",
      "scGPT - INFO - | epoch   6 | 700/3867 batches | lr 0.0001 | ms/batch 642.76 | loss 53.31 | mse 23.63 | mre 740817.36 |gepc 24.88 |\n",
      "scGPT - INFO - | epoch   6 | 800/3867 batches | lr 0.0001 | ms/batch 648.02 | loss 53.11 | mse 23.65 | mre 743999.44 |gepc 24.75 |\n",
      "scGPT - INFO - | epoch   6 | 900/3867 batches | lr 0.0001 | ms/batch 698.01 | loss 52.93 | mse 23.58 | mre 737626.13 |gepc 24.64 |\n",
      "scGPT - INFO - | epoch   6 | 1000/3867 batches | lr 0.0001 | ms/batch 625.19 | loss 56.69 | mse 24.10 | mre 759874.99 |gepc 26.27 |\n",
      "scGPT - INFO - | epoch   6 | 1100/3867 batches | lr 0.0001 | ms/batch 671.19 | loss 53.31 | mse 23.49 | mre 736398.87 |gepc 25.05 |\n",
      "scGPT - INFO - | epoch   6 | 1200/3867 batches | lr 0.0001 | ms/batch 670.86 | loss 52.90 | mse 23.45 | mre 740813.41 |gepc 24.85 |\n",
      "scGPT - INFO - | epoch   6 | 1300/3867 batches | lr 0.0001 | ms/batch 621.72 | loss 52.59 | mse 23.34 | mre 737347.96 |gepc 24.69 |\n",
      "scGPT - INFO - | epoch   6 | 1400/3867 batches | lr 0.0001 | ms/batch 609.89 | loss 52.64 | mse 23.30 | mre 732721.16 |gepc 24.61 |\n",
      "scGPT - INFO - | epoch   6 | 1500/3867 batches | lr 0.0001 | ms/batch 610.40 | loss 46.83 | mse 19.80 | mre 587181.39 |gepc 21.29 |\n",
      "scGPT - INFO - | epoch   6 | 1600/3867 batches | lr 0.0001 | ms/batch 661.46 | loss 44.63 | mse 19.45 | mre 579716.03 |gepc 20.55 |\n",
      "scGPT - INFO - | epoch   6 | 1700/3867 batches | lr 0.0001 | ms/batch 640.39 | loss 44.07 | mse 19.43 | mre 577425.96 |gepc 20.29 |\n",
      "scGPT - INFO - | epoch   6 | 1800/3867 batches | lr 0.0001 | ms/batch 610.71 | loss 44.08 | mse 19.49 | mre 576276.71 |gepc 20.28 |\n",
      "scGPT - INFO - | epoch   6 | 1900/3867 batches | lr 0.0001 | ms/batch 650.61 | loss 43.90 | mse 19.41 | mre 574897.02 |gepc 20.18 |\n",
      "scGPT - INFO - | epoch   6 | 2000/3867 batches | lr 0.0001 | ms/batch 653.74 | loss 43.83 | mse 19.40 | mre 574597.00 |gepc 20.13 |\n",
      "scGPT - INFO - | epoch   6 | 2100/3867 batches | lr 0.0001 | ms/batch 748.43 | loss 44.34 | mse 19.66 | mre 579801.85 |gepc 20.38 |\n",
      "scGPT - INFO - | epoch   6 | 2200/3867 batches | lr 0.0001 | ms/batch 641.78 | loss 43.55 | mse 19.30 | mre 577614.58 |gepc 19.96 |\n",
      "scGPT - INFO - | epoch   6 | 2300/3867 batches | lr 0.0001 | ms/batch 725.23 | loss 43.42 | mse 19.24 | mre 568805.53 |gepc 19.90 |\n",
      "scGPT - INFO - | epoch   6 | 2400/3867 batches | lr 0.0001 | ms/batch 698.88 | loss 43.38 | mse 19.22 | mre 568891.48 |gepc 19.88 |\n",
      "scGPT - INFO - | epoch   6 | 2500/3867 batches | lr 0.0001 | ms/batch 689.66 | loss 43.30 | mse 19.19 | mre 570807.32 |gepc 19.83 |\n",
      "scGPT - INFO - | epoch   6 | 2600/3867 batches | lr 0.0001 | ms/batch 737.92 | loss 52.73 | mse 21.93 | mre 674386.72 |gepc 24.80 |\n",
      "scGPT - INFO - | epoch   6 | 2700/3867 batches | lr 0.0001 | ms/batch 735.44 | loss 50.68 | mse 21.96 | mre 686387.47 |gepc 23.94 |\n",
      "scGPT - INFO - | epoch   6 | 2800/3867 batches | lr 0.0001 | ms/batch 730.78 | loss 49.97 | mse 21.87 | mre 686431.34 |gepc 23.50 |\n",
      "scGPT - INFO - | epoch   6 | 2900/3867 batches | lr 0.0001 | ms/batch 728.03 | loss 49.74 | mse 21.82 | mre 684377.82 |gepc 23.35 |\n",
      "scGPT - INFO - | epoch   6 | 3000/3867 batches | lr 0.0001 | ms/batch 610.03 | loss 49.45 | mse 21.69 | mre 684296.08 |gepc 23.21 |\n",
      "scGPT - INFO - | epoch   6 | 3100/3867 batches | lr 0.0001 | ms/batch 610.31 | loss 49.28 | mse 21.66 | mre 676829.50 |gepc 23.07 |\n",
      "scGPT - INFO - | epoch   6 | 3200/3867 batches | lr 0.0001 | ms/batch 611.37 | loss 49.46 | mse 21.73 | mre 680598.10 |gepc 23.19 |\n",
      "scGPT - INFO - | epoch   6 | 3300/3867 batches | lr 0.0001 | ms/batch 611.66 | loss 48.90 | mse 21.49 | mre 678546.28 |gepc 22.87 |\n",
      "scGPT - INFO - | epoch   6 | 3400/3867 batches | lr 0.0001 | ms/batch 608.65 | loss 48.97 | mse 21.55 | mre 678329.97 |gepc 22.89 |\n",
      "scGPT - INFO - | epoch   6 | 3500/3867 batches | lr 0.0001 | ms/batch 601.95 | loss 52.25 | mse 20.62 | mre 671118.53 |gepc 24.04 |\n",
      "scGPT - INFO - | epoch   6 | 3600/3867 batches | lr 0.0001 | ms/batch 603.20 | loss 49.15 | mse 19.77 | mre 626178.21 |gepc 21.88 |\n",
      "scGPT - INFO - | epoch   6 | 3700/3867 batches | lr 0.0001 | ms/batch 627.27 | loss 52.57 | mse 22.44 | mre 700810.60 |gepc 24.36 |\n",
      "scGPT - INFO - | epoch   6 | 3800/3867 batches | lr 0.0001 | ms/batch 605.02 | loss 51.27 | mse 19.76 | mre 636721.51 |gepc 21.94 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   6 | time: 2596.91s | valid loss/mse 23.5207 | mre 724332.6431\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "random masking at epoch   7, ratio of masked values in train:  0.3999\n",
      "scGPT - INFO - | epoch   7 | 100/3867 batches | lr 0.0001 | ms/batch 659.01 | loss 54.67 | mse 21.59 | mre 717870.08 |gepc 24.70 |\n",
      "scGPT - INFO - | epoch   7 | 200/3867 batches | lr 0.0001 | ms/batch 1409.59 | loss 49.28 | mse 20.34 | mre 683030.09 |gepc 22.40 |\n",
      "scGPT - INFO - | epoch   7 | 300/3867 batches | lr 0.0001 | ms/batch 2738.68 | loss 44.72 | mse 18.89 | mre 643747.86 |gepc 20.44 |\n",
      "scGPT - INFO - | epoch   7 | 400/3867 batches | lr 0.0001 | ms/batch 2627.56 | loss 52.82 | mse 21.78 | mre 708233.13 |gepc 23.69 |\n",
      "scGPT - INFO - | epoch   7 | 500/3867 batches | lr 0.0001 | ms/batch 2323.17 | loss 56.64 | mse 24.05 | mre 758994.07 |gepc 26.38 |\n",
      "scGPT - INFO - | epoch   7 | 600/3867 batches | lr 0.0001 | ms/batch 2026.93 | loss 53.63 | mse 23.70 | mre 746568.33 |gepc 25.06 |\n",
      "scGPT - INFO - | epoch   7 | 700/3867 batches | lr 0.0001 | ms/batch 621.67 | loss 53.24 | mse 23.63 | mre 741529.88 |gepc 24.85 |\n",
      "scGPT - INFO - | epoch   7 | 800/3867 batches | lr 0.0001 | ms/batch 614.45 | loss 52.89 | mse 23.51 | mre 738729.00 |gepc 24.65 |\n",
      "scGPT - INFO - | epoch   7 | 900/3867 batches | lr 0.0001 | ms/batch 612.59 | loss 52.75 | mse 23.49 | mre 734964.84 |gepc 24.56 |\n",
      "scGPT - INFO - | epoch   7 | 1000/3867 batches | lr 0.0001 | ms/batch 605.83 | loss 56.58 | mse 23.97 | mre 762729.37 |gepc 26.20 |\n",
      "scGPT - INFO - | epoch   7 | 1100/3867 batches | lr 0.0001 | ms/batch 613.11 | loss 54.05 | mse 23.60 | mre 745130.78 |gepc 25.31 |\n",
      "scGPT - INFO - | epoch   7 | 1200/3867 batches | lr 0.0001 | ms/batch 612.15 | loss 52.89 | mse 23.36 | mre 733516.65 |gepc 24.84 |\n",
      "scGPT - INFO - | epoch   7 | 1300/3867 batches | lr 0.0001 | ms/batch 716.75 | loss 52.47 | mse 23.30 | mre 731765.16 |gepc 24.60 |\n",
      "scGPT - INFO - | epoch   7 | 1400/3867 batches | lr 0.0001 | ms/batch 730.10 | loss 52.35 | mse 23.18 | mre 724103.20 |gepc 24.45 |\n",
      "scGPT - INFO - | epoch   7 | 1500/3867 batches | lr 0.0001 | ms/batch 731.27 | loss 46.85 | mse 19.69 | mre 582005.63 |gepc 21.21 |\n",
      "scGPT - INFO - | epoch   7 | 1600/3867 batches | lr 0.0001 | ms/batch 734.80 | loss 45.32 | mse 19.59 | mre 584032.77 |gepc 20.79 |\n",
      "scGPT - INFO - | epoch   7 | 1700/3867 batches | lr 0.0001 | ms/batch 728.00 | loss 44.21 | mse 19.48 | mre 579126.46 |gepc 20.37 |\n",
      "scGPT - INFO - | epoch   7 | 1800/3867 batches | lr 0.0001 | ms/batch 637.48 | loss 43.53 | mse 19.22 | mre 571213.07 |gepc 20.00 |\n",
      "scGPT - INFO - | epoch   7 | 1900/3867 batches | lr 0.0001 | ms/batch 610.92 | loss 43.43 | mse 19.19 | mre 571740.48 |gepc 19.94 |\n",
      "scGPT - INFO - | epoch   7 | 2000/3867 batches | lr 0.0001 | ms/batch 610.43 | loss 43.42 | mse 19.21 | mre 573159.62 |gepc 19.93 |\n",
      "scGPT - INFO - | epoch   7 | 2100/3867 batches | lr 0.0001 | ms/batch 612.21 | loss 43.38 | mse 19.19 | mre 569090.35 |gepc 19.91 |\n",
      "scGPT - INFO - | epoch   7 | 2200/3867 batches | lr 0.0001 | ms/batch 608.02 | loss 43.25 | mse 19.15 | mre 569135.72 |gepc 19.82 |\n",
      "scGPT - INFO - | epoch   7 | 2300/3867 batches | lr 0.0001 | ms/batch 610.45 | loss 43.45 | mse 19.24 | mre 571218.53 |gepc 19.93 |\n",
      "scGPT - INFO - | epoch   7 | 2400/3867 batches | lr 0.0001 | ms/batch 607.07 | loss 43.55 | mse 19.31 | mre 576196.10 |gepc 19.97 |\n",
      "scGPT - INFO - | epoch   7 | 2500/3867 batches | lr 0.0001 | ms/batch 608.69 | loss 43.57 | mse 19.34 | mre 573565.73 |gepc 19.97 |\n",
      "scGPT - INFO - | epoch   7 | 2600/3867 batches | lr 0.0001 | ms/batch 606.69 | loss 52.39 | mse 21.80 | mre 674565.46 |gepc 24.49 |\n",
      "scGPT - INFO - | epoch   7 | 2700/3867 batches | lr 0.0001 | ms/batch 611.52 | loss 50.95 | mse 21.78 | mre 687410.53 |gepc 23.95 |\n",
      "scGPT - INFO - | epoch   7 | 2800/3867 batches | lr 0.0001 | ms/batch 610.76 | loss 49.74 | mse 21.72 | mre 683694.65 |gepc 23.39 |\n",
      "scGPT - INFO - | epoch   7 | 2900/3867 batches | lr 0.0001 | ms/batch 611.31 | loss 49.32 | mse 21.61 | mre 685784.77 |gepc 23.14 |\n",
      "scGPT - INFO - | epoch   7 | 3000/3867 batches | lr 0.0001 | ms/batch 609.33 | loss 49.17 | mse 21.62 | mre 680394.68 |gepc 23.02 |\n",
      "scGPT - INFO - | epoch   7 | 3100/3867 batches | lr 0.0001 | ms/batch 613.81 | loss 49.05 | mse 21.53 | mre 681553.22 |gepc 22.98 |\n",
      "scGPT - INFO - | epoch   7 | 3200/3867 batches | lr 0.0001 | ms/batch 616.72 | loss 48.91 | mse 21.49 | mre 674601.24 |gepc 22.90 |\n",
      "scGPT - INFO - | epoch   7 | 3300/3867 batches | lr 0.0001 | ms/batch 755.18 | loss 48.95 | mse 21.54 | mre 678867.74 |gepc 22.89 |\n",
      "scGPT - INFO - | epoch   7 | 3400/3867 batches | lr 0.0001 | ms/batch 749.69 | loss 48.99 | mse 21.56 | mre 679944.61 |gepc 22.90 |\n",
      "scGPT - INFO - | epoch   7 | 3500/3867 batches | lr 0.0001 | ms/batch 729.98 | loss 51.97 | mse 20.38 | mre 656289.35 |gepc 23.66 |\n",
      "scGPT - INFO - | epoch   7 | 3600/3867 batches | lr 0.0001 | ms/batch 608.68 | loss 48.83 | mse 19.63 | mre 625255.66 |gepc 21.67 |\n",
      "scGPT - INFO - | epoch   7 | 3700/3867 batches | lr 0.0001 | ms/batch 711.91 | loss 52.49 | mse 22.42 | mre 701907.06 |gepc 24.35 |\n",
      "scGPT - INFO - | epoch   7 | 3800/3867 batches | lr 0.0001 | ms/batch 741.57 | loss 47.77 | mse 19.49 | mre 612289.80 |gepc 20.80 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   7 | time: 3401.87s | valid loss/mse 23.2657 | mre 643289.5726\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 23.2657\n",
      "random masking at epoch   8, ratio of masked values in train:  0.3999\n",
      "scGPT - INFO - | epoch   8 | 100/3867 batches | lr 0.0000 | ms/batch 613.16 | loss 53.06 | mse 21.28 | mre 706483.44 |gepc 23.80 |\n",
      "scGPT - INFO - | epoch   8 | 200/3867 batches | lr 0.0000 | ms/batch 629.50 | loss 49.00 | mse 20.22 | mre 675401.52 |gepc 22.28 |\n",
      "scGPT - INFO - | epoch   8 | 300/3867 batches | lr 0.0000 | ms/batch 658.64 | loss 44.82 | mse 18.97 | mre 639418.62 |gepc 20.53 |\n",
      "scGPT - INFO - | epoch   8 | 400/3867 batches | lr 0.0000 | ms/batch 659.07 | loss 52.39 | mse 21.66 | mre 710258.07 |gepc 23.59 |\n",
      "scGPT - INFO - | epoch   8 | 500/3867 batches | lr 0.0000 | ms/batch 655.04 | loss 55.32 | mse 23.78 | mre 749699.36 |gepc 25.73 |\n",
      "scGPT - INFO - | epoch   8 | 600/3867 batches | lr 0.0000 | ms/batch 650.78 | loss 53.39 | mse 23.65 | mre 741330.00 |gepc 24.95 |\n",
      "scGPT - INFO - | epoch   8 | 700/3867 batches | lr 0.0000 | ms/batch 613.47 | loss 52.90 | mse 23.52 | mre 739673.45 |gepc 24.67 |\n",
      "scGPT - INFO - | epoch   8 | 800/3867 batches | lr 0.0000 | ms/batch 612.48 | loss 52.31 | mse 23.29 | mre 731929.37 |gepc 24.37 |\n",
      "scGPT - INFO - | epoch   8 | 900/3867 batches | lr 0.0000 | ms/batch 622.78 | loss 52.42 | mse 23.38 | mre 735226.38 |gepc 24.41 |\n",
      "scGPT - INFO - | epoch   8 | 1000/3867 batches | lr 0.0000 | ms/batch 638.39 | loss 56.19 | mse 23.89 | mre 763609.22 |gepc 25.98 |\n",
      "scGPT - INFO - | epoch   8 | 1100/3867 batches | lr 0.0000 | ms/batch 626.63 | loss 53.75 | mse 23.65 | mre 744051.44 |gepc 25.23 |\n",
      "scGPT - INFO - | epoch   8 | 1200/3867 batches | lr 0.0000 | ms/batch 626.30 | loss 52.27 | mse 23.10 | mre 728496.75 |gepc 24.51 |\n",
      "scGPT - INFO - | epoch   8 | 1300/3867 batches | lr 0.0000 | ms/batch 628.71 | loss 52.22 | mse 23.18 | mre 731362.98 |gepc 24.52 |\n",
      "scGPT - INFO - | epoch   8 | 1400/3867 batches | lr 0.0000 | ms/batch 631.76 | loss 51.98 | mse 23.03 | mre 722681.80 |gepc 24.26 |\n",
      "scGPT - INFO - | epoch   8 | 1500/3867 batches | lr 0.0000 | ms/batch 635.12 | loss 46.34 | mse 19.57 | mre 577664.32 |gepc 21.02 |\n",
      "scGPT - INFO - | epoch   8 | 1600/3867 batches | lr 0.0000 | ms/batch 633.51 | loss 43.90 | mse 19.26 | mre 571741.49 |gepc 20.23 |\n",
      "scGPT - INFO - | epoch   8 | 1700/3867 batches | lr 0.0000 | ms/batch 639.04 | loss 43.47 | mse 19.17 | mre 569585.63 |gepc 20.00 |\n",
      "scGPT - INFO - | epoch   8 | 1800/3867 batches | lr 0.0000 | ms/batch 651.65 | loss 43.73 | mse 19.33 | mre 573347.67 |gepc 20.12 |\n",
      "scGPT - INFO - | epoch   8 | 1900/3867 batches | lr 0.0000 | ms/batch 709.69 | loss 43.66 | mse 19.30 | mre 573151.77 |gepc 20.07 |\n",
      "scGPT - INFO - | epoch   8 | 2000/3867 batches | lr 0.0000 | ms/batch 737.63 | loss 43.68 | mse 19.35 | mre 576882.86 |gepc 20.06 |\n",
      "scGPT - INFO - | epoch   8 | 2100/3867 batches | lr 0.0000 | ms/batch 655.95 | loss 43.33 | mse 19.16 | mre 570679.18 |gepc 19.89 |\n",
      "scGPT - INFO - | epoch   8 | 2200/3867 batches | lr 0.0000 | ms/batch 733.82 | loss 43.33 | mse 19.20 | mre 570205.24 |gepc 19.86 |\n",
      "scGPT - INFO - | epoch   8 | 2300/3867 batches | lr 0.0000 | ms/batch 653.47 | loss 44.09 | mse 19.59 | mre 580640.34 |gepc 20.23 |\n",
      "scGPT - INFO - | epoch   8 | 2400/3867 batches | lr 0.0000 | ms/batch 613.19 | loss 43.33 | mse 19.20 | mre 573525.69 |gepc 19.88 |\n",
      "scGPT - INFO - | epoch   8 | 2500/3867 batches | lr 0.0000 | ms/batch 614.14 | loss 43.21 | mse 19.17 | mre 568537.68 |gepc 19.79 |\n",
      "scGPT - INFO - | epoch   8 | 2600/3867 batches | lr 0.0000 | ms/batch 611.79 | loss 51.98 | mse 21.71 | mre 669456.84 |gepc 24.29 |\n",
      "scGPT - INFO - | epoch   8 | 2700/3867 batches | lr 0.0000 | ms/batch 664.16 | loss 49.77 | mse 21.59 | mre 680735.33 |gepc 23.39 |\n",
      "scGPT - INFO - | epoch   8 | 2800/3867 batches | lr 0.0000 | ms/batch 664.08 | loss 49.42 | mse 21.63 | mre 676163.81 |gepc 23.18 |\n",
      "scGPT - INFO - | epoch   8 | 2900/3867 batches | lr 0.0000 | ms/batch 665.35 | loss 49.17 | mse 21.55 | mre 678438.69 |gepc 23.03 |\n",
      "scGPT - INFO - | epoch   8 | 3000/3867 batches | lr 0.0000 | ms/batch 637.18 | loss 48.89 | mse 21.46 | mre 674466.84 |gepc 22.87 |\n",
      "scGPT - INFO - | epoch   8 | 3100/3867 batches | lr 0.0000 | ms/batch 628.95 | loss 48.97 | mse 21.53 | mre 674786.39 |gepc 22.91 |\n",
      "scGPT - INFO - | epoch   8 | 3200/3867 batches | lr 0.0000 | ms/batch 624.27 | loss 48.65 | mse 21.39 | mre 676269.42 |gepc 22.75 |\n",
      "scGPT - INFO - | epoch   8 | 3300/3867 batches | lr 0.0000 | ms/batch 614.17 | loss 49.01 | mse 21.56 | mre 680368.33 |gepc 22.93 |\n",
      "scGPT - INFO - | epoch   8 | 3400/3867 batches | lr 0.0000 | ms/batch 614.00 | loss 48.59 | mse 21.39 | mre 675963.04 |gepc 22.69 |\n",
      "scGPT - INFO - | epoch   8 | 3500/3867 batches | lr 0.0000 | ms/batch 604.43 | loss 50.90 | mse 20.21 | mre 660104.38 |gepc 23.00 |\n",
      "scGPT - INFO - | epoch   8 | 3600/3867 batches | lr 0.0000 | ms/batch 605.54 | loss 47.59 | mse 19.53 | mre 622865.21 |gepc 21.01 |\n",
      "scGPT - INFO - | epoch   8 | 3700/3867 batches | lr 0.0000 | ms/batch 612.83 | loss 52.04 | mse 22.29 | mre 698943.84 |gepc 24.10 |\n",
      "scGPT - INFO - | epoch   8 | 3800/3867 batches | lr 0.0000 | ms/batch 604.94 | loss 47.40 | mse 19.36 | mre 606815.48 |gepc 20.72 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   8 | time: 2555.61s | valid loss/mse 23.0197 | mre 622724.4167\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 23.0197\n",
      "random masking at epoch   9, ratio of masked values in train:  0.3999\n",
      "scGPT - INFO - | epoch   9 | 100/3867 batches | lr 0.0000 | ms/batch 616.26 | loss 52.40 | mse 21.20 | mre 711877.08 |gepc 23.27 |\n",
      "scGPT - INFO - | epoch   9 | 200/3867 batches | lr 0.0000 | ms/batch 615.89 | loss 48.98 | mse 20.15 | mre 666596.60 |gepc 22.24 |\n",
      "scGPT - INFO - | epoch   9 | 300/3867 batches | lr 0.0000 | ms/batch 612.57 | loss 44.51 | mse 18.83 | mre 637497.10 |gepc 20.33 |\n",
      "scGPT - INFO - | epoch   9 | 400/3867 batches | lr 0.0000 | ms/batch 609.01 | loss 51.92 | mse 21.53 | mre 705557.48 |gepc 23.33 |\n",
      "scGPT - INFO - | epoch   9 | 500/3867 batches | lr 0.0000 | ms/batch 604.92 | loss 55.27 | mse 23.65 | mre 747594.92 |gepc 25.65 |\n",
      "scGPT - INFO - | epoch   9 | 600/3867 batches | lr 0.0000 | ms/batch 612.97 | loss 52.97 | mse 23.45 | mre 741310.59 |gepc 24.75 |\n",
      "scGPT - INFO - | epoch   9 | 700/3867 batches | lr 0.0000 | ms/batch 611.81 | loss 52.70 | mse 23.44 | mre 737325.56 |gepc 24.60 |\n",
      "scGPT - INFO - | epoch   9 | 800/3867 batches | lr 0.0000 | ms/batch 612.99 | loss 52.63 | mse 23.45 | mre 740942.39 |gepc 24.55 |\n",
      "scGPT - INFO - | epoch   9 | 900/3867 batches | lr 0.0000 | ms/batch 613.12 | loss 52.21 | mse 23.29 | mre 736198.93 |gepc 24.32 |\n",
      "scGPT - INFO - | epoch   9 | 1000/3867 batches | lr 0.0000 | ms/batch 612.30 | loss 55.72 | mse 23.69 | mre 756193.11 |gepc 25.71 |\n",
      "scGPT - INFO - | epoch   9 | 1100/3867 batches | lr 0.0000 | ms/batch 606.83 | loss 52.89 | mse 23.27 | mre 732592.87 |gepc 24.81 |\n",
      "scGPT - INFO - | epoch   9 | 1200/3867 batches | lr 0.0000 | ms/batch 613.23 | loss 52.36 | mse 23.21 | mre 730421.80 |gepc 24.56 |\n",
      "scGPT - INFO - | epoch   9 | 1300/3867 batches | lr 0.0000 | ms/batch 612.13 | loss 52.09 | mse 23.13 | mre 731300.19 |gepc 24.41 |\n",
      "scGPT - INFO - | epoch   9 | 1400/3867 batches | lr 0.0000 | ms/batch 609.57 | loss 52.02 | mse 23.06 | mre 724645.25 |gepc 24.31 |\n",
      "scGPT - INFO - | epoch   9 | 1500/3867 batches | lr 0.0000 | ms/batch 615.74 | loss 45.89 | mse 19.49 | mre 581907.91 |gepc 20.87 |\n",
      "scGPT - INFO - | epoch   9 | 1600/3867 batches | lr 0.0000 | ms/batch 605.25 | loss 43.80 | mse 19.22 | mre 572838.21 |gepc 20.18 |\n",
      "scGPT - INFO - | epoch   9 | 1700/3867 batches | lr 0.0000 | ms/batch 609.64 | loss 43.94 | mse 19.40 | mre 577876.36 |gepc 20.24 |\n",
      "scGPT - INFO - | epoch   9 | 1800/3867 batches | lr 0.0000 | ms/batch 610.24 | loss 43.70 | mse 19.31 | mre 574053.58 |gepc 20.10 |\n",
      "scGPT - INFO - | epoch   9 | 1900/3867 batches | lr 0.0000 | ms/batch 613.46 | loss 43.69 | mse 19.32 | mre 577368.05 |gepc 20.09 |\n",
      "scGPT - INFO - | epoch   9 | 2000/3867 batches | lr 0.0000 | ms/batch 614.18 | loss 43.25 | mse 19.12 | mre 570265.71 |gepc 19.85 |\n",
      "scGPT - INFO - | epoch   9 | 2100/3867 batches | lr 0.0000 | ms/batch 615.15 | loss 43.38 | mse 19.20 | mre 569527.42 |gepc 19.90 |\n",
      "scGPT - INFO - | epoch   9 | 2200/3867 batches | lr 0.0000 | ms/batch 615.20 | loss 43.03 | mse 19.04 | mre 566458.07 |gepc 19.72 |\n",
      "scGPT - INFO - | epoch   9 | 2300/3867 batches | lr 0.0000 | ms/batch 619.94 | loss 43.17 | mse 19.12 | mre 568959.64 |gepc 19.80 |\n",
      "scGPT - INFO - | epoch   9 | 2400/3867 batches | lr 0.0000 | ms/batch 613.73 | loss 43.21 | mse 19.15 | mre 571323.40 |gepc 19.79 |\n",
      "scGPT - INFO - | epoch   9 | 2500/3867 batches | lr 0.0000 | ms/batch 612.13 | loss 43.27 | mse 19.17 | mre 568367.52 |gepc 19.85 |\n",
      "scGPT - INFO - | epoch   9 | 2600/3867 batches | lr 0.0000 | ms/batch 610.63 | loss 51.20 | mse 21.64 | mre 671921.09 |gepc 24.06 |\n",
      "scGPT - INFO - | epoch   9 | 2700/3867 batches | lr 0.0000 | ms/batch 614.73 | loss 49.47 | mse 21.48 | mre 675159.14 |gepc 23.23 |\n",
      "scGPT - INFO - | epoch   9 | 2800/3867 batches | lr 0.0000 | ms/batch 614.94 | loss 49.12 | mse 21.48 | mre 676834.97 |gepc 23.03 |\n",
      "scGPT - INFO - | epoch   9 | 2900/3867 batches | lr 0.0000 | ms/batch 620.70 | loss 48.74 | mse 21.38 | mre 674909.52 |gepc 22.81 |\n",
      "scGPT - INFO - | epoch   9 | 3000/3867 batches | lr 0.0000 | ms/batch 621.38 | loss 48.56 | mse 21.32 | mre 672572.34 |gepc 22.72 |\n",
      "scGPT - INFO - | epoch   9 | 3100/3867 batches | lr 0.0000 | ms/batch 622.56 | loss 48.68 | mse 21.41 | mre 675539.75 |gepc 22.77 |\n",
      "scGPT - INFO - | epoch   9 | 3200/3867 batches | lr 0.0000 | ms/batch 617.12 | loss 48.77 | mse 21.46 | mre 677206.14 |gepc 22.82 |\n",
      "scGPT - INFO - | epoch   9 | 3300/3867 batches | lr 0.0000 | ms/batch 612.46 | loss 48.46 | mse 21.34 | mre 672846.16 |gepc 22.64 |\n",
      "scGPT - INFO - | epoch   9 | 3400/3867 batches | lr 0.0000 | ms/batch 612.28 | loss 48.14 | mse 21.19 | mre 668295.37 |gepc 22.47 |\n",
      "scGPT - INFO - | epoch   9 | 3500/3867 batches | lr 0.0000 | ms/batch 603.19 | loss 50.56 | mse 20.15 | mre 659235.50 |gepc 22.92 |\n",
      "scGPT - INFO - | epoch   9 | 3600/3867 batches | lr 0.0000 | ms/batch 608.20 | loss 46.98 | mse 19.40 | mre 624756.29 |gepc 20.69 |\n",
      "scGPT - INFO - | epoch   9 | 3700/3867 batches | lr 0.0000 | ms/batch 611.73 | loss 51.97 | mse 22.34 | mre 696083.46 |gepc 24.06 |\n",
      "scGPT - INFO - | epoch   9 | 3800/3867 batches | lr 0.0000 | ms/batch 605.03 | loss 47.10 | mse 19.37 | mre 612525.22 |gepc 20.59 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   9 | time: 2454.49s | valid loss/mse 22.7528 | mre 606619.1738\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 22.7528\n",
      "random masking at epoch  10, ratio of masked values in train:  0.3999\n",
      "scGPT - INFO - | epoch  10 | 100/3867 batches | lr 0.0000 | ms/batch 613.03 | loss 51.43 | mse 21.01 | mre 703874.41 |gepc 23.06 |\n",
      "scGPT - INFO - | epoch  10 | 200/3867 batches | lr 0.0000 | ms/batch 610.88 | loss 48.34 | mse 20.02 | mre 668152.01 |gepc 21.91 |\n",
      "scGPT - INFO - | epoch  10 | 300/3867 batches | lr 0.0000 | ms/batch 610.96 | loss 44.65 | mse 18.86 | mre 635926.39 |gepc 20.39 |\n",
      "scGPT - INFO - | epoch  10 | 400/3867 batches | lr 0.0000 | ms/batch 608.04 | loss 51.98 | mse 21.49 | mre 702975.89 |gepc 23.38 |\n",
      "scGPT - INFO - | epoch  10 | 500/3867 batches | lr 0.0000 | ms/batch 609.19 | loss 54.27 | mse 23.33 | mre 738008.87 |gepc 25.16 |\n",
      "scGPT - INFO - | epoch  10 | 600/3867 batches | lr 0.0000 | ms/batch 612.70 | loss 52.71 | mse 23.37 | mre 732539.21 |gepc 24.59 |\n",
      "scGPT - INFO - | epoch  10 | 700/3867 batches | lr 0.0000 | ms/batch 612.79 | loss 52.53 | mse 23.37 | mre 736050.62 |gepc 24.50 |\n",
      "scGPT - INFO - | epoch  10 | 800/3867 batches | lr 0.0000 | ms/batch 614.31 | loss 52.55 | mse 23.41 | mre 736114.45 |gepc 24.53 |\n",
      "scGPT - INFO - | epoch  10 | 900/3867 batches | lr 0.0000 | ms/batch 613.10 | loss 52.37 | mse 23.37 | mre 732746.26 |gepc 24.41 |\n",
      "scGPT - INFO - | epoch  10 | 1000/3867 batches | lr 0.0000 | ms/batch 613.39 | loss 55.39 | mse 23.63 | mre 754769.90 |gepc 25.62 |\n",
      "scGPT - INFO - | epoch  10 | 1100/3867 batches | lr 0.0000 | ms/batch 613.98 | loss 52.61 | mse 23.15 | mre 730474.58 |gepc 24.68 |\n",
      "scGPT - INFO - | epoch  10 | 1200/3867 batches | lr 0.0000 | ms/batch 614.39 | loss 52.41 | mse 23.25 | mre 733538.02 |gepc 24.57 |\n",
      "scGPT - INFO - | epoch  10 | 1300/3867 batches | lr 0.0000 | ms/batch 612.10 | loss 52.01 | mse 23.10 | mre 730074.88 |gepc 24.37 |\n",
      "scGPT - INFO - | epoch  10 | 1400/3867 batches | lr 0.0000 | ms/batch 614.66 | loss 51.75 | mse 22.92 | mre 722040.62 |gepc 24.22 |\n",
      "scGPT - INFO - | epoch  10 | 1500/3867 batches | lr 0.0000 | ms/batch 614.92 | loss 46.57 | mse 19.59 | mre 586823.95 |gepc 21.19 |\n",
      "scGPT - INFO - | epoch  10 | 1600/3867 batches | lr 0.0000 | ms/batch 615.85 | loss 43.92 | mse 19.25 | mre 570633.83 |gepc 20.27 |\n",
      "scGPT - INFO - | epoch  10 | 1700/3867 batches | lr 0.0000 | ms/batch 620.21 | loss 43.59 | mse 19.20 | mre 571356.45 |gepc 20.06 |\n",
      "scGPT - INFO - | epoch  10 | 1800/3867 batches | lr 0.0000 | ms/batch 622.94 | loss 43.56 | mse 19.26 | mre 573006.93 |gepc 20.03 |\n",
      "scGPT - INFO - | epoch  10 | 1900/3867 batches | lr 0.0000 | ms/batch 617.38 | loss 43.38 | mse 19.17 | mre 569311.66 |gepc 19.94 |\n",
      "scGPT - INFO - | epoch  10 | 2000/3867 batches | lr 0.0000 | ms/batch 615.17 | loss 43.15 | mse 19.08 | mre 569294.78 |gepc 19.81 |\n",
      "scGPT - INFO - | epoch  10 | 2100/3867 batches | lr 0.0000 | ms/batch 614.29 | loss 43.15 | mse 19.11 | mre 569499.39 |gepc 19.79 |\n",
      "scGPT - INFO - | epoch  10 | 2200/3867 batches | lr 0.0000 | ms/batch 616.07 | loss 43.35 | mse 19.20 | mre 572252.67 |gepc 19.89 |\n",
      "scGPT - INFO - | epoch  10 | 2300/3867 batches | lr 0.0000 | ms/batch 612.68 | loss 42.86 | mse 18.97 | mre 565505.46 |gepc 19.65 |\n",
      "scGPT - INFO - | epoch  10 | 2400/3867 batches | lr 0.0000 | ms/batch 613.55 | loss 43.43 | mse 19.26 | mre 572079.92 |gepc 19.92 |\n",
      "scGPT - INFO - | epoch  10 | 2500/3867 batches | lr 0.0000 | ms/batch 613.54 | loss 42.96 | mse 19.03 | mre 566620.84 |gepc 19.68 |\n",
      "scGPT - INFO - | epoch  10 | 2600/3867 batches | lr 0.0000 | ms/batch 611.86 | loss 50.42 | mse 21.42 | mre 663987.79 |gepc 23.61 |\n",
      "scGPT - INFO - | epoch  10 | 2700/3867 batches | lr 0.0000 | ms/batch 609.45 | loss 49.21 | mse 21.35 | mre 675397.08 |gepc 23.07 |\n",
      "scGPT - INFO - | epoch  10 | 2800/3867 batches | lr 0.0000 | ms/batch 616.27 | loss 48.92 | mse 21.41 | mre 674799.78 |gepc 22.94 |\n",
      "scGPT - INFO - | epoch  10 | 2900/3867 batches | lr 0.0000 | ms/batch 614.43 | loss 48.66 | mse 21.37 | mre 674265.40 |gepc 22.77 |\n",
      "scGPT - INFO - | epoch  10 | 3000/3867 batches | lr 0.0000 | ms/batch 613.10 | loss 48.50 | mse 21.31 | mre 675541.63 |gepc 22.69 |\n",
      "scGPT - INFO - | epoch  10 | 3100/3867 batches | lr 0.0000 | ms/batch 611.79 | loss 48.50 | mse 21.35 | mre 669930.17 |gepc 22.68 |\n",
      "scGPT - INFO - | epoch  10 | 3200/3867 batches | lr 0.0000 | ms/batch 611.64 | loss 48.58 | mse 21.40 | mre 675955.62 |gepc 22.72 |\n",
      "scGPT - INFO - | epoch  10 | 3300/3867 batches | lr 0.0000 | ms/batch 611.40 | loss 48.25 | mse 21.26 | mre 671653.39 |gepc 22.53 |\n",
      "scGPT - INFO - | epoch  10 | 3400/3867 batches | lr 0.0000 | ms/batch 612.83 | loss 48.37 | mse 21.31 | mre 670885.06 |gepc 22.59 |\n",
      "scGPT - INFO - | epoch  10 | 3500/3867 batches | lr 0.0000 | ms/batch 604.36 | loss 50.22 | mse 20.11 | mre 653834.35 |gepc 23.11 |\n",
      "scGPT - INFO - | epoch  10 | 3600/3867 batches | lr 0.0000 | ms/batch 605.53 | loss 46.77 | mse 19.30 | mre 615808.41 |gepc 20.64 |\n",
      "scGPT - INFO - | epoch  10 | 3700/3867 batches | lr 0.0000 | ms/batch 612.51 | loss 51.44 | mse 22.14 | mre 692752.74 |gepc 23.81 |\n",
      "scGPT - INFO - | epoch  10 | 3800/3867 batches | lr 0.0000 | ms/batch 604.83 | loss 47.01 | mse 19.31 | mre 601061.09 |gepc 20.62 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  10 | time: 2455.59s | valid loss/mse 22.6872 | mre 619526.1075\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 22.6872\n",
      "random masking at epoch  11, ratio of masked values in train:  0.3999\n",
      "scGPT - INFO - | epoch  11 | 100/3867 batches | lr 0.0000 | ms/batch 616.80 | loss 51.31 | mse 21.00 | mre 700647.52 |gepc 22.93 |\n",
      "scGPT - INFO - | epoch  11 | 200/3867 batches | lr 0.0000 | ms/batch 610.95 | loss 48.62 | mse 20.08 | mre 666689.45 |gepc 22.07 |\n",
      "scGPT - INFO - | epoch  11 | 300/3867 batches | lr 0.0000 | ms/batch 606.58 | loss 44.63 | mse 18.88 | mre 638632.88 |gepc 20.40 |\n",
      "scGPT - INFO - | epoch  11 | 400/3867 batches | lr 0.0000 | ms/batch 610.78 | loss 52.02 | mse 21.53 | mre 701034.97 |gepc 23.36 |\n",
      "scGPT - INFO - | epoch  11 | 500/3867 batches | lr 0.0000 | ms/batch 614.92 | loss 54.28 | mse 23.39 | mre 742351.40 |gepc 25.16 |\n",
      "scGPT - INFO - | epoch  11 | 600/3867 batches | lr 0.0000 | ms/batch 620.82 | loss 52.73 | mse 23.36 | mre 733936.02 |gepc 24.61 |\n",
      "scGPT - INFO - | epoch  11 | 700/3867 batches | lr 0.0000 | ms/batch 615.17 | loss 52.36 | mse 23.27 | mre 733582.96 |gepc 24.43 |\n",
      "scGPT - INFO - | epoch  11 | 800/3867 batches | lr 0.0000 | ms/batch 612.73 | loss 52.16 | mse 23.25 | mre 730247.40 |gepc 24.32 |\n",
      "scGPT - INFO - | epoch  11 | 900/3867 batches | lr 0.0000 | ms/batch 611.92 | loss 51.94 | mse 23.12 | mre 728324.19 |gepc 24.20 |\n",
      "scGPT - INFO - | epoch  11 | 1000/3867 batches | lr 0.0000 | ms/batch 616.48 | loss 55.29 | mse 23.61 | mre 754524.71 |gepc 25.60 |\n",
      "scGPT - INFO - | epoch  11 | 1100/3867 batches | lr 0.0000 | ms/batch 614.82 | loss 52.62 | mse 23.14 | mre 731823.37 |gepc 24.66 |\n",
      "scGPT - INFO - | epoch  11 | 1200/3867 batches | lr 0.0000 | ms/batch 613.28 | loss 52.22 | mse 23.13 | mre 727250.17 |gepc 24.48 |\n",
      "scGPT - INFO - | epoch  11 | 1300/3867 batches | lr 0.0000 | ms/batch 613.25 | loss 51.95 | mse 23.07 | mre 728770.45 |gepc 24.34 |\n",
      "scGPT - INFO - | epoch  11 | 1400/3867 batches | lr 0.0000 | ms/batch 613.70 | loss 51.78 | mse 22.91 | mre 718312.21 |gepc 24.27 |\n",
      "scGPT - INFO - | epoch  11 | 1500/3867 batches | lr 0.0000 | ms/batch 611.88 | loss 46.18 | mse 19.49 | mre 580607.90 |gepc 20.97 |\n",
      "scGPT - INFO - | epoch  11 | 1600/3867 batches | lr 0.0000 | ms/batch 612.49 | loss 44.14 | mse 19.31 | mre 576039.00 |gepc 20.37 |\n",
      "scGPT - INFO - | epoch  11 | 1700/3867 batches | lr 0.0000 | ms/batch 614.13 | loss 43.38 | mse 19.12 | mre 566763.10 |gepc 19.96 |\n",
      "scGPT - INFO - | epoch  11 | 1800/3867 batches | lr 0.0000 | ms/batch 614.93 | loss 43.25 | mse 19.10 | mre 567793.89 |gepc 19.88 |\n",
      "scGPT - INFO - | epoch  11 | 1900/3867 batches | lr 0.0000 | ms/batch 617.99 | loss 43.35 | mse 19.15 | mre 569498.26 |gepc 19.94 |\n",
      "scGPT - INFO - | epoch  11 | 2000/3867 batches | lr 0.0000 | ms/batch 611.18 | loss 43.23 | mse 19.12 | mre 572380.10 |gepc 19.85 |\n",
      "scGPT - INFO - | epoch  11 | 2100/3867 batches | lr 0.0000 | ms/batch 616.27 | loss 42.98 | mse 19.00 | mre 564783.13 |gepc 19.73 |\n",
      "scGPT - INFO - | epoch  11 | 2200/3867 batches | lr 0.0000 | ms/batch 608.23 | loss 43.26 | mse 19.16 | mre 569541.43 |gepc 19.86 |\n",
      "scGPT - INFO - | epoch  11 | 2300/3867 batches | lr 0.0000 | ms/batch 613.70 | loss 42.88 | mse 18.98 | mre 565155.59 |gepc 19.66 |\n",
      "scGPT - INFO - | epoch  11 | 2400/3867 batches | lr 0.0000 | ms/batch 613.44 | loss 43.25 | mse 19.16 | mre 572380.42 |gepc 19.85 |\n",
      "scGPT - INFO - | epoch  11 | 2500/3867 batches | lr 0.0000 | ms/batch 613.27 | loss 43.03 | mse 19.08 | mre 565099.42 |gepc 19.71 |\n",
      "scGPT - INFO - | epoch  11 | 2600/3867 batches | lr 0.0000 | ms/batch 608.81 | loss 50.29 | mse 21.31 | mre 667422.54 |gepc 23.59 |\n",
      "scGPT - INFO - | epoch  11 | 2700/3867 batches | lr 0.0000 | ms/batch 613.58 | loss 49.11 | mse 21.28 | mre 671205.62 |gepc 23.01 |\n",
      "scGPT - INFO - | epoch  11 | 2800/3867 batches | lr 0.0000 | ms/batch 607.55 | loss 48.66 | mse 21.29 | mre 671032.39 |gepc 22.79 |\n",
      "scGPT - INFO - | epoch  11 | 2900/3867 batches | lr 0.0000 | ms/batch 606.09 | loss 48.73 | mse 21.38 | mre 674738.57 |gepc 22.83 |\n",
      "scGPT - INFO - | epoch  11 | 3000/3867 batches | lr 0.0000 | ms/batch 611.53 | loss 48.64 | mse 21.40 | mre 674734.61 |gepc 22.76 |\n",
      "scGPT - INFO - | epoch  11 | 3100/3867 batches | lr 0.0000 | ms/batch 607.15 | loss 48.45 | mse 21.31 | mre 670483.33 |gepc 22.67 |\n",
      "scGPT - INFO - | epoch  11 | 3200/3867 batches | lr 0.0000 | ms/batch 616.77 | loss 48.11 | mse 21.19 | mre 667204.71 |gepc 22.46 |\n",
      "scGPT - INFO - | epoch  11 | 3300/3867 batches | lr 0.0000 | ms/batch 615.29 | loss 48.01 | mse 21.14 | mre 668027.72 |gepc 22.42 |\n",
      "scGPT - INFO - | epoch  11 | 3400/3867 batches | lr 0.0000 | ms/batch 615.94 | loss 48.29 | mse 21.31 | mre 671818.02 |gepc 22.56 |\n",
      "scGPT - INFO - | epoch  11 | 3500/3867 batches | lr 0.0000 | ms/batch 605.19 | loss 48.80 | mse 19.88 | mre 650035.06 |gepc 22.06 |\n",
      "scGPT - INFO - | epoch  11 | 3600/3867 batches | lr 0.0000 | ms/batch 608.02 | loss 46.78 | mse 19.33 | mre 608545.70 |gepc 20.60 |\n",
      "scGPT - INFO - | epoch  11 | 3700/3867 batches | lr 0.0000 | ms/batch 613.00 | loss 53.39 | mse 22.17 | mre 693758.37 |gepc 24.42 |\n",
      "scGPT - INFO - | epoch  11 | 3800/3867 batches | lr 0.0000 | ms/batch 599.15 | loss 50.14 | mse 19.39 | mre 607828.32 |gepc 22.39 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  11 | time: 2453.54s | valid loss/mse 22.5576 | mre 580155.9601\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 22.5576\n",
      "random masking at epoch  12, ratio of masked values in train:  0.3999\n",
      "scGPT - INFO - | epoch  12 | 100/3867 batches | lr 0.0000 | ms/batch 614.48 | loss 51.79 | mse 20.92 | mre 696803.86 |gepc 23.13 |\n",
      "scGPT - INFO - | epoch  12 | 200/3867 batches | lr 0.0000 | ms/batch 609.86 | loss 49.33 | mse 20.13 | mre 673324.90 |gepc 22.28 |\n",
      "scGPT - INFO - | epoch  12 | 300/3867 batches | lr 0.0000 | ms/batch 606.51 | loss 44.66 | mse 18.73 | mre 630690.74 |gepc 20.35 |\n",
      "scGPT - INFO - | epoch  12 | 400/3867 batches | lr 0.0000 | ms/batch 597.66 | loss 51.96 | mse 21.43 | mre 704258.46 |gepc 23.39 |\n",
      "scGPT - INFO - | epoch  12 | 500/3867 batches | lr 0.0000 | ms/batch 604.19 | loss 54.80 | mse 23.51 | mre 733241.95 |gepc 25.33 |\n",
      "scGPT - INFO - | epoch  12 | 600/3867 batches | lr 0.0000 | ms/batch 613.37 | loss 52.69 | mse 23.33 | mre 735557.81 |gepc 24.61 |\n",
      "scGPT - INFO - | epoch  12 | 700/3867 batches | lr 0.0000 | ms/batch 612.34 | loss 51.92 | mse 23.07 | mre 724428.30 |gepc 24.20 |\n",
      "scGPT - INFO - | epoch  12 | 800/3867 batches | lr 0.0000 | ms/batch 612.70 | loss 52.16 | mse 23.23 | mre 734580.71 |gepc 24.33 |\n",
      "scGPT - INFO - | epoch  12 | 900/3867 batches | lr 0.0000 | ms/batch 612.72 | loss 51.98 | mse 23.15 | mre 729468.44 |gepc 24.24 |\n",
      "scGPT - INFO - | epoch  12 | 1000/3867 batches | lr 0.0000 | ms/batch 611.26 | loss 55.41 | mse 23.59 | mre 752693.81 |gepc 25.57 |\n",
      "scGPT - INFO - | epoch  12 | 1100/3867 batches | lr 0.0000 | ms/batch 610.90 | loss 52.94 | mse 23.21 | mre 732775.75 |gepc 24.81 |\n",
      "scGPT - INFO - | epoch  12 | 1200/3867 batches | lr 0.0000 | ms/batch 614.93 | loss 52.02 | mse 22.99 | mre 727022.86 |gepc 24.39 |\n",
      "scGPT - INFO - | epoch  12 | 1300/3867 batches | lr 0.0000 | ms/batch 613.19 | loss 51.96 | mse 23.04 | mre 728460.92 |gepc 24.36 |\n",
      "scGPT - INFO - | epoch  12 | 1400/3867 batches | lr 0.0000 | ms/batch 611.42 | loss 51.59 | mse 22.85 | mre 720715.73 |gepc 24.16 |\n",
      "scGPT - INFO - | epoch  12 | 1500/3867 batches | lr 0.0000 | ms/batch 615.01 | loss 46.44 | mse 19.46 | mre 580647.93 |gepc 21.03 |\n",
      "scGPT - INFO - | epoch  12 | 1600/3867 batches | lr 0.0000 | ms/batch 613.86 | loss 43.83 | mse 19.10 | mre 567302.92 |gepc 20.20 |\n",
      "scGPT - INFO - | epoch  12 | 1700/3867 batches | lr 0.0000 | ms/batch 617.98 | loss 43.16 | mse 18.98 | mre 563708.62 |gepc 19.87 |\n",
      "scGPT - INFO - | epoch  12 | 1800/3867 batches | lr 0.0000 | ms/batch 615.57 | loss 43.40 | mse 19.15 | mre 568247.68 |gepc 19.97 |\n",
      "scGPT - INFO - | epoch  12 | 1900/3867 batches | lr 0.0000 | ms/batch 613.50 | loss 43.04 | mse 19.00 | mre 565403.23 |gepc 19.79 |\n",
      "scGPT - INFO - | epoch  12 | 2000/3867 batches | lr 0.0000 | ms/batch 610.69 | loss 43.17 | mse 19.09 | mre 569221.35 |gepc 19.82 |\n",
      "scGPT - INFO - | epoch  12 | 2100/3867 batches | lr 0.0000 | ms/batch 610.39 | loss 43.38 | mse 19.20 | mre 566995.59 |gepc 19.93 |\n",
      "scGPT - INFO - | epoch  12 | 2200/3867 batches | lr 0.0000 | ms/batch 609.98 | loss 43.31 | mse 19.18 | mre 573407.46 |gepc 19.89 |\n",
      "scGPT - INFO - | epoch  12 | 2300/3867 batches | lr 0.0000 | ms/batch 610.32 | loss 43.07 | mse 19.07 | mre 569778.84 |gepc 19.77 |\n",
      "scGPT - INFO - | epoch  12 | 2400/3867 batches | lr 0.0000 | ms/batch 613.64 | loss 43.01 | mse 19.06 | mre 569276.52 |gepc 19.72 |\n",
      "scGPT - INFO - | epoch  12 | 2500/3867 batches | lr 0.0000 | ms/batch 614.14 | loss 42.91 | mse 19.01 | mre 564820.48 |gepc 19.67 |\n",
      "scGPT - INFO - | epoch  12 | 2600/3867 batches | lr 0.0000 | ms/batch 610.30 | loss 50.51 | mse 21.15 | mre 663080.19 |gepc 23.40 |\n",
      "scGPT - INFO - | epoch  12 | 2700/3867 batches | lr 0.0000 | ms/batch 610.31 | loss 50.25 | mse 21.44 | mre 677024.26 |gepc 23.48 |\n",
      "scGPT - INFO - | epoch  12 | 2800/3867 batches | lr 0.0000 | ms/batch 615.36 | loss 49.44 | mse 21.46 | mre 677625.28 |gepc 23.13 |\n",
      "scGPT - INFO - | epoch  12 | 2900/3867 batches | lr 0.0000 | ms/batch 615.88 | loss 48.64 | mse 21.27 | mre 670301.64 |gepc 22.77 |\n",
      "scGPT - INFO - | epoch  12 | 3000/3867 batches | lr 0.0000 | ms/batch 613.45 | loss 48.32 | mse 21.19 | mre 666415.08 |gepc 22.60 |\n",
      "scGPT - INFO - | epoch  12 | 3100/3867 batches | lr 0.0000 | ms/batch 612.00 | loss 48.29 | mse 21.22 | mre 671211.06 |gepc 22.58 |\n",
      "scGPT - INFO - | epoch  12 | 3200/3867 batches | lr 0.0000 | ms/batch 613.39 | loss 48.06 | mse 21.14 | mre 664386.69 |gepc 22.47 |\n",
      "scGPT - INFO - | epoch  12 | 3300/3867 batches | lr 0.0000 | ms/batch 613.19 | loss 48.41 | mse 21.32 | mre 674979.62 |gepc 22.64 |\n",
      "scGPT - INFO - | epoch  12 | 3400/3867 batches | lr 0.0000 | ms/batch 611.58 | loss 48.11 | mse 21.19 | mre 668944.90 |gepc 22.48 |\n",
      "scGPT - INFO - | epoch  12 | 3500/3867 batches | lr 0.0000 | ms/batch 605.07 | loss 48.68 | mse 19.79 | mre 647837.79 |gepc 21.97 |\n",
      "scGPT - INFO - | epoch  12 | 3600/3867 batches | lr 0.0000 | ms/batch 606.74 | loss 46.65 | mse 19.26 | mre 612511.80 |gepc 20.67 |\n",
      "scGPT - INFO - | epoch  12 | 3700/3867 batches | lr 0.0000 | ms/batch 614.74 | loss 51.96 | mse 22.20 | mre 695292.17 |gepc 24.10 |\n",
      "scGPT - INFO - | epoch  12 | 3800/3867 batches | lr 0.0000 | ms/batch 602.82 | loss 46.90 | mse 19.28 | mre 603890.50 |gepc 20.39 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  12 | time: 2449.42s | valid loss/mse 22.5333 | mre 576834.7301\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 22.5333\n",
      "random masking at epoch  13, ratio of masked values in train:  0.3999\n",
      "scGPT - INFO - | epoch  13 | 100/3867 batches | lr 0.0000 | ms/batch 612.59 | loss 50.88 | mse 20.76 | mre 697259.62 |gepc 22.82 |\n",
      "scGPT - INFO - | epoch  13 | 200/3867 batches | lr 0.0000 | ms/batch 609.83 | loss 48.45 | mse 20.05 | mre 665583.32 |gepc 21.98 |\n",
      "scGPT - INFO - | epoch  13 | 300/3867 batches | lr 0.0000 | ms/batch 603.28 | loss 44.62 | mse 18.73 | mre 632185.16 |gepc 20.37 |\n",
      "scGPT - INFO - | epoch  13 | 400/3867 batches | lr 0.0000 | ms/batch 604.10 | loss 51.85 | mse 21.38 | mre 697325.40 |gepc 23.26 |\n",
      "scGPT - INFO - | epoch  13 | 500/3867 batches | lr 0.0000 | ms/batch 610.46 | loss 53.95 | mse 23.30 | mre 730201.80 |gepc 24.91 |\n",
      "scGPT - INFO - | epoch  13 | 600/3867 batches | lr 0.0000 | ms/batch 618.11 | loss 52.18 | mse 23.13 | mre 724258.61 |gepc 24.34 |\n",
      "scGPT - INFO - | epoch  13 | 700/3867 batches | lr 0.0000 | ms/batch 611.04 | loss 52.18 | mse 23.19 | mre 728338.83 |gepc 24.34 |\n",
      "scGPT - INFO - | epoch  13 | 800/3867 batches | lr 0.0000 | ms/batch 609.57 | loss 52.20 | mse 23.25 | mre 730045.07 |gepc 24.35 |\n",
      "scGPT - INFO - | epoch  13 | 900/3867 batches | lr 0.0000 | ms/batch 611.20 | loss 51.99 | mse 23.18 | mre 729468.58 |gepc 24.24 |\n",
      "scGPT - INFO - | epoch  13 | 1000/3867 batches | lr 0.0000 | ms/batch 609.23 | loss 55.00 | mse 23.47 | mre 743717.80 |gepc 25.34 |\n",
      "scGPT - INFO - | epoch  13 | 1100/3867 batches | lr 0.0000 | ms/batch 613.54 | loss 52.87 | mse 23.12 | mre 731226.68 |gepc 24.74 |\n",
      "scGPT - INFO - | epoch  13 | 1200/3867 batches | lr 0.0000 | ms/batch 611.69 | loss 52.04 | mse 22.99 | mre 726361.34 |gepc 24.40 |\n",
      "scGPT - INFO - | epoch  13 | 1300/3867 batches | lr 0.0000 | ms/batch 608.17 | loss 52.10 | mse 23.09 | mre 730481.95 |gepc 24.42 |\n",
      "scGPT - INFO - | epoch  13 | 1400/3867 batches | lr 0.0000 | ms/batch 613.55 | loss 51.59 | mse 22.83 | mre 718043.74 |gepc 24.13 |\n",
      "scGPT - INFO - | epoch  13 | 1500/3867 batches | lr 0.0000 | ms/batch 609.13 | loss 45.48 | mse 19.30 | mre 576394.71 |gepc 20.67 |\n",
      "scGPT - INFO - | epoch  13 | 1600/3867 batches | lr 0.0000 | ms/batch 605.86 | loss 43.97 | mse 19.16 | mre 569419.07 |gepc 20.24 |\n",
      "scGPT - INFO - | epoch  13 | 1700/3867 batches | lr 0.0000 | ms/batch 606.27 | loss 43.38 | mse 19.12 | mre 571110.52 |gepc 19.95 |\n",
      "scGPT - INFO - | epoch  13 | 1800/3867 batches | lr 0.0000 | ms/batch 609.56 | loss 43.40 | mse 19.15 | mre 567280.06 |gepc 19.97 |\n",
      "scGPT - INFO - | epoch  13 | 1900/3867 batches | lr 0.0000 | ms/batch 613.00 | loss 43.17 | mse 19.05 | mre 567427.85 |gepc 19.86 |\n",
      "scGPT - INFO - | epoch  13 | 2000/3867 batches | lr 0.0000 | ms/batch 611.19 | loss 42.79 | mse 18.89 | mre 560018.09 |gepc 19.64 |\n",
      "scGPT - INFO - | epoch  13 | 2100/3867 batches | lr 0.0000 | ms/batch 613.38 | loss 43.31 | mse 19.16 | mre 570464.54 |gepc 19.90 |\n",
      "scGPT - INFO - | epoch  13 | 2200/3867 batches | lr 0.0000 | ms/batch 612.24 | loss 43.19 | mse 19.12 | mre 570858.11 |gepc 19.82 |\n",
      "scGPT - INFO - | epoch  13 | 2300/3867 batches | lr 0.0000 | ms/batch 612.08 | loss 42.97 | mse 19.01 | mre 564478.76 |gepc 19.72 |\n",
      "scGPT - INFO - | epoch  13 | 2400/3867 batches | lr 0.0000 | ms/batch 611.59 | loss 42.93 | mse 19.00 | mre 566162.90 |gepc 19.68 |\n",
      "scGPT - INFO - | epoch  13 | 2500/3867 batches | lr 0.0000 | ms/batch 616.03 | loss 43.09 | mse 19.09 | mre 566620.69 |gepc 19.77 |\n",
      "scGPT - INFO - | epoch  13 | 2600/3867 batches | lr 0.0000 | ms/batch 609.90 | loss 49.78 | mse 21.34 | mre 657715.33 |gepc 23.22 |\n",
      "scGPT - INFO - | epoch  13 | 2700/3867 batches | lr 0.0000 | ms/batch 612.31 | loss 49.02 | mse 21.31 | mre 674376.02 |gepc 22.92 |\n",
      "scGPT - INFO - | epoch  13 | 2800/3867 batches | lr 0.0000 | ms/batch 611.01 | loss 48.32 | mse 21.12 | mre 668534.59 |gepc 22.58 |\n",
      "scGPT - INFO - | epoch  13 | 2900/3867 batches | lr 0.0000 | ms/batch 608.31 | loss 48.34 | mse 21.19 | mre 668537.17 |gepc 22.61 |\n",
      "scGPT - INFO - | epoch  13 | 3000/3867 batches | lr 0.0000 | ms/batch 607.96 | loss 48.42 | mse 21.27 | mre 672194.36 |gepc 22.64 |\n",
      "scGPT - INFO - | epoch  13 | 3100/3867 batches | lr 0.0000 | ms/batch 607.64 | loss 48.16 | mse 21.15 | mre 670581.35 |gepc 22.52 |\n",
      "scGPT - INFO - | epoch  13 | 3200/3867 batches | lr 0.0000 | ms/batch 614.72 | loss 48.39 | mse 21.29 | mre 671724.47 |gepc 22.65 |\n",
      "scGPT - INFO - | epoch  13 | 3300/3867 batches | lr 0.0000 | ms/batch 617.25 | loss 48.19 | mse 21.23 | mre 670607.76 |gepc 22.51 |\n",
      "scGPT - INFO - | epoch  13 | 3400/3867 batches | lr 0.0000 | ms/batch 610.99 | loss 47.74 | mse 21.01 | mre 663431.13 |gepc 22.29 |\n",
      "scGPT - INFO - | epoch  13 | 3500/3867 batches | lr 0.0000 | ms/batch 601.58 | loss 48.58 | mse 19.85 | mre 657728.74 |gepc 21.91 |\n",
      "scGPT - INFO - | epoch  13 | 3600/3867 batches | lr 0.0000 | ms/batch 607.82 | loss 46.65 | mse 19.24 | mre 612834.05 |gepc 20.60 |\n",
      "scGPT - INFO - | epoch  13 | 3700/3867 batches | lr 0.0000 | ms/batch 612.02 | loss 51.49 | mse 22.02 | mre 685371.58 |gepc 23.81 |\n",
      "scGPT - INFO - | epoch  13 | 3800/3867 batches | lr 0.0000 | ms/batch 602.50 | loss 46.77 | mse 19.29 | mre 600900.06 |gepc 20.49 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  13 | time: 2444.88s | valid loss/mse 22.3563 | mre 576575.1468\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 22.3563\n",
      "random masking at epoch  14, ratio of masked values in train:  0.3999\n",
      "scGPT - INFO - | epoch  14 | 100/3867 batches | lr 0.0000 | ms/batch 610.48 | loss 50.84 | mse 20.68 | mre 692923.21 |gepc 22.69 |\n",
      "scGPT - INFO - | epoch  14 | 200/3867 batches | lr 0.0000 | ms/batch 607.05 | loss 48.39 | mse 19.85 | mre 660340.00 |gepc 21.96 |\n",
      "scGPT - INFO - | epoch  14 | 300/3867 batches | lr 0.0000 | ms/batch 603.19 | loss 44.67 | mse 18.79 | mre 640741.33 |gepc 20.39 |\n",
      "scGPT - INFO - | epoch  14 | 400/3867 batches | lr 0.0000 | ms/batch 605.94 | loss 51.96 | mse 21.40 | mre 700943.16 |gepc 23.25 |\n",
      "scGPT - INFO - | epoch  14 | 500/3867 batches | lr 0.0000 | ms/batch 605.37 | loss 53.90 | mse 23.27 | mre 728277.71 |gepc 24.88 |\n",
      "scGPT - INFO - | epoch  14 | 600/3867 batches | lr 0.0000 | ms/batch 617.18 | loss 51.95 | mse 22.99 | mre 725784.05 |gepc 24.21 |\n",
      "scGPT - INFO - | epoch  14 | 700/3867 batches | lr 0.0000 | ms/batch 611.77 | loss 52.26 | mse 23.21 | mre 732981.75 |gepc 24.36 |\n",
      "scGPT - INFO - | epoch  14 | 800/3867 batches | lr 0.0000 | ms/batch 614.80 | loss 51.99 | mse 23.12 | mre 728661.15 |gepc 24.23 |\n",
      "scGPT - INFO - | epoch  14 | 900/3867 batches | lr 0.0000 | ms/batch 606.95 | loss 51.81 | mse 23.09 | mre 726790.65 |gepc 24.14 |\n",
      "scGPT - INFO - | epoch  14 | 1000/3867 batches | lr 0.0000 | ms/batch 614.26 | loss 54.48 | mse 23.30 | mre 734352.04 |gepc 25.10 |\n",
      "scGPT - INFO - | epoch  14 | 1100/3867 batches | lr 0.0000 | ms/batch 609.36 | loss 52.80 | mse 23.13 | mre 730796.18 |gepc 24.73 |\n",
      "scGPT - INFO - | epoch  14 | 1200/3867 batches | lr 0.0000 | ms/batch 613.58 | loss 51.80 | mse 22.89 | mre 725082.41 |gepc 24.27 |\n",
      "scGPT - INFO - | epoch  14 | 1300/3867 batches | lr 0.0000 | ms/batch 611.74 | loss 52.12 | mse 23.13 | mre 731595.54 |gepc 24.44 |\n",
      "scGPT - INFO - | epoch  14 | 1400/3867 batches | lr 0.0000 | ms/batch 609.96 | loss 51.42 | mse 22.82 | mre 719971.65 |gepc 24.04 |\n",
      "scGPT - INFO - | epoch  14 | 1500/3867 batches | lr 0.0000 | ms/batch 609.56 | loss 45.21 | mse 19.31 | mre 577491.13 |gepc 20.60 |\n",
      "scGPT - INFO - | epoch  14 | 1600/3867 batches | lr 0.0000 | ms/batch 610.16 | loss 43.82 | mse 19.13 | mre 570667.30 |gepc 20.16 |\n",
      "scGPT - INFO - | epoch  14 | 1700/3867 batches | lr 0.0000 | ms/batch 611.17 | loss 43.37 | mse 19.10 | mre 566631.79 |gepc 19.96 |\n",
      "scGPT - INFO - | epoch  14 | 1800/3867 batches | lr 0.0000 | ms/batch 610.33 | loss 43.17 | mse 19.05 | mre 567366.84 |gepc 19.86 |\n",
      "scGPT - INFO - | epoch  14 | 1900/3867 batches | lr 0.0000 | ms/batch 612.46 | loss 43.27 | mse 19.13 | mre 569433.53 |gepc 19.89 |\n",
      "scGPT - INFO - | epoch  14 | 2000/3867 batches | lr 0.0000 | ms/batch 611.24 | loss 43.18 | mse 19.09 | mre 570806.01 |gepc 19.84 |\n",
      "scGPT - INFO - | epoch  14 | 2100/3867 batches | lr 0.0000 | ms/batch 610.38 | loss 42.63 | mse 18.84 | mre 563717.94 |gepc 19.55 |\n",
      "scGPT - INFO - | epoch  14 | 2200/3867 batches | lr 0.0000 | ms/batch 612.14 | loss 43.01 | mse 19.04 | mre 567296.60 |gepc 19.74 |\n",
      "scGPT - INFO - | epoch  14 | 2300/3867 batches | lr 0.0000 | ms/batch 605.34 | loss 42.64 | mse 18.86 | mre 561992.92 |gepc 19.55 |\n",
      "scGPT - INFO - | epoch  14 | 2400/3867 batches | lr 0.0000 | ms/batch 610.83 | loss 42.71 | mse 18.90 | mre 564787.42 |gepc 19.58 |\n",
      "scGPT - INFO - | epoch  14 | 2500/3867 batches | lr 0.0000 | ms/batch 610.72 | loss 43.05 | mse 19.08 | mre 569086.24 |gepc 19.74 |\n",
      "scGPT - INFO - | epoch  14 | 2600/3867 batches | lr 0.0000 | ms/batch 609.47 | loss 48.83 | mse 21.01 | mre 655218.07 |gepc 22.78 |\n",
      "scGPT - INFO - | epoch  14 | 2700/3867 batches | lr 0.0000 | ms/batch 606.34 | loss 48.93 | mse 21.28 | mre 672920.23 |gepc 22.89 |\n",
      "scGPT - INFO - | epoch  14 | 2800/3867 batches | lr 0.0000 | ms/batch 608.43 | loss 48.48 | mse 21.21 | mre 671506.07 |gepc 22.70 |\n",
      "scGPT - INFO - | epoch  14 | 2900/3867 batches | lr 0.0000 | ms/batch 611.68 | loss 48.19 | mse 21.14 | mre 666156.84 |gepc 22.54 |\n",
      "scGPT - INFO - | epoch  14 | 3000/3867 batches | lr 0.0000 | ms/batch 611.72 | loss 48.01 | mse 21.09 | mre 663290.16 |gepc 22.44 |\n",
      "scGPT - INFO - | epoch  14 | 3100/3867 batches | lr 0.0000 | ms/batch 605.44 | loss 48.30 | mse 21.24 | mre 669457.65 |gepc 22.59 |\n",
      "scGPT - INFO - | epoch  14 | 3200/3867 batches | lr 0.0000 | ms/batch 610.39 | loss 48.01 | mse 21.14 | mre 667126.41 |gepc 22.43 |\n",
      "scGPT - INFO - | epoch  14 | 3300/3867 batches | lr 0.0000 | ms/batch 612.31 | loss 48.05 | mse 21.15 | mre 665833.52 |gepc 22.45 |\n",
      "scGPT - INFO - | epoch  14 | 3400/3867 batches | lr 0.0000 | ms/batch 612.06 | loss 47.90 | mse 21.09 | mre 665899.05 |gepc 22.37 |\n",
      "scGPT - INFO - | epoch  14 | 3500/3867 batches | lr 0.0000 | ms/batch 600.18 | loss 48.13 | mse 19.79 | mre 646771.92 |gepc 21.70 |\n",
      "scGPT - INFO - | epoch  14 | 3600/3867 batches | lr 0.0000 | ms/batch 605.34 | loss 46.40 | mse 19.26 | mre 616718.44 |gepc 20.34 |\n",
      "scGPT - INFO - | epoch  14 | 3700/3867 batches | lr 0.0000 | ms/batch 606.83 | loss 51.45 | mse 22.08 | mre 688961.45 |gepc 23.77 |\n",
      "scGPT - INFO - | epoch  14 | 3800/3867 batches | lr 0.0000 | ms/batch 599.23 | loss 46.28 | mse 19.15 | mre 597321.94 |gepc 20.22 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  14 | time: 2442.03s | valid loss/mse 22.4037 | mre 568203.9686\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "random masking at epoch  15, ratio of masked values in train:  0.3999\n",
      "scGPT - INFO - | epoch  15 | 100/3867 batches | lr 0.0000 | ms/batch 609.45 | loss 51.15 | mse 20.76 | mre 696806.39 |gepc 22.81 |\n",
      "scGPT - INFO - | epoch  15 | 200/3867 batches | lr 0.0000 | ms/batch 606.89 | loss 48.81 | mse 19.98 | mre 666270.47 |gepc 22.21 |\n",
      "scGPT - INFO - | epoch  15 | 300/3867 batches | lr 0.0000 | ms/batch 607.78 | loss 44.44 | mse 18.68 | mre 636314.92 |gepc 20.30 |\n",
      "scGPT - INFO - | epoch  15 | 400/3867 batches | lr 0.0000 | ms/batch 600.83 | loss 51.61 | mse 21.32 | mre 699336.57 |gepc 23.13 |\n",
      "scGPT - INFO - | epoch  15 | 500/3867 batches | lr 0.0000 | ms/batch 602.91 | loss 54.09 | mse 23.41 | mre 731159.89 |gepc 25.00 |\n",
      "scGPT - INFO - | epoch  15 | 600/3867 batches | lr 0.0000 | ms/batch 609.85 | loss 52.29 | mse 23.13 | mre 729007.99 |gepc 24.38 |\n",
      "scGPT - INFO - | epoch  15 | 700/3867 batches | lr 0.0000 | ms/batch 609.37 | loss 52.13 | mse 23.17 | mre 727788.04 |gepc 24.30 |\n",
      "scGPT - INFO - | epoch  15 | 800/3867 batches | lr 0.0000 | ms/batch 606.21 | loss 51.91 | mse 23.09 | mre 725163.79 |gepc 24.21 |\n",
      "scGPT - INFO - | epoch  15 | 900/3867 batches | lr 0.0000 | ms/batch 607.42 | loss 51.48 | mse 22.93 | mre 720480.45 |gepc 23.97 |\n",
      "scGPT - INFO - | epoch  15 | 1000/3867 batches | lr 0.0000 | ms/batch 611.64 | loss 54.40 | mse 23.27 | mre 741068.30 |gepc 25.09 |\n",
      "scGPT - INFO - | epoch  15 | 1100/3867 batches | lr 0.0000 | ms/batch 610.26 | loss 52.55 | mse 23.03 | mre 726601.14 |gepc 24.61 |\n",
      "scGPT - INFO - | epoch  15 | 1200/3867 batches | lr 0.0000 | ms/batch 611.51 | loss 51.93 | mse 22.92 | mre 723277.32 |gepc 24.34 |\n",
      "scGPT - INFO - | epoch  15 | 1300/3867 batches | lr 0.0000 | ms/batch 613.48 | loss 52.11 | mse 23.10 | mre 731777.71 |gepc 24.44 |\n",
      "scGPT - INFO - | epoch  15 | 1400/3867 batches | lr 0.0000 | ms/batch 609.33 | loss 51.55 | mse 22.81 | mre 718002.81 |gepc 24.14 |\n",
      "scGPT - INFO - | epoch  15 | 1500/3867 batches | lr 0.0000 | ms/batch 608.42 | loss 45.52 | mse 19.45 | mre 579832.52 |gepc 20.72 |\n",
      "scGPT - INFO - | epoch  15 | 1600/3867 batches | lr 0.0000 | ms/batch 609.09 | loss 43.87 | mse 19.13 | mre 568460.50 |gepc 20.20 |\n",
      "scGPT - INFO - | epoch  15 | 1700/3867 batches | lr 0.0000 | ms/batch 610.74 | loss 43.11 | mse 18.99 | mre 563839.19 |gepc 19.83 |\n",
      "scGPT - INFO - | epoch  15 | 1800/3867 batches | lr 0.0000 | ms/batch 612.25 | loss 43.19 | mse 19.07 | mre 566259.62 |gepc 19.86 |\n",
      "scGPT - INFO - | epoch  15 | 1900/3867 batches | lr 0.0000 | ms/batch 613.69 | loss 43.26 | mse 19.13 | mre 569436.25 |gepc 19.88 |\n",
      "scGPT - INFO - | epoch  15 | 2000/3867 batches | lr 0.0000 | ms/batch 614.06 | loss 42.97 | mse 19.00 | mre 566292.14 |gepc 19.73 |\n",
      "scGPT - INFO - | epoch  15 | 2100/3867 batches | lr 0.0000 | ms/batch 614.00 | loss 43.02 | mse 19.04 | mre 568246.39 |gepc 19.74 |\n",
      "scGPT - INFO - | epoch  15 | 2200/3867 batches | lr 0.0000 | ms/batch 609.37 | loss 42.96 | mse 19.00 | mre 565200.59 |gepc 19.71 |\n",
      "scGPT - INFO - | epoch  15 | 2300/3867 batches | lr 0.0000 | ms/batch 611.47 | loss 42.63 | mse 18.85 | mre 562699.41 |gepc 19.56 |\n",
      "scGPT - INFO - | epoch  15 | 2400/3867 batches | lr 0.0000 | ms/batch 612.18 | loss 43.00 | mse 19.05 | mre 567631.81 |gepc 19.73 |\n",
      "scGPT - INFO - | epoch  15 | 2500/3867 batches | lr 0.0000 | ms/batch 612.69 | loss 42.57 | mse 18.83 | mre 559475.79 |gepc 19.51 |\n",
      "scGPT - INFO - | epoch  15 | 2600/3867 batches | lr 0.0000 | ms/batch 606.85 | loss 49.10 | mse 21.20 | mre 663922.59 |gepc 22.91 |\n",
      "scGPT - INFO - | epoch  15 | 2700/3867 batches | lr 0.0000 | ms/batch 608.73 | loss 48.65 | mse 21.18 | mre 668766.20 |gepc 22.74 |\n",
      "scGPT - INFO - | epoch  15 | 2800/3867 batches | lr 0.0000 | ms/batch 612.70 | loss 48.51 | mse 21.23 | mre 669230.72 |gepc 22.68 |\n",
      "scGPT - INFO - | epoch  15 | 2900/3867 batches | lr 0.0000 | ms/batch 613.39 | loss 48.38 | mse 21.23 | mre 668889.92 |gepc 22.64 |\n",
      "scGPT - INFO - | epoch  15 | 3000/3867 batches | lr 0.0000 | ms/batch 613.42 | loss 48.07 | mse 21.11 | mre 666911.43 |gepc 22.47 |\n",
      "scGPT - INFO - | epoch  15 | 3100/3867 batches | lr 0.0000 | ms/batch 606.59 | loss 47.97 | mse 21.09 | mre 667775.72 |gepc 22.42 |\n",
      "scGPT - INFO - | epoch  15 | 3200/3867 batches | lr 0.0000 | ms/batch 613.71 | loss 48.03 | mse 21.14 | mre 665321.71 |gepc 22.43 |\n",
      "scGPT - INFO - | epoch  15 | 3300/3867 batches | lr 0.0000 | ms/batch 612.06 | loss 47.72 | mse 21.01 | mre 664275.41 |gepc 22.27 |\n",
      "scGPT - INFO - | epoch  15 | 3400/3867 batches | lr 0.0000 | ms/batch 612.29 | loss 47.81 | mse 21.05 | mre 665539.38 |gepc 22.32 |\n",
      "scGPT - INFO - | epoch  15 | 3500/3867 batches | lr 0.0000 | ms/batch 601.27 | loss 48.00 | mse 19.76 | mre 648240.61 |gepc 21.60 |\n",
      "scGPT - INFO - | epoch  15 | 3600/3867 batches | lr 0.0000 | ms/batch 605.37 | loss 46.35 | mse 19.23 | mre 618763.32 |gepc 20.36 |\n",
      "scGPT - INFO - | epoch  15 | 3700/3867 batches | lr 0.0000 | ms/batch 617.80 | loss 51.33 | mse 22.03 | mre 686387.97 |gepc 23.64 |\n",
      "scGPT - INFO - | epoch  15 | 3800/3867 batches | lr 0.0000 | ms/batch 598.84 | loss 46.71 | mse 19.24 | mre 603335.48 |gepc 20.44 |\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch  15 | time: 2441.84s | valid loss/mse 22.2974 | mre 608023.6081\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 22.2974\n",
      "scGPT - INFO - Saving model to save/dev_COVID-11月03-15-20\n",
      "scGPT - INFO - Evaluating cls cell embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4287/4287 [12:25<00:00,  5.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - ERROR - column str_batch is not in obs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1241973/399840806.py\", line 228, in eval_testdata\n",
      "    results = eval_scib_metrics(adata_t)\n",
      "  File \"/data1/chenyx/anaconda3/envs/scGPT/lib/python3.8/site-packages/scgpt/utils/util.py\", line 313, in eval_scib_metrics\n",
      "    results = scib.metrics.metrics(\n",
      "  File \"/data1/chenyx/anaconda3/envs/scGPT/lib/python3.8/site-packages/scib/metrics/metrics.py\", line 284, in metrics\n",
      "    check_batch(batch_key, adata.obs)\n",
      "  File \"/data1/chenyx/anaconda3/envs/scGPT/lib/python3.8/site-packages/scib/utils.py\", line 12, in check_batch\n",
      "    raise ValueError(f\"column {batch} is not in obs\")\n",
      "ValueError: column str_batch is not in obs\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Could not find key str_batch in .var_names or .obs.columns.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 52\u001b[0m\n\u001b[1;32m     49\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(best_model\u001b[38;5;241m.\u001b[39mstate_dict(), save_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_e\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_model_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# eval on testdata\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43meval_testdata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbest_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43madata_t\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madata_sorted\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mper_seq_batch_sample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43madata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_umap\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msavefig(\n\u001b[1;32m     58\u001b[0m     save_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings_batch_umap[cls]_e\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_model_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m, dpi\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m\n\u001b[1;32m     59\u001b[0m )\n\u001b[1;32m     61\u001b[0m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcelltype_umap\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msavefig(\n\u001b[1;32m     62\u001b[0m     save_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings_celltype_umap[cls]_e\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_model_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m, dpi\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m\n\u001b[1;32m     63\u001b[0m )\n",
      "Cell \u001b[0;32mIn[23], line 235\u001b[0m, in \u001b[0;36meval_testdata\u001b[0;34m(model, adata_t, include_types)\u001b[0m\n\u001b[1;32m    233\u001b[0m sc\u001b[38;5;241m.\u001b[39mpp\u001b[38;5;241m.\u001b[39mneighbors(adata_t, use_rep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX_scGPT\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    234\u001b[0m sc\u001b[38;5;241m.\u001b[39mtl\u001b[38;5;241m.\u001b[39mumap(adata_t, min_dist\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m)\n\u001b[0;32m--> 235\u001b[0m fig \u001b[38;5;241m=\u001b[39m \u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mumap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43madata_t\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstr_batch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtitle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch, avg_bio = \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mresults\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mavg_bio\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m:\u001b[39;49;00m\u001b[38;5;124;43m.4f\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframeon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_fig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_umap\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m fig\n\u001b[1;32m    246\u001b[0m sc\u001b[38;5;241m.\u001b[39mpp\u001b[38;5;241m.\u001b[39mneighbors(adata_t, use_rep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX_scGPT\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/data1/chenyx/anaconda3/envs/scGPT/lib/python3.8/site-packages/scanpy/plotting/_tools/scatterplots.py:668\u001b[0m, in \u001b[0;36mumap\u001b[0;34m(adata, **kwargs)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;129m@_wraps_plot_scatter\u001b[39m\n\u001b[1;32m    610\u001b[0m \u001b[38;5;129m@_doc_params\u001b[39m(\n\u001b[1;32m    611\u001b[0m     adata_color_etc\u001b[38;5;241m=\u001b[39mdoc_adata_color_etc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    615\u001b[0m )\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mumap\u001b[39m(adata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Axes, List[Axes], \u001b[38;5;28;01mNone\u001b[39;00m]:\n\u001b[1;32m    617\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\\\u001b[39;00m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;124;03m    Scatter plot in UMAP basis.\u001b[39;00m\n\u001b[1;32m    619\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;124;03m    tl.umap\u001b[39;00m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 668\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43madata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mumap\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data1/chenyx/anaconda3/envs/scGPT/lib/python3.8/site-packages/scanpy/plotting/_tools/scatterplots.py:256\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, groups, components, dimensions, layer, projection, scale_factor, color_map, cmap, palette, na_color, na_in_legend, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, colorbar_loc, vmax, vmin, vcenter, norm, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# use itertools.product to make a plot for each color and for each component\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# For example if color=[gene1, gene2] and components=['1,2, '2,3'].\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# The plots are: [\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;66;03m#     color=gene1, components=[1,2], color=gene1, components=[2,3],\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m#     color=gene2, components = [1, 2], color=gene2, components=[2,3],\u001b[39;00m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;66;03m# ]\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m count, (value_to_plot, dims) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(color, dimensions)):\n\u001b[0;32m--> 256\u001b[0m     color_source_vector \u001b[38;5;241m=\u001b[39m \u001b[43m_get_color_source_vector\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43madata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue_to_plot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_raw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgene_symbols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgene_symbols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    264\u001b[0m     color_vector, categorical \u001b[38;5;241m=\u001b[39m _color_vector(\n\u001b[1;32m    265\u001b[0m         adata,\n\u001b[1;32m    266\u001b[0m         value_to_plot,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    269\u001b[0m         na_color\u001b[38;5;241m=\u001b[39mna_color,\n\u001b[1;32m    270\u001b[0m     )\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;66;03m# Order points\u001b[39;00m\n",
      "File \u001b[0;32m/data1/chenyx/anaconda3/envs/scGPT/lib/python3.8/site-packages/scanpy/plotting/_tools/scatterplots.py:1168\u001b[0m, in \u001b[0;36m_get_color_source_vector\u001b[0;34m(adata, value_to_plot, use_raw, gene_symbols, layer, groups)\u001b[0m\n\u001b[1;32m   1166\u001b[0m     values \u001b[38;5;241m=\u001b[39m adata\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mobs_vector(value_to_plot)\n\u001b[1;32m   1167\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1168\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43madata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobs_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue_to_plot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m groups \u001b[38;5;129;01mand\u001b[39;00m is_categorical_dtype(values):\n\u001b[1;32m   1170\u001b[0m     values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mremove_categories(values\u001b[38;5;241m.\u001b[39mcategories\u001b[38;5;241m.\u001b[39mdifference(groups))\n",
      "File \u001b[0;32m/data1/chenyx/anaconda3/envs/scGPT/lib/python3.8/site-packages/anndata/_core/anndata.py:1383\u001b[0m, in \u001b[0;36mAnnData.obs_vector\u001b[0;34m(self, k, layer)\u001b[0m\n\u001b[1;32m   1377\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1378\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn a future version of AnnData, access to `.X` by passing\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1379\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `layer=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m` will be removed. Instead pass `layer=None`.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1380\u001b[0m             \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m   1381\u001b[0m         )\n\u001b[1;32m   1382\u001b[0m         layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1383\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvar\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data1/chenyx/anaconda3/envs/scGPT/lib/python3.8/site-packages/anndata/_core/index.py:191\u001b[0m, in \u001b[0;36mget_vector\u001b[0;34m(adata, k, coldim, idxdim, layer)\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    188\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m could be found in both .\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midxdim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_names and .\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcoldim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.columns\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    189\u001b[0m     )\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (in_col \u001b[38;5;241m+\u001b[39m in_idx) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 191\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[1;32m    192\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find key \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in .\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midxdim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_names or .\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcoldim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.columns.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    193\u001b[0m     )\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m in_col:\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(adata, coldim)[k]\u001b[38;5;241m.\u001b[39mvalues\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Could not find key str_batch in .var_names or .obs.columns.'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 320x320 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 293,
       "width": 316
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "best_avg_bio = 0.0\n",
    "best_model = None\n",
    "define_wandb_metrcis()\n",
    "\n",
    "for epoch in range(1, config.epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train_data_pt, valid_data_pt = prepare_data(sort_seq_batch=per_seq_batch_sample)\n",
    "    train_loader = prepare_dataloader(\n",
    "        train_data_pt,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        intra_domain_shuffle=True,\n",
    "        drop_last=False,\n",
    "    )\n",
    "    valid_loader = prepare_dataloader(\n",
    "        valid_data_pt,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        intra_domain_shuffle=False,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    if config.do_train:\n",
    "        train(\n",
    "            model,\n",
    "            loader=train_loader,\n",
    "        )\n",
    "    val_loss, val_mre = evaluate(\n",
    "        model,\n",
    "        loader=valid_loader,\n",
    "    )\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    logger.info(\"-\" * 89)\n",
    "    logger.info(\n",
    "        f\"| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | \"\n",
    "        f\"valid loss/mse {val_loss:5.4f} | mre {val_mre:5.4f}\"\n",
    "    )\n",
    "    logger.info(\"-\" * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "        best_model_epoch = epoch\n",
    "        logger.info(f\"Best model with score {best_val_loss:5.4f}\")\n",
    "\n",
    "    if epoch % config.save_eval_interval == 0 or epoch == config.epochs:\n",
    "        logger.info(f\"Saving model to {save_dir}\")\n",
    "        torch.save(best_model.state_dict(), save_dir / f\"model_e{best_model_epoch}.pt\")\n",
    "\n",
    "        # eval on testdata\n",
    "        results = eval_testdata(\n",
    "            best_model,\n",
    "            adata_t=adata_sorted if per_seq_batch_sample else adata,\n",
    "            include_types=[\"cls\"],\n",
    "        )\n",
    "        results[\"batch_umap\"].savefig(\n",
    "            save_dir / f\"embeddings_batch_umap[cls]_e{best_model_epoch}.png\", dpi=300\n",
    "        )\n",
    "\n",
    "        results[\"celltype_umap\"].savefig(\n",
    "            save_dir / f\"embeddings_celltype_umap[cls]_e{best_model_epoch}.png\", dpi=300\n",
    "        )\n",
    "        metrics_to_log = {\"test/\" + k: v for k, v in results.items()}\n",
    "        metrics_to_log[\"test/batch_umap\"] = wandb.Image(\n",
    "            str(save_dir / f\"embeddings_batch_umap[cls]_e{best_model_epoch}.png\"),\n",
    "            caption=f\"celltype avg_bio epoch {best_model_epoch}\",\n",
    "        )\n",
    "\n",
    "        metrics_to_log[\"test/celltype_umap\"] = wandb.Image(\n",
    "            str(save_dir / f\"embeddings_celltype_umap[cls]_e{best_model_epoch}.png\"),\n",
    "            caption=f\"celltype avg_bio epoch {best_model_epoch}\",\n",
    "        )\n",
    "        metrics_to_log[\"test/best_model_epoch\"] = best_model_epoch\n",
    "        wandb.log(metrics_to_log)\n",
    "        wandb.log({\"avg_bio\": results.get(\"avg_bio\", 0.0)})\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save the best model\n",
    "save_dir = Path(\"/nfs/public/cell_gpt_data/Intergation_COVID/Results/\")\n",
    "torch.save(best_model.state_dict(), save_dir / \"scGPT_best_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-04T02:05:54.199266Z",
     "iopub.status.busy": "2023-11-04T02:05:54.198409Z",
     "iopub.status.idle": "2023-11-04T02:05:54.212939Z",
     "shell.execute_reply": "2023-11-04T02:05:54.211993Z",
     "shell.execute_reply.started": "2023-11-04T02:05:54.199210Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def GetEmbedding(model: nn.Module,\n",
    "    adata: AnnData,\n",
    "    include_types: List[str] = [\"cls\"],):\n",
    "    model.eval()\n",
    "\n",
    "    # copy adata_t to avoid reuse previously computed results stored in adata_t\n",
    "    adata_t = adata.copy()\n",
    "\n",
    "    all_counts = (\n",
    "        adata_t.layers[input_layer_key].A\n",
    "        if issparse(adata_t.layers[input_layer_key])\n",
    "        else adata_t.layers[input_layer_key]\n",
    "    )\n",
    "\n",
    "    celltypes_labels = adata_t.obs[\"celltype\"].tolist()\n",
    "    celltypes_labels = np.array(celltypes_labels)\n",
    "\n",
    "    batch_ids = adata_t.obs[\"batch_id\"].tolist()\n",
    "    batch_ids = np.array(batch_ids)\n",
    "\n",
    "    # Evaluate cls cell embeddings\n",
    "    if \"cls\" in include_types:\n",
    "        logger.info(\"Evaluating cls cell embeddings\")\n",
    "        tokenized_all = tokenize_and_pad_batch(\n",
    "            all_counts,\n",
    "            gene_ids,\n",
    "            max_len=max_seq_len,\n",
    "            vocab=vocab,\n",
    "            pad_token=pad_token,\n",
    "            pad_value=pad_value,\n",
    "            append_cls=True,  # append <cls> token at the beginning\n",
    "            include_zero_gene=True,\n",
    "        )\n",
    "        all_gene_ids, all_values = tokenized_all[\"genes\"], tokenized_all[\"values\"]\n",
    "        src_key_padding_mask = all_gene_ids.eq(vocab[pad_token])\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast(enabled=config.amp):\n",
    "            cell_embeddings = model.encode_batch(\n",
    "                all_gene_ids,\n",
    "                all_values.float(),\n",
    "                src_key_padding_mask=src_key_padding_mask,\n",
    "                batch_size=config.batch_size,\n",
    "                batch_labels=torch.from_numpy(batch_ids).long() if DSBN else None,\n",
    "                time_step=0,\n",
    "                return_np=True,\n",
    "            )\n",
    "        cell_embeddings = cell_embeddings / np.linalg.norm(\n",
    "            cell_embeddings, axis=1, keepdims=True\n",
    "        )\n",
    "\n",
    "        adata_t.obsm[\"X_scGPT\"] = cell_embeddings\n",
    "        return adata_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-04T02:05:57.519108Z",
     "iopub.status.busy": "2023-11-04T02:05:57.517829Z",
     "iopub.status.idle": "2023-11-04T02:05:57.525250Z",
     "shell.execute_reply": "2023-11-04T02:05:57.524425Z",
     "shell.execute_reply.started": "2023-11-04T02:05:57.519049Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-04T02:06:02.541954Z",
     "iopub.status.busy": "2023-11-04T02:06:02.540642Z",
     "iopub.status.idle": "2023-11-04T02:18:52.862623Z",
     "shell.execute_reply": "2023-11-04T02:18:52.861624Z",
     "shell.execute_reply.started": "2023-11-04T02:06:02.541895Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - Evaluating cls cell embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4287/4287 [12:24<00:00,  5.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "770.3121161460876\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "adata_result = GetEmbedding(best_model, adata)\n",
    "end_time = time.time()\n",
    "print(end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-04T02:20:35.046667Z",
     "iopub.status.busy": "2023-11-04T02:20:35.045594Z",
     "iopub.status.idle": "2023-11-04T02:20:35.056458Z",
     "shell.execute_reply": "2023-11-04T02:20:35.055693Z",
     "shell.execute_reply.started": "2023-11-04T02:20:35.046606Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 274346 × 1997\n",
       "    obs: 'age', 'age_range', 'anatomical_region', 'anatomical_region_detailed', 'batch', 'dataset', 'disease', 'donor', 'ethnicity', 'ethnicity_mixed', 'last_author/PI', 'lung_vs_nasal', 'original_celltype_ann', 'pack_years', 'sample', 'sample_alias', 'sample_type', 'sampling_method', 'sex', 'smoking', 'total_counts', 'log10_total_counts', 'n_genes_detected', 'mito_frac', 'ribo_frac', 'compl', 'ann_level_1', 'ann_level_2', 'ann_level_3', 'ann_level_4', 'ann_level_5', 'ann_highest_res', 'ann_new', 'subject_type', 'study', 'study2', 'celltype', 'condition', 'cellnames', 'cluster', 'stage', 'ID', 'sample_new', 'chemistry', 'data_type', 'dpt_pseudotime', 'final_annotation', 'mt_frac', 'n_counts', 'n_genes', 'sample_ID', 'size_factors', 'species', 'tissue', 'batch_id'\n",
       "    var: 'gene_name', 'id_in_vocab'\n",
       "    uns: 'log1p'\n",
       "    obsm: 'bin_edges', 'X_scGPT'\n",
       "    layers: 'X_normed', 'X_binned'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-04T02:20:40.996254Z",
     "iopub.status.busy": "2023-11-04T02:20:40.995102Z",
     "iopub.status.idle": "2023-11-04T02:21:34.223791Z",
     "shell.execute_reply": "2023-11-04T02:21:34.222852Z",
     "shell.execute_reply.started": "2023-11-04T02:20:40.996203Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "adata_result.write_h5ad(\"/nfs/public/cell_gpt_data/Intergation_COVID/Results/scGPT_result.h5ad\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scGPT",
   "language": "python",
   "name": "scgpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
